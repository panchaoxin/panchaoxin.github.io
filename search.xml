<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Nuxt HelloWorld]]></title>
    <url>%2F2019%2F04%2F28%2FVue%2FNuxt%2FNuxt-HelloWorld%2F</url>
    <content type="text"><![CDATA[1. 创建项目安装 create-nuxt-app，这是创建nuxt项目的脚手架 1npm install -g create-nuxt-app 使用脚手架创建项目 create-nuxt-app &lt;项目名&gt;123456789101112131415161718192021&gt; create-nuxt-app hello? Project name hello? Project description My super Nuxt.js project? Use a custom server framework express? Choose features to install Axios? Use a custom UI framework element-ui? Use a custom test framework none? Choose rendering mode Single Page App? Author name panchaoxin? Choose a package manager npm To get started: cd hello npm run dev To build &amp; start for production: cd hello npm run build npm start 还要安装一些基本依赖123cnpm i css-loader -Dcnpm i node-less -Dcnpm i less-loader -D 2. 项目目录结构参考官方：https://zh.nuxtjs.org/guide/directory-structure assets和static目录的区别：这两个目录都是存放静态资源的，webpack会编译assets中的静态资源，但是不会编译static中的静态资源]]></content>
      <tags>
        <tag>Nuxt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nuxt 概述]]></title>
    <url>%2F2019%2F04%2F28%2FVue%2FNuxt%2FNuxt-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"></content>
      <tags>
        <tag>Nuxt</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Algorithms 12 整数转罗马数字]]></title>
    <url>%2F2019%2F04%2F27%2FProblem%2FLeetCode%2FLeetCode%20Algorithms%2FLeetCode-Algorithms-12-%E6%95%B4%E6%95%B0%E8%BD%AC%E7%BD%97%E9%A9%AC%E6%95%B0%E5%AD%97%2F</url>
    <content type="text"><![CDATA[1. 题目描述罗马数字包含以下七种字符： I， V， X， L，C，D 和 M。 12345678字符 数值I 1V 5X 10L 50C 100D 500M 1000 例如， 罗马数字 2 写做 II ，即为两个并列的 1。12 写做 XII ，即为 X + II 。 27 写做 XXVII, 即为 XX + V + II 。 通常情况下，罗马数字中小的数字在大的数字的右边。但也存在特例，例如 4 不写做 IIII，而是 IV。数字 1 在数字 5 的左边，所表示的数等于大数 5 减小数 1 得到的数值 4 。同样地，数字 9 表示为 IX。这个特殊的规则只适用于以下六种情况： I 可以放在 V (5) 和 X (10) 的左边，来表示 4 和 9。 X 可以放在 L (50) 和 C (100) 的左边，来表示 40 和 90。 C 可以放在 D (500) 和 M (1000) 的左边，来表示 400 和 900。 给定一个整数，将其转为罗马数字。输入确保在 1 到 3999 的范围内。 示例 1: 12输入: 3输出: &quot;III&quot; 示例 2: 12输入: 4输出: &quot;IV&quot; 示例 3: 12输入: 9输出: &quot;IX&quot; 示例 4: 123输入: 58输出: &quot;LVIII&quot;解释: L = 50, V = 5, III = 3. 示例 5: 123输入: 1994输出: &quot;MCMXCIV&quot;解释: M = 1000, CM = 900, XC = 90, IV = 4. 2. 题解2.1. 思路1准备数字表 2.2. Java实现思路1：准备数字表 12345678910111213141516class Solution &#123; public String intToRoman(int num) &#123; String[][] c = &#123; // 个位 &#123;"", "I", "II", "III", "IV", "V", "VI", "VII", "VIII", "IX"&#125;, // 十位 &#123;"", "X", "XX", "XXX", "XL", "L", "LX", "LXX", "LXXX", "XC"&#125;, // 百位 &#123;"", "C", "CC", "CCC", "CD", "D", "DC", "DCC", "DCCC", "CM"&#125;, // 千位 &#123;"", "M", "MM", "MMM"&#125; &#125;; return c[3][num / 1000 % 10] + c[2][num / 100 % 10] + c[1][num / 10 % 10] + c[0][num % 10]; &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Algorithms 11 盛最多水的容器]]></title>
    <url>%2F2019%2F04%2F27%2FProblem%2FLeetCode%2FLeetCode%20Algorithms%2FLeetCode-Algorithms-11-%E7%9B%9B%E6%9C%80%E5%A4%9A%E6%B0%B4%E7%9A%84%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1. 题目描述给定 n 个非负整数 a1，a2，…，an，每个数代表坐标中的一个点 (i, ai) 。在坐标内画 n 条垂直线，垂直线 i 的两个端点分别为 (i, ai) 和 (i, 0)。找出其中的两条线，使得它们与 x 轴共同构成的容器可以容纳最多的水。 说明：你不能倾斜容器，且 n 的值至少为 2。 图中垂直线代表输入数组 [1,8,6,2,5,4,8,3,7]。在此情况下，容器能够容纳水（表示为蓝色部分）的最大值为 49。 示例: 12输入: [1,8,6,2,5,4,8,3,7]输出: 49 2. 题解题意就是求哪两条线围成的面积最大 2.1. 思路1暴力法：枚举所有窗口[i,j] 复杂度分析 时间复杂度：O(n^2)，要枚举所有区间 空间复杂度：O(1)，使用恒定的额外空间 2.2. 思路2双指针法。最初我们考虑由最外围两条线段构成的区域。现在，为了使面积最大化，我们需要考虑更长的两条线段之间的区域。如果我们试图将指向较长线段的指针向内侧移动，矩形区域的面积将受限于较短的线段而不会获得任何增加。但是，在同样的条件下，移动指向较短线段的指针尽管造成了矩形宽度的减小，但却可能会有助于面积的增大。因为移动较短线段的指针会得到一条相对较长的线段，这可以克服由宽度减小而引起的面积减小。 复杂度分析 时间复杂度：O(n)，一次扫描 空间复杂度：O(1)，使用恒定的空间 2.3. Java实现暴力法 123456789101112class Solution &#123; public int maxArea(int[] height) &#123; int res = 0; for (int i = 0; i &lt; height.length; i++) &#123; for (int j = i + 1; j &lt; height.length; j++) &#123; int h = Math.min(height[i], height[j]); res = Math.max(res, (j - i) * h); &#125; &#125; return res; &#125;&#125; 双指针法 123456789101112131415class Solution &#123; public int maxArea(int[] height) &#123; int maxArea = 0, l = 0, r = height.length - 1; while (l &lt; r) &#123; int h = Math.min(height[l], height[r]); maxArea = Math.max(maxArea, h * (r - l)); if (height[l] &lt; height[r]) &#123; l++; &#125; else &#123; r--; &#125; &#125; return maxArea; &#125;&#125;]]></content>
      <tags>
        <tag>LeetCode Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Algorithms 10 正则表达式匹配]]></title>
    <url>%2F2019%2F04%2F27%2FProblem%2FLeetCode%2FLeetCode%20Algorithms%2FLeetCode-Algorithms-10-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E5%8C%B9%E9%85%8D%2F</url>
    <content type="text"><![CDATA[1. 题目描述给定一个字符串 (s) 和一个字符模式 (p)。实现支持 &#39;.&#39; 和 &#39;*&#39; 的正则表达式匹配。 12&apos;.&apos; 匹配任意单个字符。&apos;*&apos; 匹配零个或多个前面的元素。 匹配应该覆盖整个字符串 (s) ，而不是部分字符串。 说明: s 可能为空，且只包含从 a-z 的小写字母。 p 可能为空，且只包含从 a-z 的小写字母，以及字符 . 和 *。 示例 1: 12345输入:s = &quot;aa&quot;p = &quot;a&quot;输出: false解释: &quot;a&quot; 无法匹配 &quot;aa&quot; 整个字符串。 示例 2: 12345输入:s = &quot;aa&quot;p = &quot;a*&quot;输出: true解释: &apos;*&apos; 代表可匹配零个或多个前面的元素, 即可以匹配 &apos;a&apos; 。因此, 重复 &apos;a&apos; 一次, 字符串可变为 &quot;aa&quot;。 示例 3: 12345输入:s = &quot;ab&quot;p = &quot;.*&quot;输出: true解释: &quot;.*&quot; 表示可匹配零个或多个(&apos;*&apos;)任意字符(&apos;.&apos;)。 示例 4: 12345输入:s = &quot;aab&quot;p = &quot;c*a*b&quot;输出: true解释: &apos;c&apos; 可以不被重复, &apos;a&apos; 可以被重复一次。因此可以匹配字符串 &quot;aab&quot;。 示例 5: 1234输入:s = &quot;mississippi&quot;p = &quot;mis*is*p*.&quot;输出: false 2. 题解2.1. 思路1直接使用正则表达式，显然面试不能这样来 2.2. Java实现12345678910import java.util.regex.Matcher;import java.util.regex.Pattern;class Solution &#123; public boolean isMatch(String s, String p) &#123; Pattern pattern = Pattern.compile(p); Matcher m = pattern.matcher(s); return m.matches(); &#125;&#125;]]></content>
      <tags>
        <tag>LeetCode Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Algorithms 9 回文数]]></title>
    <url>%2F2019%2F04%2F27%2FProblem%2FLeetCode%2FLeetCode%20Algorithms%2FLeetCode-Algorithms-9-%E5%9B%9E%E6%96%87%E6%95%B0%2F</url>
    <content type="text"><![CDATA[1. 题目描述判断一个整数是否是回文数。回文数是指正序（从左向右）和倒序（从右向左）读都是一样的整数。 示例 1: 12输入: 121输出: true 示例 2: 123输入: -121输出: false解释: 从左向右读, 为 -121 。 从右向左读, 为 121- 。因此它不是一个回文数。 示例 3: 123输入: 10输出: false解释: 从右向左读, 为 01 。因此它不是一个回文数。 进阶: 你能不将整数转为字符串来解决这个问题吗？ 2. 题解2.1. 思路1反转数字，然后看与原数字是否相等。 根据样例-121反转后是121-，所以如果输入是负数，就直接判断为false即可 2.2. Java实现123456789101112131415161718class Solution &#123; public int reverseNum(int x) &#123; int n = 0; do &#123; n = n * 10 + x % 10; x /= 10; &#125; while (x != 0); return n; &#125; public boolean isPalindrome(int x) &#123; // 输入是负数，直接判断为false if (x &lt; 0) &#123; return false; &#125; // 看反转之后与原数字是否相等 return x == reverseNum(x); &#125;&#125;]]></content>
      <tags>
        <tag>LeetCode Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Algorithms 8 字符串转换整数 (atoi)]]></title>
    <url>%2F2019%2F04%2F27%2FProblem%2FLeetCode%2FLeetCode%20Algorithms%2FLeetCode-Algorithms-8-%E5%AD%97%E7%AC%A6%E4%B8%B2%E8%BD%AC%E6%8D%A2%E6%95%B4%E6%95%B0-atoi%2F</url>
    <content type="text"><![CDATA[1. 题目描述请你来实现一个 atoi 函数，使其能将字符串转换成整数。 首先，该函数会根据需要丢弃无用的开头空格字符，直到寻找到第一个非空格的字符为止。 当我们寻找到的第一个非空字符为正或者负号时，则将该符号与之后面尽可能多的连续数字组合起来，作为该整数的正负号；假如第一个非空字符是数字，则直接将其与之后连续的数字字符组合起来，形成整数。 该字符串除了有效的整数部分之后也可能会存在多余的字符，这些字符可以被忽略，它们对于函数不应该造成影响。 注意：假如该字符串中的第一个非空格字符不是一个有效整数字符、字符串为空或字符串仅包含空白字符时，则你的函数不需要进行转换。 在任何情况下，若函数不能进行有效的转换时，请返回 0。 说明： 假设我们的环境只能存储 32 位大小的有符号整数，那么其数值范围为 [−231, 231 − 1]。如果数值超过这个范围，qing返回 INT_MAX (231 − 1) 或 INT_MIN (−231) 。 示例 1: 12输入: &quot;42&quot;输出: 42 示例 2: 1234输入: &quot; -42&quot;输出: -42解释: 第一个非空白字符为 &apos;-&apos;, 它是一个负号。 我们尽可能将负号与后面所有连续出现的数字组合起来，最后得到 -42 。 示例 3: 123输入: &quot;4193 with words&quot;输出: 4193解释: 转换截止于数字 &apos;3&apos; ，因为它的下一个字符不为数字。 示例 4: 1234输入: &quot;words and 987&quot;输出: 0解释: 第一个非空字符是 &apos;w&apos;, 但它不是数字或正、负号。 因此无法执行有效的转换。 示例 5: 1234输入: &quot;-91283472332&quot;输出: -2147483648解释: 数字 &quot;-91283472332&quot; 超过 32 位有符号整数范围。 因此返回 INT_MIN (−231) 。 2. 题解2.1. 思路1用正则提取出前缀数字。 注意要用高精度类来提取，可能存在10000000000000000000000 2.2. Java实现12345678910111213141516171819202122232425262728293031import java.util.regex.Matcher;import java.util.regex.Pattern;import java.math.BigInteger;class Solution &#123; public int myAtoi(String str) &#123; /* [\\s]* 前面可以有任意个空白符 ([+-]?[0-9]+) 提取带符号的数字 */ final Pattern pattern = Pattern.compile("^[\\s]*([+-]?[0-9]+)"); Matcher m = pattern.matcher(str); if (!m.find()) &#123; // 如果数字不存在，则返回0 return 0; &#125; // 本题要使用高精确度 BigInteger res = new BigInteger(m.group(1)); final BigInteger up = BigInteger.valueOf(Integer.MAX_VALUE); final BigInteger lower = BigInteger.valueOf(Integer.MIN_VALUE); if (res.compareTo(up) &gt; 0) &#123; // 比Integer.MAX_VALUE大，就返回Integer.MAX_VALUE return Integer.MAX_VALUE; &#125; else if (res.compareTo(lower) &lt; 0) &#123; // 比Integer.MIN_VALUE小，就返回Integer.MIN_VALUE return Integer.MIN_VALUE; &#125; return res.intValue(); &#125;&#125;]]></content>
      <tags>
        <tag>LeetCode Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Algorithms 7 整数反转]]></title>
    <url>%2F2019%2F04%2F27%2FProblem%2FLeetCode%2FLeetCode%20Algorithms%2FLeetCode-Algorithms-7-%E6%95%B4%E6%95%B0%E5%8F%8D%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[1. 题目描述给出一个 32 位的有符号整数，你需要将这个整数中每位上的数字进行反转。 示例 1: 12输入: 123输出: 321 示例 2: 12输入: -123输出: -321 示例 3: 12输入: 120输出: 21 注意: 假设我们的环境只能存储得下 32 位的有符号整数，则其数值范围为 [−2^31, 2^31 − 1]。请根据这个假设，如果反转后整数溢出那么就返回 0。 2. 题解2.1. 思路求余反转即可，正负数不需要分开实现。 注意题目的特别要求：溢出就返回0 时间复杂度：O(lg(n)) 空间复杂度：O(1) 2.2. Java实现1234567891011class Solution &#123; public int reverse(int x) &#123; long n = 0; do &#123; n *= 10 + x % 10; x /= 10; &#125; while (x != 0); // 溢出就返回0 return (int) (n &gt; Integer.MAX_VALUE || n &lt; Integer.MIN_VALUE ? 0 : n); &#125;&#125;]]></content>
      <tags>
        <tag>LeetCode Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Algorithms 6 Z 字形变换]]></title>
    <url>%2F2019%2F04%2F27%2FProblem%2FLeetCode%2FLeetCode%20Algorithms%2FLeetCode-Algorithms-6-Z-%E5%AD%97%E5%BD%A2%E5%8F%98%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[1. 题目描述将一个给定字符串根据给定的行数，以从上往下、从左到右进行 Z 字形排列。 比如输入字符串为 &quot;LEETCODEISHIRING&quot; 行数为 3 时，排列如下： 123L C I RE T O E S I I GE D H N 之后，你的输出需要从左往右逐行读取，产生出一个新的字符串，比如：&quot;LCIRETOESIIGEDHN&quot;。 请你实现这个将字符串进行指定行数变换的函数： 1string convert(string s, int numRows); 示例 1: 12输入: s = &quot;LEETCODEISHIRING&quot;, numRows = 3输出: &quot;LCIRETOESIIGEDHN&quot; 示例 2: 12345678输入: s = &quot;LEETCODEISHIRING&quot;, numRows = 4输出: &quot;LDREOEIIECIHNTSG&quot;解释:L D RE O E I IE C I H NT S G 2. 题解数学找规律题，不必重视 2.1. 思路2.2. Java实现1234567891011121314151617181920212223class Solution &#123; public String convert(String s, int numRows) &#123; if (numRows == 1) return s; List&lt;StringBuilder&gt; rows = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; Math.min(numRows, s.length()); i++) rows.add(new StringBuilder()); int curRow = 0; boolean goingDown = false; for (char c : s.toCharArray()) &#123; rows.get(curRow).append(c); if (curRow == 0 || curRow == numRows - 1) goingDown = !goingDown; curRow += goingDown ? 1 : -1; &#125; StringBuilder ret = new StringBuilder(); for (StringBuilder row : rows) ret.append(row); return ret.toString(); &#125;&#125;]]></content>
      <tags>
        <tag>LeetCode Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Algorithms 5 最长回文子串]]></title>
    <url>%2F2019%2F04%2F27%2FProblem%2FLeetCode%2FLeetCode%20Algorithms%2FLeetCode-Algorithms-5-%E6%9C%80%E9%95%BF%E5%9B%9E%E6%96%87%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[1. 题目描述给定一个字符串 s，找到 s 中最长的回文子串。你可以假设 s 的最大长度为 1000。 示例 1： 123输入: &quot;babad&quot;输出: &quot;bab&quot;注意: &quot;aba&quot; 也是一个有效答案。 示例 2： 12输入: &quot;cbbd&quot;输出: &quot;bb&quot; 2. 题解2.1. 思路Manacher算法 2.2. Java实现123456789101112131415161718192021222324252627class Solution &#123; public String longestPalindrome(String s) &#123; String t = "#"; for (char c : s.toCharArray()) &#123; t += c; t += '#'; &#125; int[] r = new int[t.length()]; int pos = 0, maxLen = 0, maxRight = 0; String res = ""; for (int i = 0; i &lt; t.length(); i++) &#123; r[i] = i &lt; maxRight ? Math.min(maxRight - i, r[2 * pos - i]) : 1; while (i - r[i] &gt;= 0 &amp;&amp; i + r[i] &lt; t.length() &amp;&amp; t.charAt(i - r[i]) == t.charAt(i + r[i])) &#123; ++r[i]; &#125; if (maxRight &lt; i + r[i] - 1) &#123; maxRight = i + r[i] - 1; pos = i; &#125; if (r[i] &gt; maxLen) &#123; maxLen = r[i]; res = t.substring(i - r[i] + 1, i + r[i]); &#125; &#125; return res.replaceAll("#", ""); &#125;&#125;]]></content>
      <tags>
        <tag>LeetCode Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Algorithms 4 寻找两个有序数组的中位数]]></title>
    <url>%2F2019%2F04%2F27%2FProblem%2FLeetCode%2FLeetCode%20Algorithms%2FLeetCode-Algorithms-4-%E5%AF%BB%E6%89%BE%E4%B8%A4%E4%B8%AA%E6%9C%89%E5%BA%8F%E6%95%B0%E7%BB%84%E7%9A%84%E4%B8%AD%E4%BD%8D%E6%95%B0%2F</url>
    <content type="text"><![CDATA[1. 题目描述给定两个大小为 m 和 n 的有序数组 nums1 和 nums2。 请你找出这两个有序数组的中位数，并且要求算法的时间复杂度为 O(log(m + n))。 你可以假设 nums1 和 nums2 不会同时为空。 示例 1: 1234nums1 = [1, 3]nums2 = [2]则中位数是 2.0 示例 2: 1234nums1 = [1, 2]nums2 = [3, 4]则中位数是 (2 + 3)/2 = 2.5 2. 题解2.1. 思路2.2. Java实现1234567891011121314151617181920212223242526272829303132333435363738394041424344class Solution &#123; public double findMedianSortedArrays(int[] A, int[] B) &#123; int m = A.length; int n = B.length; if (m &gt; n) &#123; return findMedianSortedArrays(B, A); &#125; int iMin = 0, iMax = m, halfLen = (m + n + 1) / 2; while (iMin &lt;= iMax) &#123; int i = (iMin + iMax) / 2; int j = halfLen - i; if (i &lt; iMax &amp;&amp; B[j - 1] &gt; A[i]) &#123; iMin = i + 1; // i is too small &#125; else if (i &gt; iMin &amp;&amp; A[i - 1] &gt; B[j]) &#123; iMax = i - 1; // i is too big &#125; else &#123; // i is perfect int maxLeft = 0; if (i == 0) &#123; maxLeft = B[j - 1]; &#125; else if (j == 0) &#123; maxLeft = A[i - 1]; &#125; else &#123; maxLeft = Math.max(A[i - 1], B[j - 1]); &#125; if ((m + n) % 2 == 1) &#123; return maxLeft; &#125; int minRight = 0; if (i == m) &#123; minRight = B[j]; &#125; else if (j == n) &#123; minRight = A[i]; &#125; else &#123; minRight = Math.min(B[j], A[i]); &#125; return (maxLeft + minRight) / 2.0; &#125; &#125; return 0.0; &#125;&#125;]]></content>
      <tags>
        <tag>LeetCode Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Algorithms 3 无重复字符的最长子串]]></title>
    <url>%2F2019%2F04%2F27%2FProblem%2FLeetCode%2FLeetCode%20Algorithms%2FLeetCode-Algorithms-3-%E6%97%A0%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E6%9C%80%E9%95%BF%E5%AD%90%E4%B8%B2%2F</url>
    <content type="text"><![CDATA[1. 题目描述给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。 示例 1: 123输入: &quot;abcabcbb&quot;输出: 3 解释: 因为无重复字符的最长子串是 &quot;abc&quot;，所以其长度为 3。 示例 2: 123输入: &quot;bbbbb&quot;输出: 1解释: 因为无重复字符的最长子串是 &quot;b&quot;，所以其长度为 1。 示例 3: 1234输入: &quot;pwwkew&quot;输出: 3解释: 因为无重复字符的最长子串是 &quot;wke&quot;，所以其长度为 3。 请注意，你的答案必须是 子串 的长度，&quot;pwke&quot; 是一个子序列，不是子串。 2. 题解2.1. 思路1：暴力法用两层循环遍历所有子串，取不重复的最长的 12345for (int i = 0; i &lt; n; i++) &#123; for (int j = i + 1; j &lt; n; j++) &#123; // 取不重复的最长，更新maxLen &#125;&#125; 2.2. 思路2：滑动窗口保证窗口[i, j)是当前无重复字符的局部最长子串 2.3. Java实现思路1：暴力法，用HashSet进行判重。如果[i,j]范围内的子串出现重复，j就不用必向后遍历了，直接break 123456789101112131415161718192021class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; if (s == null || s.length() == 0) &#123; return 0; &#125; int n = s.length(); int maxLen = 1; for (int i = 0; i &lt; n; i++) &#123; Set&lt;Character&gt; set = new HashSet&lt;&gt;(); set.add(s.charAt(i)); for (int j = i + 1; j &lt; n; j++) &#123; if (set.contains(s.charAt(j))) &#123; break; &#125; set.add(s.charAt(j)); maxLen = Math.max(maxLen, j - i + 1); &#125; &#125; return maxLen; &#125;&#125; 思路2：滑动窗口 12345678910111213141516171819class Solution &#123; public int lengthOfLongestSubstring(String s) &#123; int n = s.length(); Set&lt;Character&gt; set = new HashSet&lt;&gt;(); int ans = 0, i = 0, j = 0; while (i &lt; n &amp;&amp; j &lt; n) &#123; if (!set.contains(s.charAt(j))) &#123; // 局部最长子串 + 1 set.add(s.charAt(j++)); // 更新全局最长子串的长度 ans = Math.max(ans, j - i); &#125; else &#123; // 窗口中有重复串存在，i前移 set.remove(s.charAt(i++)); &#125; &#125; return ans; &#125;&#125;]]></content>
      <tags>
        <tag>LeetCode Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Algorithms 2 两数相加]]></title>
    <url>%2F2019%2F04%2F26%2FProblem%2FLeetCode%2FLeetCode%20Algorithms%2FLeetCode-Algorithms-2-%E4%B8%A4%E6%95%B0%E7%9B%B8%E5%8A%A0%2F</url>
    <content type="text"><![CDATA[1. 题目描述给出两个 非空 的链表用来表示两个非负的整数。其中，它们各自的位数是按照 逆序 的方式存储的，并且它们的每个节点只能存储 一位 数字。 如果，我们将这两个数相加起来，则会返回一个新的链表来表示它们的和。 您可以假设除了数字 0 之外，这两个数都不会以 0 开头。 示例： 123输入：(2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)输出：7 -&gt; 0 -&gt; 8原因：342 + 465 = 807 2. 题解2.1. 思路竖式加法。左边是低位，右边是高位1234 (2 -&gt; 4 -&gt; 3) + (5 -&gt; 6 -&gt; 4)-----------------= (7 -&gt; 0 -&gt; 8) 假设m和n分别代表两个链表的长度 时间复杂度：O(max{m, n})，因为长的链表遍历完就结束 空间复杂度：O(max{m, n})，因为新链表长度取决于长链表 2.2. Java实现123456789101112131415161718192021222324252627282930313233343536/** * Definition for singly-linked list. * public class ListNode &#123; * long val; * ListNode next; * ListNode(long x) &#123; val = x; &#125; * &#125; */class Solution &#123; public ListNode addTwoNumbers(ListNode l1, ListNode l2) &#123; ListNode p = l1; ListNode q = l2; ListNode list = new ListNode(-1); ListNode tail = list; int sumWithCarry = 0; while (p != null || q != null) &#123; if (p != null) &#123; sumWithCarry += p.val; p = p.next; &#125; if (q != null) &#123; sumWithCarry += q.val; q = q.next; &#125; tail.next = new ListNode(sumWithCarry % 10); tail = tail.next; sumWithCarry /= 10; &#125; // 如果有多余的进位，则再添加1个节点 if (sumWithCarry &gt; 0) &#123; tail.next = new ListNode(sumWithCarry); tail = tail.next; &#125; return list.next; &#125;&#125;]]></content>
      <tags>
        <tag>LeetCode Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Algorithms 1 两数之和]]></title>
    <url>%2F2019%2F04%2F26%2FProblem%2FLeetCode%2FLeetCode%20Algorithms%2FLeetCode-Algorithms-1-%E4%B8%A4%E6%95%B0%E4%B9%8B%E5%92%8C%2F</url>
    <content type="text"><![CDATA[1. 题目描述给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。 示例: 1234给定 nums = [2, 7, 11, 15], target = 9因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 2. 题解说明： 样例nums[0] + nums[1] = 2 + 7 = 9，你返回[0, 1]或者[1, 0]都是对的 题目保证有解 2.1. 思路遍历数组，构建HashMap&lt;数组元素，元素对应下标&gt; 遍历nums[i]时，看一下target - nums[i]是否在HashMap中，如果有，就返回解 时间复杂度：O(n)，因为要遍历整个数组 空间复杂度：O(n)，因为用到HashMap 2.2. Java实现123456789101112131415161718192021class Solution &#123; public int[] twoSum(int[] nums, int target) &#123; Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); int[] res = &#123;-1, -1&#125;; if (nums == null || nums.length &lt; 2) &#123; return res; &#125; for (int i = 0; i &lt; nums.length; i++) &#123; int a = nums[i]; int b = target - nums[i]; if (map.containsKey(b)) &#123; // 取出b的下标j，返回解 &#123;i, j&#125; int j = map.get(b); res = new int[]&#123;i, j&#125;; break; &#125; map.put(a, i); &#125; return res; &#125;&#125;]]></content>
      <tags>
        <tag>LeetCode Algorithms</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC 处理静态资源请求]]></title>
    <url>%2F2019%2F04%2F26%2FSpringMVC%2FSpringMVC-%E5%A4%84%E7%90%86%E9%9D%99%E6%80%81%E8%B5%84%E6%BA%90%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[1. DispatcherServlet拦截路径导致静态资源无法访问的问题首先谈一下Tomcat几个内置的用于处理请求资源的Servlet。 DefaultServlet：拦截所有的静态资源请求，返回给客户端 JspServlet：拦截所有的jsp请求，渲染jsp页面返回给客户端 12345678910111213141516171819202122232425&lt;!-- DefaultServlet会应用于所有的tomcat项目，用于处理静态资源请求。 哪些资源是静态资源呢？简单地讲，除了jsp/servlet（jsp本质还是servlet），其它的资源都是静态资源 DefaultServlet是如何处理静态资源的请求呢？以用户访问index.html为例 DefaultServlet会在服务器下找到index.html这个资源，并返回给用户--&gt;&lt;servlet&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.catalina.servlets.DefaultServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;debug&lt;/param-name&gt; &lt;param-value&gt;0&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;listings&lt;/param-name&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 12345678910111213141516171819&lt;servlet&gt; &lt;servlet-name&gt;jsp&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.jasper.servlet.JspServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;fork&lt;/param-name&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;xpoweredBy&lt;/param-name&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;3&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;jsp&lt;/servlet-name&gt; &lt;url-pattern&gt;*.jsp&lt;/url-pattern&gt; &lt;url-pattern&gt;*.jspx&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; DispatcherServlet常用URL映射拦截规则如下： /*：拦截所有请求，优先级高于DefaultServlet和JspServlet的url-pattern，所以静态资源和jsp都无法获取 /：覆盖DefaultServlet的url-pattern，所以静态资源无法获取，jsp页面正常显示 .action或.do：只拦截指定后缀的请求。静态资源和jsp都正常获取 DispatcherServlet的常用URL映射拦截规则如下，/*的拦截范围比/更大，还会拦截到*.jsp请求，这样jsp页面就无法显示了 2. 问题解决你可能会想，那直接把DispatcherServlet的url-pattern设置为.do之类的后缀，不能自然而然解决了吗。 但是实际上不建议处理带后缀的请求，因为优秀的REST风格的URL不希望带有任何后缀。 一般我们会把DispatcherServlet的url-pattern设置为/，但是会导致静态资源无法获取。解决方法如下： 2.1. 方式一：激活Tomcat的defaultServlet来处理静态文件给DefaultServlet添加多个url-pattern，能够处理各种静态文件 123456789101112&lt;servlet-mapping&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;url-pattern&gt;*.js&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;url-pattern&gt;*.css&lt;/url-pattern&gt; &lt;/servlet-mapping&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;url-pattern&gt;*.jpg&lt;/url-pattern&gt; &lt;/servlet-mapping&gt; 2.2. 方式二：使用&lt;mvc:resources/&gt;&lt;mvc:resources/&gt;最灵活，可以拦截各种形式的请求。location可以是webapp的路径，也可以是classpath路径 1234567&lt;!-- resources mapping --&gt;&lt;mvc:resources location="/css/" mapping="/css/**"/&gt;&lt;mvc:resources location="/js/" mapping="/js/**"/&gt;&lt;mvc:resources location="/images/" mapping="/images/**"/&gt;&lt;mvc:resources location="/html/" mapping="/html/**"/&gt;&lt;mvc:resources location="/fonts/" mapping="/fonts/**"/&gt;&lt;mvc:resources location="/plugins/" mapping="/plugins/**"/&gt; 2.3. 方式三：使用&lt;mvc:default-servlet-handler/&gt;&lt;mvc:default-servlet-handler/&gt;的作用是，对进入DispatcherServlet的请求进行筛选 将无法处理的请求（没有经过映射的请求）转发给DefaultServlet去处理 如果是经过映射的请求，则交给DispatcherServlet继续进行处理 在springmvc.xml中配置如下： 1&lt;mvc:default-servlet-handler/&gt; 配置&lt;mvc:default-servlet-handler/&gt;之后，你会发现所有的静态资源都可以访问了，但是访问@Controller中的url映射，都是返回404。这是因为default-servlet-handler只认识通过BeanNameUrlHandlerMapping方式配置的处理器，但是不认识@RequestMapping注解方式配置的处理器。所以default-servlet-handler误以为该请求没有对应的处理器，所以直接将给DispatcherServlet去处理，因为没有对应的资源，所以返回404 解决方法是加上&lt;mvc:annotation-driven/&gt;，这样就有HandlerMapping和HandlerAdapter支持注解方式的配置 12&lt;mvc:annotation-driven/&gt;&lt;mvc:default-servlet-handler/&gt;]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring context:component-scan与context:annotation-config的区别]]></title>
    <url>%2F2019%2F04%2F26%2FSpring%2FSpring-context-component-scan%E4%B8%8Econtext-annotation-config%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[1. 区别概述1.1. &lt;context:annotation-config/&gt;的作用&lt;context:annotation-config/&gt;用来注册几个常用的BeanPostProcessor，包括 AutowiredAnnotationBeanPostProcessor CommonAnnotationBeanPostProcessor PersistenceAnnotationBeanPostProcessor RequiredAnnotationBeanPostProcessor 这四个Processor，注册这4个BeanPostProcessor的作用，就是为了你的系统能够识别相应的注解。BeanPostProcessor就是处理注解的处理器。 比如我们要使用@Autowired注解，那么就必须事先在 Spring 容器中声明AutowiredAnnotationBeanPostProcessor Bean。传统声明方式如下1&lt;bean class="org.springframework.beans.factory.annotation.AutowiredAnnotationBeanPostProcessor"/&gt; 如果想使用@Resource、@PostConstruct、@PreDestroy等注解就必须声明CommonAnnotationBeanPostProcessor。传统声明方式如下1&lt;bean class="org.springframework.beans.factory.annotation.CommonAnnotationBeanPostProcessor"/&gt; 如果想使用@PersistenceContext注解，就必须声明PersistenceAnnotationBeanPostProcessor的Bean1&lt;bean class="org.springframework.beans.factory.annotation.PersistenceAnnotationBeanPostProcessor"/&gt; 如果想使用 @Required的注解，就必须声明RequiredAnnotationBeanPostProcessor的Bean1&lt;bean class="org.springframework.beans.factory.annotation.RequiredAnnotationBeanPostProcessor"/&gt; 一般来说，像@Resource、@PostConstruct、@Antowired这些注解在自动注入还是比较常用，所以如果总是需要按照传统的方式一条一条配置显得有些繁琐和没有必要，于是spring给我们提供&lt;context:annotation-config/&gt;的简化配置方式，自动帮你完成声明 但是&lt;context:annotation-config/&gt;存在一定的局限性，它不能激活@Component、@Controller、@Service等常用注解 1.2. &lt;context:component-scan&gt;的作用包含了&lt;context:annotation-config/&gt;的功能，同时还注册base-package下的注解类，即可以识别@Component、@Controller、@Service等注解。如果理解了上述所说，那么也就懂得，no bean named springsessionrepositoryfilter is defined，就是因为没配置此两项之一 1.3. 二者的总结&lt;context:component-scan&gt;包含了&lt;context:annotation-config/&gt;的功能，所以实际开发中，只要配置&lt;context:component-scan&gt;就可以了 如果同时使用这两个配置会不会出现重复注入的情况呢？答案是不会的。如果都配置了，&lt;context:annotation-config/&gt;会被忽略，相当于只配置了&lt;context:component-scan&gt;，所以@Autowire，@esource等注入注解只会被注入一次 2. 源码分析2.1. &lt;context:annotation-config/&gt;源码首先找到ContextNamespaceHandler，它的作用是解析context命名空间下的元素，看一下继承层次图 12345678public class ContextNamespaceHandler extends NamespaceHandlerSupport &#123; @Override public void init() &#123; // 只保留重要的两句 registerBeanDefinitionParser("annotation-config", new AnnotationConfigBeanDefinitionParser()); registerBeanDefinitionParser("component-scan", new ComponentScanBeanDefinitionParser()); &#125;&#125; annotation-config对应AnnotationConfigBeanDefinitionParser，主要看parse方法，1234567891011public class AnnotationConfigBeanDefinitionParser implements BeanDefinitionParser &#123; @Override @Nullable public BeanDefinition parse(Element element, ParserContext parserContext) &#123; Object source = parserContext.extractSource(element); // 获取所有与BeanPostProcessors有关的bean定义. Set&lt;BeanDefinitionHolder&gt; processorDefinitions = AnnotationConfigUtils.registerAnnotationConfigProcessors(parserContext.getRegistry(), source); return null; &#125;&#125; 在AnnotationConfigUtils.registerAnnotationConfigProcessors中获取所有BeanPostProcessor的bean1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889public abstract class AnnotationConfigUtils &#123; public static final String CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME = "org.springframework.context.annotation.internalConfigurationAnnotationProcessor"; public static final String CONFIGURATION_BEAN_NAME_GENERATOR = "org.springframework.context.annotation.internalConfigurationBeanNameGenerator"; public static final String AUTOWIRED_ANNOTATION_PROCESSOR_BEAN_NAME = "org.springframework.context.annotation.internalAutowiredAnnotationProcessor"; public static final String COMMON_ANNOTATION_PROCESSOR_BEAN_NAME = "org.springframework.context.annotation.internalCommonAnnotationProcessor"; public static final String PERSISTENCE_ANNOTATION_PROCESSOR_BEAN_NAME = "org.springframework.context.annotation.internalPersistenceAnnotationProcessor"; private static final String PERSISTENCE_ANNOTATION_PROCESSOR_CLASS_NAME = "org.springframework.orm.jpa.support.PersistenceAnnotationBeanPostProcessor"; public static final String EVENT_LISTENER_PROCESSOR_BEAN_NAME = "org.springframework.context.event.internalEventListenerProcessor"; public static final String EVENT_LISTENER_FACTORY_BEAN_NAME = "org.springframework.context.event.internalEventListenerFactory"; public static Set&lt;BeanDefinitionHolder&gt; registerAnnotationConfigProcessors( BeanDefinitionRegistry registry, @Nullable Object source) &#123; DefaultListableBeanFactory beanFactory = unwrapDefaultListableBeanFactory(registry); if (beanFactory != null) &#123; if (!(beanFactory.getDependencyComparator() instanceof AnnotationAwareOrderComparator)) &#123; beanFactory.setDependencyComparator(AnnotationAwareOrderComparator.INSTANCE); &#125; if (!(beanFactory.getAutowireCandidateResolver() instanceof ContextAnnotationAutowireCandidateResolver)) &#123; beanFactory.setAutowireCandidateResolver(new ContextAnnotationAutowireCandidateResolver()); &#125; &#125; Set&lt;BeanDefinitionHolder&gt; beanDefs = new LinkedHashSet&lt;&gt;(8); if (!registry.containsBeanDefinition(CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME)) &#123; RootBeanDefinition def = new RootBeanDefinition(ConfigurationClassPostProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME)); &#125; if (!registry.containsBeanDefinition(AUTOWIRED_ANNOTATION_PROCESSOR_BEAN_NAME)) &#123; RootBeanDefinition def = new RootBeanDefinition(AutowiredAnnotationBeanPostProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, AUTOWIRED_ANNOTATION_PROCESSOR_BEAN_NAME)); &#125; // Check for JSR-250 support, and if present add the CommonAnnotationBeanPostProcessor. if (jsr250Present &amp;&amp; !registry.containsBeanDefinition(COMMON_ANNOTATION_PROCESSOR_BEAN_NAME)) &#123; RootBeanDefinition def = new RootBeanDefinition(CommonAnnotationBeanPostProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, COMMON_ANNOTATION_PROCESSOR_BEAN_NAME)); &#125; // Check for JPA support, and if present add the PersistenceAnnotationBeanPostProcessor. if (jpaPresent &amp;&amp; !registry.containsBeanDefinition(PERSISTENCE_ANNOTATION_PROCESSOR_BEAN_NAME)) &#123; RootBeanDefinition def = new RootBeanDefinition(); try &#123; def.setBeanClass(ClassUtils.forName(PERSISTENCE_ANNOTATION_PROCESSOR_CLASS_NAME, AnnotationConfigUtils.class.getClassLoader())); &#125; catch (ClassNotFoundException ex) &#123; throw new IllegalStateException( "Cannot load optional framework class: " + PERSISTENCE_ANNOTATION_PROCESSOR_CLASS_NAME, ex); &#125; def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, PERSISTENCE_ANNOTATION_PROCESSOR_BEAN_NAME)); &#125; if (!registry.containsBeanDefinition(EVENT_LISTENER_PROCESSOR_BEAN_NAME)) &#123; RootBeanDefinition def = new RootBeanDefinition(EventListenerMethodProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, EVENT_LISTENER_PROCESSOR_BEAN_NAME)); &#125; if (!registry.containsBeanDefinition(EVENT_LISTENER_FACTORY_BEAN_NAME)) &#123; RootBeanDefinition def = new RootBeanDefinition(DefaultEventListenerFactory.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, EVENT_LISTENER_FACTORY_BEAN_NAME)); &#125; return beanDefs; &#125;&#125; 这里用到了CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME、AUTOWIRED_ANNOTATION_PROCESSOR_BEAN_NAME等几个常量，在此列了一个表 常量 对应的BeanPostProcessor 对应的注解 CONFIGURATION_ANNOTATION_PROCESSOR_BEAN_NAME ConfigurationClassPostProcessor @Configuration AUTOWIRED_ANNOTATION_PROCESSOR_BEAN_NAME AutowiredAnnotationBeanPostProcessor @AutoWired REQUIRED_ANNOTATION_PROCESSOR_BEAN_NAME RequiredAnnotationBeanPostProcessor @Required COMMON_ANNOTATION_PROCESSOR_BEAN_NAME CommonAnnotationBeanPostProcessor @javax.annotation.PostConstruct、@javax.annotation.PreDestroy 2.2. &lt;context:component-scan/&gt;源码类似的，我们看ComponentScanBeanDefinitionParser的parse方法 123456789101112131415161718public class ComponentScanBeanDefinitionParser implements BeanDefinitionParser &#123; @Override @Nullable public BeanDefinition parse(Element element, ParserContext parserContext) &#123; String basePackage = element.getAttribute(BASE_PACKAGE_ATTRIBUTE); basePackage = parserContext.getReaderContext().getEnvironment().resolvePlaceholders(basePackage); String[] basePackages = StringUtils.tokenizeToStringArray(basePackage, ConfigurableApplicationContext.CONFIG_LOCATION_DELIMITERS); // Actually scan for bean definitions and register them. ClassPathBeanDefinitionScanner scanner = configureScanner(parserContext, element); Set&lt;BeanDefinitionHolder&gt; beanDefinitions = scanner.doScan(basePackages); // 重点 registerComponents(parserContext.getReaderContext(), beanDefinitions, element); return null; &#125;&#125; 简单来说，就是扫描属性base-package指定包下的类，然后注册，那重点就在registerComponents方法了。12345678910111213141516171819202122232425262728public class ComponentScanBeanDefinitionParser implements BeanDefinitionParser &#123; protected void registerComponents( XmlReaderContext readerContext, Set&lt;BeanDefinitionHolder&gt; beanDefinitions, Element element) &#123; Object source = readerContext.extractSource(element); CompositeComponentDefinition compositeDef = new CompositeComponentDefinition(element.getTagName(), source); for (BeanDefinitionHolder beanDefHolder : beanDefinitions) &#123; compositeDef.addNestedComponent(new BeanComponentDefinition(beanDefHolder)); &#125; // Register annotation config processors, if necessary. boolean annotationConfig = true; if (element.hasAttribute(ANNOTATION_CONFIG_ATTRIBUTE)) &#123; annotationConfig = Boolean.valueOf(element.getAttribute(ANNOTATION_CONFIG_ATTRIBUTE)); &#125; if (annotationConfig) &#123; // 跟&lt;context:annotation-config/&gt;一样，也调用了AnnotationConfigUtils.registerAnnotationConfigProcessors Set&lt;BeanDefinitionHolder&gt; processorDefinitions = AnnotationConfigUtils.registerAnnotationConfigProcessors(readerContext.getRegistry(), source); for (BeanDefinitionHolder processorDefinition : processorDefinitions) &#123; compositeDef.addNestedComponent(new BeanComponentDefinition(processorDefinition)); &#125; &#125; readerContext.fireComponentRegistered(compositeDef); &#125;&#125; 可以看出此方法就做了两件事，一是注册base-package下的类，二是调用AnnotationConfigUtils.registerAnnotationConfigProcessors 注册BeanPostProcessor，跟&lt;context:annotation-config/&gt;一模一样。到此就真相大白了]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 事务控制]]></title>
    <url>%2F2019%2F04%2F25%2FSpring%2FSpring-%E4%BA%8B%E5%8A%A1%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring AOP]]></title>
    <url>%2F2019%2F04%2F25%2FSpring%2FSpring-AOP%2F</url>
    <content type="text"><![CDATA[1. AOP简介1.1. 什么是AOPAOP：全称是Aspect Oriented Programming 即：面向切面编程 简单的说它就是把我们程序重复的代码抽取出来，在需要执行的时候，使用动态代理的技术，在不修改源码的基础上，对我们的已有方法进行增强 1.2. AOP的作用及优势作用：在程序运行期间，不修改源码对已有方法进行增强 优势：减少重复代码、提高开发效率、维护方便 1.3. AOP的实现原理动态代理。Spring会根据目标类是否实现了接口来决定采用哪种动态代理的方式。 1.4. AOP相关术语Joinpoint（连接点）：被拦截到的点。Spring只支持方法类型的连接点，所以连接点就是要拦截的方法。简单地讲，ServiceImpl中所有的方法，都是连接点 Pointcut（切入点）：被切面增强的连接点 Advice（通知/增强）：所谓通知是指拦截到Joinpoint之后所要做的事情就是通知。通知的类型：前置通知(before)，后置通知(after)，异常通知(afterThrowing)，返回通知(afterReturning)，环绕通知(around) Introduction（引介）：引介是一种特殊的通知在不修改类代码的前提下，Introduction可以在运行期为类动态地添加一些方法或Field Target（目标对象）：代理的目标对象，即被代理对象，比如ServiceImpl Weaving（织入）：是指把增强应用到目标对象来创建新的代理对象的过程。Spring采用动态代理织入，而AspectJ采用编译期织入和类装载期织入。 Proxy（代理）：一个类被AOP织入增强后，就产生一个结果代理类 Aspect（切面）：是切入点和通知（引介）的结合 连接点和切入点的区别： 连接点不一定是切入点，切入点一定是连接点 连接点是被拦截的方法，该方法可能会被增强，也可能不会，如果被切面增强，就是切入点 1.5. 学习Spring中的AOP要明确的事开发阶段（我们做的） 编写核心业务代码（开发主线）：大部分程序员来做，要求熟悉业务需求。 把公用代码抽取出来，制作成通知。（开发阶段最后再做） 在配置文件中，声明切入点与通知间的关系，即切面。 运行阶段（Spring框架完成的） Spring框架监控切入点方法的执行。一旦监控到切入点方法被运行，使用代理机制，动态创建目标对象的代理对象，根据通知类别，在代理对象的对应位置，将通知对应的功能织入，完成完整的代码逻辑运行。 1.6. 实际开发中切入点表达式的写法切到业务层实现类ServiceImpl下所有的方法 2. 切入点表达式切入点表达式指定要对哪些方法进行增强，即指定切入点 表达式形式：返回值 返回类型 包名.类名.方法名(形参类型列表) 2.1. public修饰符可省略切入点表达式的访问权限默认是public，所以以下两个表达式是等价的12public void demo.spring.service.UserServiceImpl.addUser()void demo.spring.service.UserServiceImpl.addUser() // 省略public 2.2. 形参类型列表参数是基本类型，直接写名称。如果是引用类型，则写全限定名12* *..*.*(int)* *..*.*(int, long, java.lang.String) 2.3. 使用通配符返回值类型可以使用通配符。12void demo.spring.service.UserServiceImpl.addUser() // 返回类型是void* demo.spring.service.UserServiceImpl.addUser() // 返回类型可以是任意类型 方法名可以使用通配符12* demo.spring.service.UserServiceImpl.addUser() // 匹配addUser()方法* demo.spring.service.UserServiceImpl.*() // 匹配所有空参方法 参数类型列表可以使用通配符，注意通配符是两个点.12* demo.spring.service.UserServiceImpl.*() // 匹配所有空参方法* demo.spring.service.UserServiceImpl.*(..) // 匹配所有方法（参数列表任意） 类名可以使用通配符123* demo.spring.service.UserServiceImpl.*(..) // 匹配UserServiceImpl类* demo.spring.service.*ServiceImpl.*(..) // 匹配以ServiceImpl名称结尾的类* demo.spring.service.*.*(..) // 匹配demo.spring.service包下的所有类 包名分隔符可以写作两个点.，表示当前包及其子包的通配12* demo.spring.service.*.*(..) // 匹配demo.spring.service包下的所有类* demo.spring.service..*.*(..) // 匹配demo.spring.service包及其子包下的所有类（递归匹配） 全通配写法 * *..*.*(..) 3. AOP各通知执行顺序AOP各通知执行逻辑如下： 1234567891011try &#123; try &#123; // @Before 前置通知 method.invoke(..); &#125; finally &#123; // @After 后置通知 &#125; // @AfterReturning 返回通知&#125; catch () &#123; // @AfterThrowing 异常通知&#125; 执行分两种情况： 执行正常：before、目标方法、after、afterReturning 执行异常：before、目标方法、after、afterThrowing 也就是说，before、目标方法、after这3个步骤是一定会执行的，最后一步是异常通知还是返回通知，取决于是否发生异常 4. AOP案例环境搭建4.1. 依赖123456789101112&lt;!-- Spring核心 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.1.5.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!-- AOP --&gt;&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.9.2&lt;/version&gt;&lt;/dependency&gt; 4.2. 目标对象Target1234public interface UserService &#123; void addUser(); void deleteUser();&#125; 1234567891011121314151617181920212223/** * UserServiceImpl是被增强的对象，即目标对象Target */public class UserServiceImpl implements UserService &#123; /* 连接点: 目标对象中，所有可以被增强的方法 * addUser()和deleteUser()就是连接点 * */ /* 切入点: 被切面增强的连接点。 * addUser()和deleteUser()都被增强了，所以都是切入点 * */ @Override public void addUser() &#123; System.out.println("目标方法: Adduser"); &#125; @Override public void deleteUser() &#123; System.out.println("目标方法: deleteuser"); int a = 1 / 0; // 制造异常 &#125;&#125; 4.3. 自定义切面类12345678910111213141516171819202122232425262728293031323334353637383940package demo.spring.springlearning.aspect;import org.aspectj.lang.JoinPoint;import org.aspectj.lang.ProceedingJoinPoint;public class MyAspect &#123; public void before(JoinPoint joinPoint) &#123; System.out.println("before"); &#125; public void after() &#123; System.out.println("after"); &#125; public Object around(ProceedingJoinPoint pjp) &#123; Object result = null; try &#123; try &#123; Object[] args = pjp.getArgs(); // 获取方法参数 System.out.println("前置通知"); result = pjp.proceed(args); // 调用目标方法 &#125; finally &#123; System.out.println("后置通知"); &#125; System.out.println("返回通知"); &#125; catch (Throwable t) &#123; System.out.println("异常通知"); &#125; return result; &#125; public void afterThrowing(JoinPoint joinPoint, Throwable e) &#123; System.out.println("afterThrowing " + e.getMessage()); &#125; public void afterReturning(JoinPoint joinPoint) &#123; System.out.println("afterReturning"); &#125;&#125; 4.4. 案例：前/后/异常/返回 通知编辑applicationContext.xml 1234567891011121314151617181920212223242526272829&lt;!-- 定义目标Bean --&gt;&lt;bean class="demo.spring.springlearning.service.impl.UserServiceImpl"/&gt;&lt;!-- 定义切面Bean --&gt;&lt;bean id="myAspect" class="demo.spring.springlearning.aspect.MyAspect"/&gt;&lt;!-- aop:config 声明AOP配置 --&gt;&lt;aop:config&gt; &lt;!-- aop:pointcut 配置切入点 --&gt; &lt;aop:pointcut id="pc" expression="execution(* demo.spring.springlearning.service.impl.UserServiceImpl.*(..))"/&gt; &lt;!-- aop:aspect配置切面 id: 给切面起一个标识，可以省略 ref: 指定切面Bean --&gt; &lt;aop:aspect id="userAdvice" ref="myAspect"&gt; &lt;!-- 前置通知 method: 指定切面的哪个方法作为前置通知 pointcut-ref: 指定&lt;aop:pointcut&gt;的id --&gt; &lt;aop:before method="before" pointcut-ref="pc"/&gt; &lt;!-- 后置通知 --&gt; &lt;aop:after method="after" pointcut-ref="pc"/&gt; &lt;!-- 返回通知 --&gt; &lt;aop:after-returning method="afterReturning" pointcut-ref="pc"/&gt; &lt;!-- 异常通知 --&gt; &lt;aop:after-throwing method="afterThrowing" pointcut-ref="pc" throwing="e"/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 测试12345678@Testpublic void test() &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("classpath:applicationContext.xml"); UserService userService = ioc.getBean(UserService.class); userService.addUser(); System.out.println("============================"); userService.deleteUser();&#125; 4.5. 案例：环绕通知环绕通知与其它通知，一般单独使用。环绕通知具体要做什么，由程序员决定，而不是由Spring来决定，所以灵活性最好。 实现方式是：在环绕通知切入点方法中，传入ProceedingJoinPoint参数。ProceedingJoinPoint是一个接口。你可以通过pjp.getArgs()获取目标方法的执行参数，通过pjp.proceed(args)执行目标方法。最后你要返回执行目标方法的返回值。 1234567891011121314151617public Object around(ProceedingJoinPoint pjp) &#123; Object result = null; try &#123; try &#123; Object[] args = pjp.getArgs(); // 获取方法参数 System.out.println("前置通知"); result = pjp.proceed(args); // 调用目标方法 &#125; finally &#123; System.out.println("后置通知"); &#125; System.out.println("返回通知"); &#125; catch (Throwable t) &#123; System.out.println("异常通知"); &#125; return result;&#125; 12345678910111213141516171819&lt;!-- 定义目标Bean --&gt;&lt;bean class="demo.spring.springlearning.service.impl.UserServiceImpl"/&gt;&lt;!-- 定义切面Bean --&gt;&lt;bean id="myAspect" class="demo.spring.springlearning.aspect.MyAspect"/&gt;&lt;!-- aop:config 声明AOP配置 --&gt;&lt;aop:config&gt; &lt;!-- aop:pointcut 配置切入点 --&gt; &lt;aop:pointcut id="pc" expression="execution(* demo.spring.springlearning.service.impl.UserServiceImpl.*(..))"/&gt; &lt;!-- aop:aspect配置切面 id: 给切面起一个标识，可以省略 ref: 指定切面Bean --&gt; &lt;aop:aspect id="userAdvice" ref="myAspect"&gt; &lt;!-- 环绕通知 --&gt; &lt;aop:around method="around" pointcut-ref="pc"/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 5. AOP配置详解5.1. 切入点的几种配置方式5.1.1. 通过pointcut属性设置切入点表达式123456789101112&lt;!-- 定义目标Bean --&gt;&lt;bean class="demo.spring.springlearning.service.impl.UserServiceImpl"/&gt;&lt;!-- 定义切面Bean --&gt;&lt;bean id="myAspect" class="demo.spring.springlearning.aspect.MyAspect"/&gt;&lt;!-- AOP配置 --&gt;&lt;aop:config&gt; &lt;aop:aspect id="userAdvice" ref="myAspect"&gt; &lt;!-- pointcut="切入点表达式" --&gt; &lt;aop:before method="before" pointcut="execution(* demo.spring.springlearning.service.impl.*.*(..))"/&gt; &lt;aop:after method="after" pointcut="execution(* demo.spring.springlearning.service.impl.*.*(..))"/&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 5.1.2. 通过pointcut-ref引用已配置的切入点多个通知的切入点表达式可能会重复，显得冗余。这时候可以用&lt;aop:pointcut&gt;单独定义一个切入点，在配置通知时，使用pointcut-ref来引用对应的切入点 1234567891011121314&lt;!-- 定义目标Bean --&gt;&lt;bean class="demo.spring.springlearning.service.impl.UserServiceImpl"/&gt;&lt;!-- 定义切面Bean --&gt;&lt;bean id="myAspect" class="demo.spring.springlearning.aspect.MyAspect"/&gt;&lt;!-- AOP配置 --&gt;&lt;aop:config&gt; &lt;aop:aspect id="userAdvice" ref="myAspect"&gt; &lt;!-- aop:pointcut 配置切入点 --&gt; &lt;aop:pointcut id="pc" expression="execution(* demo.spring.springlearning.service.impl.UserServiceImpl.*(..))"/&gt; &lt;!-- pointcut-ref 引用切入点 --&gt; &lt;aop:before method="before" pointcut-ref="pc" /&gt; &lt;aop:after method="after" pointcut-ref="pc" /&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; AOP可以配置多个&lt;aop:aspect&gt;切面，以上示例的&lt;aop:pointcut&gt;是配置在&lt;aop:aspect&gt;内部的，只有该切面内部的通知才能引用该切入点。如果想定义好的切入点可以被所有的通知引用，就把&lt;aop:pointcut&gt;放到外面。注意DTD约束要求&lt;aop:pointcut&gt;必须写在&lt;aop:aspect&gt;之后，否则会出错 1234567891011121314&lt;!-- 定义目标Bean --&gt;&lt;bean class="demo.spring.springlearning.service.impl.UserServiceImpl"/&gt;&lt;!-- 定义切面Bean --&gt;&lt;bean id="myAspect" class="demo.spring.springlearning.aspect.MyAspect"/&gt;&lt;!-- AOP配置 --&gt;&lt;aop:config&gt; &lt;!-- 将切入点放到外面。此时&lt;aop:pointcut&gt;必须在&lt;aop:aspect&gt;之前定义 --&gt; &lt;aop:pointcut id="pc" expression="execution(* demo.spring.springlearning.service.impl.UserServiceImpl.*(..))"/&gt; &lt;aop:aspect id="userAdvice" ref="myAspect"&gt; &lt;!-- pointcut-ref 引用切入点 --&gt; &lt;aop:before method="before" pointcut-ref="pc" /&gt; &lt;aop:after method="after" pointcut-ref="pc" /&gt; &lt;/aop:aspect&gt;&lt;/aop:config&gt; 6. AOP基于注解配置6.1. 注解定义Bean开启包扫描 1&lt;context:component-scan base-package="demo.spring.springlearning"/&gt; 使用@Service标识Target 12@Servicepublic class UserServiceImpl implements UserService &#123; 使用@Component标识自定义切面 12@Componentpublic class MyAspect &#123; 6.2. AOP注解配置开启AOP注解支持 1&lt;aop:aspectj-autoproxy/&gt; 使用@Aspect声明切面类。使用@Before等等定义对应的通知，value是切入点表达式 1234567891011121314151617181920212223@Component@Aspect // 声明当前类是一个切面类，相当于&lt;aop:config&gt;public class MyAspect &#123; @Before("execution(* demo.spring.springlearning.service.impl.UserServiceImpl.*(..))") public void before(JoinPoint joinPoint) &#123; System.out.println("before"); &#125; @After("execution(* demo.spring.springlearning.service.impl.UserServiceImpl.*(..))") public void after() &#123; System.out.println("after"); &#125; @AfterThrowing(value = "execution(* demo.spring.springlearning.service.impl.UserServiceImpl.*(..))", throwing = "e") public void afterThrowing(JoinPoint joinPoint, Throwable e) &#123; System.out.println("afterThrowing " + e.getMessage()); &#125; @AfterReturning("execution(* demo.spring.springlearning.service.impl.UserServiceImpl.*(..))") public void afterReturning(JoinPoint joinPoint) &#123; System.out.println("afterReturning"); &#125;&#125; 觉得切入点表达式重复冗余，可以使用@Pointcut定义一个单独的切入点 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Component@Aspect // 声明当前类是一个切面类，相当于&lt;aop:config&gt;public class MyAspect &#123; /** * 定义一个切入点表达式，相当于&lt;aop:pointcut&gt; */ @Pointcut("execution(* demo.spring.springlearning.service.impl.UserServiceImpl.*(..))") public void pt() &#123; &#125; /** 引用切入点 */ @Before("pt()") public void before(JoinPoint joinPoint) &#123; System.out.println("before"); &#125; @After("pt()") public void after() &#123; System.out.println("after"); &#125; // @Around("pt()") public Object around(ProceedingJoinPoint pjp) &#123; Object result = null; try &#123; try &#123; Object[] args = pjp.getArgs(); // 获取方法参数 System.out.println("前置通知"); result = pjp.proceed(args); // 调用目标方法 &#125; finally &#123; System.out.println("后置通知"); &#125; System.out.println("返回通知"); &#125; catch (Throwable t) &#123; System.out.println("异常通知"); &#125; return result; &#125; @AfterThrowing(value = "pt()", throwing = "e") public void afterThrowing(JoinPoint joinPoint, Throwable e) &#123; System.out.println("afterThrowing " + e.getMessage()); &#125; @AfterReturning("pt()") public void afterReturning(JoinPoint joinPoint) &#123; System.out.println("afterReturning"); &#125;&#125; 6.3. AOP完全注解配置（不使用XML）定义Spring配置类 1234567@Configuration// 相当于&lt;context:component-scan base-package="demo.spring.springlearning"/&gt;@ComponentScan(basePackages = "demo.spring.springlearning")// 相当于&lt;aop:aspectj-autoproxy/&gt;@EnableAspectJAutoProxypublic class SpringConfig &#123;&#125; 测试 12345678@Testpublic void test() &#123; ApplicationContext ioc = new AnnotationConfigApplicationContext(SpringConfig.class); UserService userService = ioc.getBean(UserService.class); userService.addUser(); System.out.println("============================"); userService.deleteUser();&#125;]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC 请求转发和重定向]]></title>
    <url>%2F2019%2F04%2F24%2FSpringMVC%2FSpringMVC-%E8%AF%B7%E6%B1%82%E8%BD%AC%E5%8F%91%E5%92%8C%E9%87%8D%E5%AE%9A%E5%90%91%2F</url>
    <content type="text"><![CDATA[1. 请求转发1.1. 返回String方式1.1.1. 转发给另一个请求123456789101112131415@Controllerpublic class HelloController &#123; @GetMapping("/hello1") public String hello1() &#123; System.out.println("hello1"); return "forward:/hello2"; &#125; @GetMapping("/hello2") @ResponseBody public String hello2() &#123; System.out.println("hello2"); return "&lt;h1&gt;helloworld&lt;/h1&gt;"; &#125;&#125; 1.1.2. 转发到指定jsp页面不带forward前缀，直接返回字符串。该字符串会拼接视图解析器的前后缀，例如前缀是/WEB-INF/jsp，后缀是.jsp，那么success对应的页面是/WEB-INF/jsp/success.jsp12345678@Controllerpublic class HelloController &#123; @GetMapping("/hello1") public String hello1() &#123; // return "success"; &#125;&#125; 如果设置了视图解析器，前缀是/WEB-INF/jsp，后缀是.jsp，但是你又想转发给/success.jsp，可以使用相对路径回退到上级目录12345678@Controllerpublic class HelloController &#123; @GetMapping("/hello1") public String hello1() &#123; // 拼接得到 /WEB-INF/jsp/../../success.jsp，相当于/success.jsp return "../../success"; &#125;&#125; 也可以使用带forward前缀的形式，转发到指定jsp资源。注意这里不要使用forward:success，因为转发时不会加入视图解析器的前后缀。forward:success是一种相对路径的写法，当前映射是/hello1，那么相对路径success对应的绝对路径就是/success，即等价于forward:/success 1234567@Controllerpublic class HelloController &#123; @GetMapping("/hello1") public String hello1() &#123; return "forward:/WEB-INF/jsp/success.jsp"; &#125;&#125; 2. 请求重定向2.1. 返回String方式在使用原生Servlet进行重定向时，路径要带上虚拟目录，但是SpringMVC不用，SpringMVC会自动帮你补全虚拟目录。注意，redirect也不会拼接视图解析器的前后缀 1234567891011121314@Controllerpublic class HelloController &#123; @GetMapping("/hello1") public String hello1() &#123; return "redirect:/hello2"; &#125; @GetMapping("/hello2") @ResponseBody public String hello2() &#123; System.out.println("hello2"); return "&lt;h1&gt;helloworld&lt;/h1&gt;"; &#125;&#125;]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[业务 七牛云图片上传]]></title>
    <url>%2F2019%2F04%2F24%2F%E4%B8%9A%E5%8A%A1%2F%E4%B8%9A%E5%8A%A1-%E4%B8%83%E7%89%9B%E4%BA%91%E5%9B%BE%E7%89%87%E4%B8%8A%E4%BC%A0%2F</url>
    <content type="text"><![CDATA[1. 七牛云OSS配置1.1. 创建存储空间 存储空间名称：自定义 存储区域：与服务器在同一区域即可，保证网络通畅 访问空间：公开空间谁都可以访问，私有空间需要身份认证才能访问。如果作为图片服务器，谁都能看，就选择公开空间 2. 七牛云图片上传实现原理2.1. 资源路径说明七牛云通过存储空间名+文件key来唯一定位一个资源。 常用的key有几种形式： UUID 原始文件名（非中文） 自定义的具有唯一性的字符串 key由用户上传时决定 上传之后，资源的URL路径为 http://外链域名/key 2.2. 上传原理七牛云图片上传分为两步 用账号的AccessKey和SecretKey连接七牛服务器进行身份校验，成功则得到token 用token就可以上传资源 2.3. 上传实现方式实现上传有几种思路： 纯后端实现：前端将图片上传到后端，再由后端将图片上传到七牛云 纯前端实现：前端直接将图片上传到七牛云 前后端结合：后端提供获取token的接口，前端获取token之后，再根据token直接将图片上传到七牛云 纯后端实现，安全，但是服务器压力大。纯前端实现，前端要知道AccessKey和SecretKey，又不安全。 前后端结合的方式是最佳的。前端不需要知道AccessKey和SecretKey是什么，只需要访问后端接口，由后端去获取token，再将token返回给前端，最后由前端上传图片。这样既安全又高效 2.4. HTTP上传（表单上传）接口前端通过HTTP接口上传，具体文档可以查询 七云开发者OSS文档-&gt;上传资源-&gt;表单上传 https://developer.qiniu.com/kodo/manual/1272/form-upload HTML表单示例 12345678910&lt;form method="post" action="http://upload.qiniup.com/" enctype="multipart/form-data"&gt; &lt;input name="key" type="hidden" value="&lt;resource_key&gt;"&gt; &lt;input name="x:&lt;custom_name&gt;" type="hidden" value="&lt;custom_value&gt;"&gt; &lt;input name="token" type="hidden" value="&lt;upload_token&gt;"&gt; &lt;input name="crc32" type="hidden" /&gt; &lt;input name="accept" type="hidden" /&gt; &lt;input name="file" type="file" /&gt; &lt;input type="submit" value="上传文件" /&gt;&lt;/form&gt; 上传时的参数如下 名称 类型 必填 说明 action string 是 上传地址，可参考存储区域 resource_key string 否 资源名，必须是 UTF-8 编码。注意： 如果上传凭证中 scope 指定为 :， 则该字段也必须指定。 custom_name string 否 自定义变量的名字，不限个数。 custom_value string 否 自定义变量的值。 upload_token string 是 必须是一个符合相应规格的上传凭证，否则会返回 401 表示权限认证失败。 crc32 string 否 上传内容的 crc32 校验码。如填入，则七牛服务器会使用此值进行内容检验。 accept string 否 当 HTTP 请求指定 accept 头部时，七牛会返回 content-type 头部的值。该值用于兼容低版本 IE 浏览器行为。低版本 IE 浏览器在表单上传时，返回 application/json 表示下载，返回 text/plain 才会显示返回内容。 file file 是 文件本身。二进制数据 action是上传地址，不算参数。主要考虑resource_key、upload_token、file这3个参数 3. SpringMVC后端实现3.1. 依赖123456789101112&lt;!-- SpringMVC --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- qiniu --&gt;&lt;dependency&gt; &lt;groupId&gt;com.qiniu&lt;/groupId&gt; &lt;artifactId&gt;qiniu-java-sdk&lt;/artifactId&gt; &lt;version&gt;[7.2.0, 7.2.99]&lt;/version&gt;&lt;/dependency&gt; 3.2. qiniu.properties 七云配置文件123456# 七牛账号的accessKey qiniu.accessKey=XXXXXXXXXXX-XXXX-XXXXXXXXXXXXXXXXXXXXXXX# 七牛账号的secretKeyqiniu.secretKey=BBB-BBBB-BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB# OSS存储空间名qiniu.bucket=image 3.3. QiniuTokenVO 响应返回的VO封装key和token结果，返回给客户端 1234567@Data@NoArgsConstructor@AllArgsConstructorpublic class QiniuTokenVO &#123; private String key; private String token;&#125; 3.4. QiniuUtils 获取Token的工具类123456789101112131415161718192021222324252627282930313233343536public class QiniuUtils &#123; private static String ACCESS_KEY; private static String SECRET_KEY; private static String BUCKET; static &#123; try &#123; // 从配置文件中读取 InputStream in = QiniuUtils.class.getClassLoader().getResourceAsStream("properties/qiniu.properties"); Properties properties = new Properties(); properties.load(new InputStreamReader(in, "UTF-8")); ACCESS_KEY = (String) properties.get("qiniu.accessKey"); SECRET_KEY = (String) properties.get("qiniu.secretKey"); BUCKET = (String) properties.get("qiniu.bucket"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; /** 获取上传所需的token */ public static QiniuTokenVO getUpToken() &#123; Auth auth = Auth.create(ACCESS_KEY, SECRET_KEY); String key = UUID.randomUUID().toString(); // uploadToken(bucket) 只指定将资源上传到哪个bucket（存储空间） /// String upToken = auth.uploadToken(BUCKET); // uploadToken(bucket, key) 指定bucket和key。这样在上传资源时，限定资源名必须是key，否则验证不通过 String upToken = auth.uploadToken(bucket, key); return new QiniuTokenVO(key, upToken); &#125; public static void main(String[] args) &#123; // 测试 QiniuTokenVO upToken = getUpToken(); System.out.println(upToken); &#125;&#125; 3.5. 处理器1234567@RestControllerpublic class QiniuController &#123; @GetMapping("/qiniu/upload/token") public QiniuTokenVO uptoken() &#123; return QiniuUtils.getUpToken(); &#125;&#125; 4. Vue前端实现4.1. @/views/qiniu/upload.vue 图片上传组件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;template&gt; &lt;el-upload :data="dataObj" action="https://upload.qbox.me" :before-upload="beforeUpload" :on-success="handleUploadSuccess" :on-progress="handleUploadProgess" :show-file-list="false" accept=".jpg,.jpeg,.png,.gif,.bmp,,.JPG,.JPEG,.PNG,.GIF,.BMP" drag&gt; &lt;!-- 如果上传成功，则显示图片 --&gt; &lt;img v-if="imageUrl" :src="imageUrl" width="100%" height="100%"&gt; &lt;!-- 否则显示上传图片的提示 --&gt; &lt;div v-else v-loading="loading"&gt; &lt;i class="el-icon-upload" /&gt; &lt;div class="el-upload__text"&gt;将文件拖到此处，或&lt;em&gt;点击上传&lt;/em&gt;&lt;/div&gt; &lt;/div&gt; &lt;/el-upload&gt;&lt;/template&gt;&lt;script&gt;export default &#123; props: &#123; // 父子组件v-model双向绑定，名称必须是value value: &#123; type: String, default: '' &#125; &#125;, computed: &#123; // 相当于将value重命名为imageUrl imageUrl() &#123; return this.value &#125; &#125;, data() &#123; return &#123; // 上传到七牛云，需要token和key参数 dataObj: &#123; token: '', key: '' &#125;, // 图片加载 loading: false, &#125; &#125;, methods: &#123; beforeUpload() &#123; this.$emit('input', '') const _self = this return new Promise((resolve, reject) =&gt; &#123; this.axios.get('/qiniu/upload/token').then(response =&gt; &#123; _self._data.dataObj.token = response.data.token _self._data.dataObj.key = response.data.key resolve(true) &#125;).catch(err =&gt; &#123; console.log(err) reject(false) &#125;) &#125;) &#125;, handleUploadProgess(event, file) &#123; this.loading = true &#125;, handleUploadSuccess(response, file) &#123; this.loading = false // 上传成功，将父子组件双向绑定的值修改，格式为 this.$emit('input', 新值)response.key是文件的key，拼接上图片的URL前缀，就得到完整的URL this.$emit('input', "http://pq6yxn7wg.bkt.clouddn.com/" + response.key) console.log('response: ', response) console.log('file: ', file) &#125;, &#125;&#125;&lt;/script&gt; 4.2. 图片上传测试页1234567891011121314151617181920212223&lt;template&gt; &lt;div class="home" &gt; &lt;!-- v-model父子组件双向绑定通信 --&gt; &lt;Upload v-model="logoUrl" /&gt; &lt;/div&gt;&lt;/template&gt;&lt;script&gt;// 导入图片上传组件import Upload from '@/views/qiniu/upload.vue'export default &#123; name: 'home', components: &#123; Upload &#125;, data() &#123; return &#123; logoUrl: '', &#125; &#125;,&#125;&lt;/script&gt; 5. HTML前端表单提交实现表单方式实现如下 12345678910111213141516&lt;script src="https://cdn.bootcss.com/jquery/3.4.0/jquery.min.js"&gt;&lt;/script&gt;&lt;script&gt; $(() =&gt; &#123; $.get('http://localhost:8080/qiniu/upload/token', data =&gt; &#123; $('#key').val(data.key) $('#token').val(data.token) console.log(data.key, data.token) &#125;) &#125;)&lt;/script&gt;&lt;form method="post" action="http://upload.qiniup.com/" enctype="multipart/form-data"&gt; &lt;input id="key" name="key" type="hidden"&gt; &lt;input id="token" name="token" type="hidden"&gt; &lt;input name="file" type="file" /&gt; &lt;input type="submit" value="提交" /&gt;&lt;/form&gt;]]></content>
      <tags>
        <tag>业务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interview Spring]]></title>
    <url>%2F2019%2F04%2F24%2FInterview%2FInterview-Spring%2F</url>
    <content type="text"><![CDATA[1. 什么是SpringSpring是用于构建企业级应用的轻量级一站式框架 一站是指Spring提供了表现层（SpringMVC）到业务层（Spring）再到持久层（SpringData）的全套解决方案 轻量级是相对于其它重量级容器而言的。Spring的核心包很小，但是EJB很大，EJB在启动时需要消耗大量的内存，CPU 2. 谈谈你对Spring IoC和DI的理解，它们有什么区别所谓IoC就是反转了对象的创建方式和依赖注入方式，以前对象的创建由开发人员自己维护，依赖关系（类成员有其它类）也是自己注入，现在可以反转给Spring去处理 3. BeanFactory接口和ApplicationContext接口有什么区别ApplicationContext接口继承BeanFactory接口。它们都是bean工厂，或者说bean容器（IOC容器）。它按照我们的要求，生产我们需要的各种各样的bean，提供给我们使用。只是在生产bean的过程中，需要解决bean之间的依赖问题，才引入了依赖注入(DI)这种技术。也就是说依赖注入是beanFactory生产bean时为了解决bean之间的依赖的一种技术而已 BeanFactory是Spring原始的接口。具有的功能非常单一。实现BeanFactory的类，特点是在获取bean时才创建bean（采取延迟加载）。这是因为BeanFactory出现的年代较早，当时内存资源匮乏，用到bean再创建bean ApplicationContext是Spring的新型接口。具有较多的功能。ApplicationContext容器在启动时就会创建所有配置的对象，而不是等到用户获取bean时再创建 4. IOC的优点是什么IOC 或 依赖注入把应用的代码量降到最低。它使应用容易测试，单元测试不再需要单例和JNDI查找机制。最小的代价和最小的侵入性使松散耦合得以实现。IOC容器支持加载服务时的饿汉式初始化和懒加载。 5. Spring框架中的单例Bean是线程安全的吗Spring框架并没有对单例bean进行任何多线程的封装处理。关于单例bean的线程安全和并发问题需要开发者自行去搞定。但实际上，大部分的单例Bean并没有可变的状态（比如Serview类和Dao类），所以在某种程度上说Spring的单例bean是线程安全的。如果你的bean有多种状态的话（比如 View Model 对象），就需要自行保证线程安全。 6. 在Spring中如何注入一个Java集合？Spring提供以下几种集合的配置元素： &lt;list&gt;类型用于注入一列值，允许有相同的值。 &lt;set&gt; 类型用于注入一组值，不允许有相同的值。 &lt;map&gt; 类型用于注入一组键值对，键和值都可以为任意类型。 &lt;props&gt;类型用于注入一组键值对，键和值都只能为String类型。]]></content>
      <tags>
        <tag>Interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shiro 集成Spring]]></title>
    <url>%2F2019%2F04%2F24%2FJava%2FShiro%2FShiro-%E9%9B%86%E6%88%90Spring%2F</url>
    <content type="text"><![CDATA[1. 环境搭建1.1. 依赖12 1.2. 配置Springweb.xml 配置监听器，初始化Spring容器 12345678&lt;!-- Spring --&gt;&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext-*.xml&lt;/param-value&gt;&lt;/context-param&gt;&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt; 1.3. 配置SpringMVCweb.xml 配置DispatcherServlet，以及常用的Filter 1234567891011121314151617181920212223242526&lt;filter&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;utf-8&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&lt;servlet&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; springmvc.xml 1234567891011121314&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:mvc="http://www.springframework.org/schema/mvc" xmlns:mv="http://www.springframework.org/schema/mvc" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc http://www.springframework.org/schema/mvc/spring-mvc.xsd"&gt; &lt;context:component-scan base-package="demo.shiro.controller"/&gt; &lt;mvc:default-servlet-handler/&gt; &lt;mvc:annotation-driven/&gt; &lt;mvc:cors&gt; &lt;mvc:mapping path="/**"/&gt; &lt;/mvc:cors&gt;&lt;/beans&gt; 1.4. 配置Shiro的Filter先在web.xml中，配置DelegatingFilterProxy 12345678910111213&lt;filter&gt; &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.DelegatingFilterProxy&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;targetFilterLifecycle&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;shiroFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 在applicationContext.xml中注册Bean 12 1.5. 注册Shrio相关的Bean编辑 applicationContext.xml1234&lt;!-- 配置CacheManager --&gt;&lt;bean id="cacheManager" class="org.apache.shiro.cache.ehcache.EhCacheManager"&gt; &lt;property name="cacheManagerConfigFile" value="classpath:ehcache.xml"/&gt;&lt;/bean&gt; 再添加ehcache的jar和配置文件ehcache.xml 1.6. 配置Realm123&lt;!-- 配置realm，实际是实现了Realm接口的bean --&gt;&lt;bean id="jdbcRealm" class="demo.shiro.realm.ShiroRealm"&gt;&lt;/bean&gt; 1.7. 配置LifecycleBeanPostProcessor可以自动调用在Shiro Bean的生命周期方法1&lt;bean id="lifecycleBeanPostProcessor" class="org.apache.shiro.spring.LifecycleBeanPostProcessor"/&gt;]]></content>
      <tags>
        <tag>Shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shiro HelloWorld]]></title>
    <url>%2F2019%2F04%2F23%2FJava%2FShiro%2FShiro-HelloWorld%2F</url>
    <content type="text"><![CDATA[1. HelloWorld案例1.1. 依赖123456789101112&lt;!-- shiro核心包 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.shiro&lt;/groupId&gt; &lt;artifactId&gt;shiro-core&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 必须加入commons-logging，否则报错 --&gt;&lt;dependency&gt; &lt;groupId&gt;commons-logging&lt;/groupId&gt; &lt;artifactId&gt;commons-logging&lt;/artifactId&gt; &lt;version&gt;1.2&lt;/version&gt;&lt;/dependency&gt; 1.2. log4j.properties123456789101112131415161718log4j.rootLogger=INFO, stdoutlog4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m %n# General Apache librarieslog4j.logger.org.apache=WARN# Springlog4j.logger.org.springframework=WARN# Default Shiro logginglog4j.logger.org.apache.shiro=TRACE# Disable verbose logginglog4j.logger.org.apache.shiro.util.ThreadContext=WARNlog4j.logger.org.apache.shiro.cache.ehcache.EhCache=WARN 1.3. shiro.ini123456789101112131415161718192021222324252627[users]# user 'root' with password 'secret' and the 'admin' roleroot = secret, admin# user 'guest' with the password 'guest' and the 'guest' roleguest = guest, guest# user 'presidentskroob' with password '12345' ("That's the same combination on# my luggage!!!" ;)), and role 'president'presidentskroob = 12345, president# user 'darkhelmet' with password 'ludicrousspeed' and roles 'darklord' and 'schwartz'darkhelmet = ludicrousspeed, darklord, schwartz# user 'lonestarr' with password 'vespa' and roles 'goodguy' and 'schwartz'lonestarr = vespa, goodguy, schwartz# -----------------------------------------------------------------------------# Roles with assigned permissions# # Each line conforms to the format defined in the# org.apache.shiro.realm.text.TextConfigurationRealm#setRoleDefinitions JavaDoc# -----------------------------------------------------------------------------[roles]# 'admin' role has all permissions, indicated by the wildcard '*'admin = *# The 'schwartz' role can do anything (*) with any lightsaber:schwartz = lightsaber:*# The 'goodguy' role is allowed to 'delete' (action) the user (type) with# license plate 'zhangsan' (instance specific id)goodguy = user:delete:zhangsan 1.4. 官方HelloWorld实例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889package demo.shiro;import org.apache.shiro.SecurityUtils;import org.apache.shiro.authc.*;import org.apache.shiro.config.IniSecurityManagerFactory;import org.apache.shiro.mgt.SecurityManager;import org.apache.shiro.session.Session;import org.apache.shiro.subject.Subject;import org.apache.shiro.util.Factory;import org.slf4j.Logger;import org.slf4j.LoggerFactory;public class Quickstart &#123; private static final transient Logger log = LoggerFactory.getLogger(Quickstart.class); public static void main(String[] args) &#123; // 通过shiro.ini，得到SecurityManager工厂类 Factory&lt;SecurityManager&gt; factory = new IniSecurityManagerFactory("classpath:shiro.ini"); // 获得SecurityManager实例 SecurityManager securityManager = factory.getInstance(); // 使得SecurityManager变为单例，大多应用不会这样做 SecurityUtils.setSecurityManager(securityManager); // 获取当前的Subject Subject currentUser = SecurityUtils.getSubject(); // ========== 测试使用Session，不需要Web或者EJB容器 Session session = currentUser.getSession(); session.setAttribute("someKey", "aValue"); String value = (String) session.getAttribute("someKey"); if (value.equals("aValue")) &#123; log.info("Retrieved the correct value! [" + value + "]"); &#125; // currentUser.isAuthenticated()测试当前用户是否已经被认证，即是否已经登录 if (!currentUser.isAuthenticated()) &#123; // 传入用户名和密码，得到UsernamePasswordToken对象。这里的用户名和密码是在shiro.ini中配置的 UsernamePasswordToken token = new UsernamePasswordToken("lonestarr", "vespa"); token.setRememberMe(true); try &#123; // 执行登录 currentUser.login(token); &#125; catch (UnknownAccountException uae) &#123; // 如果该用户不存在，则抛出UnknownAccountException log.info("There is no user with username of " + token.getPrincipal()); &#125; catch (IncorrectCredentialsException ice) &#123; // 如果密码不对，则抛出IncorrectCredentialsException log.info("Password for account " + token.getPrincipal() + " was incorrect!"); &#125; catch (LockedAccountException lae) &#123; // 如果用户被锁定，则抛出LockedAccountException log.info("The account for username " + token.getPrincipal() + " is locked. " + "Please contact your administrator to unlock it."); &#125; catch (AuthenticationException ae) &#123; // AuthenticationException代表认证异常，是以上所有异常的父类 &#125; &#125; log.info("User [" + currentUser.getPrincipal() + "] logged in successfully."); // 测试是否存在某个角色 if (currentUser.hasRole("schwartz")) &#123; log.info("May the Schwartz be with you!"); &#125; else &#123; log.info("Hello, mere mortal."); &#125; // 测试用户是否具有某个操作的权限 if (currentUser.isPermitted("lightsaber:wield")) &#123; log.info("You may use a lightsaber ring. Use it wisely."); &#125; else &#123; log.info("Sorry, lightsaber rings are for schwartz masters only."); &#125; // 测试用户是否具有操作某个对象的权限 if (currentUser.isPermitted("winnebago:drive:eagle5")) &#123; log.info("You are permitted to 'drive' the winnebago with license plate (id) 'eagle5'. " + "Here are the keys - have fun!"); &#125; else &#123; log.info("Sorry, you aren't allowed to drive the 'eagle5' winnebago!"); &#125; // 退出登录 currentUser.logout(); System.exit(0); &#125;&#125;]]></content>
      <tags>
        <tag>Shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shiro 概述]]></title>
    <url>%2F2019%2F04%2F23%2FJava%2FShiro%2FShiro-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1. 什么是权限在做一个系统时，通常会涉及到权限问题。不同的人登录，能使用的功能、按钮、菜单是不一样的，这就是权限 2. Shiro简介Apache Shiro是Java的一个安全（权限）框架。与之类似的还有Spring Security，但是Shiro更加主流、更加简单应用。 Shiro可以非常容易的开发出足够好的应用，其不仅可以用在JavaSE环境，也可以用在JavaEE环境。 Shiro可以完成：认证、授权、加密、会话管理、与Web 集成、缓存等。 官网下载：http://shiro.apache.org/ 3. Shiro功能基本功能点如下图所示： Authentication（认证）：验证用户是不是拥有相应的身份，说白了就是登录。我们可以利用Shiro帮我们完成登录，密码匹配就是由Shiro来完成的 Authorization（授权）：当我们点击一个链接、按钮时，Shiro会判断你是否有这个权限 Session Management（会话管理）：负责管理Session。用户登录后就是一次会话，在没有退出之前，它的所有信息都在会话中；会话可以是普通JavaSE环境，也可以是Web环境的。在Web环境下，我们可以使用HttpSession。使用Shiro框架，即使在非Web环境下，我们也可以使用Session，这个Session是由Shiro提供的 Cryptography（加密）：对密码进行加密 Web Support（Web支持）：对Web应用提供支持。Shiro可以很容易与JavaEE Web应用集成 Concurrency：Shiro支持多线程应用的并发验证，即如在一个线程中开启另一个线程，能把权限自动传播过去 Testing：提供测试支持 Caching（缓存）：Shiro提供了缓存机制，加速运行速度 Run As：已登录用户以另外一个用户身份来操作当前项目和系统 Remember Me：记住我，这个是非常常见的功能，即一次登录后，下次再来的话不用登录了 4. Shiro架构4.1. 从外部看从外部来看Shiro，即从应用程序角度的来观察如何使用Shiro完成工作 主要有以下3个组件： Subject：应用代码直接交互的对象是Subject，也就是说Shiro的对外API 核心就是Subject。Subject 代表了当前“用户”，这个用户不一定是一个具体的人，与当前应用交互的任何东西都是Subject，如网络爬虫，机器人等；与Subject 的所有交互都会委托给SecurityManager；Subject 其实是一个门面，SecurityManager才是实际的执行者； SecurityManager：安全管理器；即所有与安全有关的操作都会与SecurityManager交互；且其管理着所有Subject；可以看出它是Shiro的核心，它负责与Shiro的其他组件进行交互，它相当于SpringMVC中DispatcherServlet的角色 Realm：Shiro从Realm 获取安全数据（如用户、角色、权限），就是说SecurityManager要验证用户身份，那么它需要从Realm 获取相应的用户进行比较以确定用户身份是否合法；也需要从Realm 得到用户相应的角色/权限进行验证用户是否能进行操作；可以把Realm 看成DataSource Application Code 就是我们的Java应用程序。应用程序去访问Shiro，一定是访问Subject这个组件 Subject实际上是一个门面，真正的核心是SecurityManager（负责管理所有的Subject） 当我们需要访问一些安全数据时，比方说获取用户信息、权限信息，需要用到Realm，相当于SecurityDao 4.2. 从内部看 Subject：任何可以与应用交互的“用户”； SecurityManager：相当于SpringMVC中的DispatcherServlet；是Shiro的心脏；所有具体的交互都通过SecurityManager进行控制；它管理着所有Subject、且负责进行认证、授权、会话及缓存的管理。 Authenticator：负责Subject 认证，是一个扩展点，可以自定义实现；可以使用认证策略（Authentication Strategy），即什么情况下算用户认证通过了； Authorizer：授权器、即访问控制器，用来决定主体是否有权限进行相应的操作；即控制着用户能访问应用中的哪些功能； Realm：可以有1 个或多个Realm，可以认为是安全实体数据源，即用于获取安全实体的；可以是JDBC 实现，也可以是内存实现等等；由用户提供；所以一般在应用中都需要实现自己的Realm； SessionManager：管理Session 生命周期的组件；而Shiro并不仅仅可以用在Web 环境，也可以用在如普通的JavaSE环境 CacheManager：缓存控制器，来管理如用户、角色、权限等的缓存的；因为这些数据基本上很少改变，放到缓存中后可以提高访问的性能 Cryptography：密码模块，Shiro提高了一些常见的加密组件用于如密码加密/解密。]]></content>
      <tags>
        <tag>Shiro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn 决策树]]></title>
    <url>%2F2019%2F04%2F22%2FMachineLearning%2Fsklearn-%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[1. 决策树 iris案例]]></content>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn 朴素贝叶斯]]></title>
    <url>%2F2019%2F04%2F22%2FMachineLearning%2Fsklearn-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[1. 朴素贝叶斯 iris案例12345678910111213141516171819202122232425262728293031323334353637from sklearn import datasetsfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import MinMaxScalerfrom sklearn.svm import SVCfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB""" 加载数据"""iris = datasets.load_iris()X_data, y_data = iris.data, iris.targetX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.25)""" 特征归一化"""scaler = MinMaxScaler()X_train = scaler.fit_transform(X_train)X_test = scaler.transform(X_test)def test(clf): """ 训练 """ clf.fit(X_train, y_train) """ 评估 """ print('=' * 60) print("train score: ", clf.score(X_train, y_train)) print("test score: ", clf.score(X_test, y_test))test(MultinomialNB()) # 多项式模型test(GaussianNB()) # 高斯模型test(BernoulliNB()) # Bernoulli模型]]></content>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn SVM]]></title>
    <url>%2F2019%2F04%2F22%2FMachineLearning%2Fsklearn-SVM%2F</url>
    <content type="text"><![CDATA[1. SVM iris案例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748from sklearn import datasetsfrom sklearn.model_selection import GridSearchCVfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.svm import SVC""" 加载数据"""iris = datasets.load_iris()X_data, y_data = iris.data, iris.targetX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.25)""" 特征归一化"""scaler = StandardScaler()X_train = scaler.fit_transform(X_train)X_test = scaler.transform(X_test)""" 训练"""clf = SVC()clf.fit(X_train, y_train)""" 评估"""print("train score: ", clf.score(X_train, y_train))print("test score: ", clf.score(X_test, y_test))""" 网格搜索"""param_grid = [ &#123; 'kernel': ['rbf', 'poly', 'linear'], 'C': [0.01, 0.1, 1], 'gamma': [0.125, 0.25, 0.5, 1, 2, 4] &#125;]clf = SVC()grid_search = GridSearchCV(clf, param_grid)grid_search.fit(X_train, y_train)print("Best set score: &#123;:.2f&#125;".format(grid_search.best_score_))print("Best parameters: &#123;&#125;".format(grid_search.best_params_))print("Test set score: &#123;:.2f&#125;".format(grid_search.score(X_test, y_test))) 2. SVC参数（1）C: 目标函数的惩罚系数C，用来平衡分类间隔margin和错分样本的，default C = 1.0；（2）kernel：参数选择有RBF, Linear, Poly, Sigmoid, 默认的是”RBF”;（3）degree：if you choose ‘Poly’ in param 2, this is effective, degree决定了多项式的最高次幂；（4）gamma：核函数的系数(‘Poly’, ‘RBF’ and ‘Sigmoid’), 默认是gamma = 1 / n_features;（5）coef0：核函数中的独立项，’RBF’ and ‘Poly’有效；（6）probablity: 可能性估计是否使用(true or false)；（7）shrinking：是否进行启发式；（8）tol（default = 1e - 3）: svm结束标准的精度;（9）cache_size: 制定训练所需要的内存（以MB为单位）；（10）class_weight: 每个类所占据的权重，不同的类设置不同的惩罚参数C, 缺省的话自适应；（11）verbose: 跟多线程有关，不大明白啥意思具体；（12）max_iter: 最大迭代次数，default = 1， if max_iter = -1, no limited;（13）decision_function_shape ： ‘ovo’ 一对一, ‘ovr’ 多对多 or None 无, default=None（14）random_state ：用于概率估计的数据重排时的伪随机数生成器的种子。]]></content>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python 文件目录操作]]></title>
    <url>%2F2019%2F04%2F22%2Fpython%2Fpython-%E6%96%87%E4%BB%B6%E7%9B%AE%E5%BD%95%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1. 判断文件是否存在123import ospath = r"C:\1.txt"print(os.path.exists(path)) 2. 遍历目录12345678910111213import osdirname = r"C:\output"for filename in os.listdir(dirname): filepath = os.path.join(dirname, filename) print('=' * 60) print('文件名: ', filename) print('文件路径：', filepath) # 可以继续对文件做一些IO操作 with open(filepath, 'r', encoding='utf-8') as f: pass 3. 文件的打开和关闭方式11234filepath = r"C:\1.txt"f = open(filepath, 'r', encoding='utf-8') # 打开文件# TODOf.close() # 关闭文件 方式2：使用with语句，执行完后会自动关闭文件123filepath = r"C:\1.txt"with open(filepath, 'r', encoding='utf-8') as f: pass 4. 读取文本文件f.read() 一次读取所有文本（读到EOF为止）1234567filepath = r"C:\1.txt"with open(filepath, 'r', encoding='utf-8') as f: # 一次读完所有文本 text = f.read() # 按行遍历 for line in text.split('\n'): print(line) for line in f 迭代一行一行读取，适用于读取大文件，可节省内存123456filepath = r"C:\1.txt"with open(filepath, 'r', encoding='utf-8') as f: # 一行一行读取 for line in f: line = line.strip('\n') # 去掉结尾的换行符 print(line) f.readline() 读取1行123456filepath = r"C:\1.txt"with open(filepath, 'r', encoding='utf-8') as f: # 读取1行 print(f.readline()) # 再读取1行 print(f.readline()) f.readlines() -&gt; List[str] 读取所有行12345filepath = r"C:\1.txt"with open(filepath, 'r', encoding='utf-8') as f: for line in f.readlines(): line = line.strip('\n') # 去掉'\n' print(line) 5. 写文本]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sklearn KNN]]></title>
    <url>%2F2019%2F04%2F21%2FMachineLearning%2Fsklearn-KNN%2F</url>
    <content type="text"><![CDATA[1. KNN iris案例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from sklearn.neighbors import KNeighborsClassifierfrom sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaler""" 加载数据"""iris = datasets.load_iris()X_data, y_data = iris.data, iris.targetX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size=0.25)""" 特征归一化"""scaler = StandardScaler()X_train = scaler.fit_transform(X_train)X_test = scaler.transform(X_test)""" 训练"""clf = KNeighborsClassifier(n_neighbors=5)clf.fit(X_train, y_train)""" 评估"""print("train score: ", clf.score(X_train, y_train))print("test score: ", clf.score(X_test, y_test))""" 网格搜索"""param_grid = [ &#123; 'weights': ['uniform', 'distance'], 'n_neighbors': [i for i in range(3, 11)] &#125;]clf = KNeighborsClassifier()grid_search = GridSearchCV(clf, param_grid)grid_search.fit(X_train, y_train)print("Best set score: &#123;:.2f&#125;".format(grid_search.best_score_))print("Best parameters: &#123;&#125;".format(grid_search.best_params_))print("Test set score: &#123;:.2f&#125;".format(grid_search.score(X_test, y_test))) 2. KNeighborsClassifier参数说明参考： http://mdp-toolkit.sourceforge.net/api/mdp.nodes.RadiusNeighborsClassifierScikitsLearnNode-class.html https://www.cnblogs.com/pinard/p/6065607.html n_neighbors: int, optional (default = 5) KNN的K weights : str or callable, optional (default = “uniform”) uniform： p : integer, optional (default = 2) p是metric的附属参数。在metric=&quot;manhattan&quot;和metric=&quot;minkowski&quot;时，指定对应的p参数 metric : string or DistanceMetric object (default=’minkowski’) 指定距离的计算方式 欧式距离 “euclidean” 曼哈顿距离 “manhattan” 切比雪夫距离 “chebyshev” 闵可夫斯基距离 “minkowski” 带权重闵可夫斯基距离 “wminkowski” 标准化欧式距离 “seuclidean” 马氏距离 “mahalanobis”]]></content>
      <tags>
        <tag>sklearn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras 情感分析]]></title>
    <url>%2F2019%2F04%2F20%2FDeepLearning%2FKeras%2FKeras-%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[参考 http://blog.sina.com.cn/s/blog_1450ac3c60102x79x.html 数据集：https://github.com/aespresso/chinese_sentiment 1. 处理1.1. 统一序列的维度处理文本序列时，维度必须统一。文本A分词之后有200个token，文本B分词之后有1000个token，你必须统一成一个维度，例如限定序列长度是500。文本A不到500个token，用0填充，文本B就截取一部分 123456# 求出所有序列的长度num_tokens = [len(tokens) for tokens in train_tokens]num_tokens = np.array(num_tokens)# 序列长度统一成 长度的平均值 +２个标准差max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)max_tokens = int(max_tokens) 2. 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216""" 导包"""import numpy as npimport matplotlib.pyplot as pltfrom keras.utils.vis_utils import plot_modelimport reimport jiebafrom gensim.models import KeyedVectorsimport osfrom keras.models import Sequentialfrom keras.layers import Dense, GRU, Embedding, LSTM, Bidirectionalfrom keras.preprocessing.text import Tokenizerfrom keras.preprocessing.sequence import pad_sequencesfrom keras.optimizers import RMSpropfrom keras.optimizers import Adamfrom keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateaufrom sklearn.model_selection import train_test_split""" 加载词向量模型"""cn_model = KeyedVectors.load_word2vec_format( 'C:/Users/Administrator/Desktop/project/pyproject/pytest/data/sgns.literature.word', binary=False)embedding_dim = 300""" 加载数据"""pos_txts = os.listdir('pos')neg_txts = os.listdir('neg')# 存储所有评价，每例评价为一条stringtrain_texts_orig = []for i in range(len(pos_txts)): with open('pos/' + pos_txts[i], 'r', errors='ignore') as f: text = f.read().strip() train_texts_orig.append(text) f.close()for i in range(len(neg_txts)): with open('neg/' + neg_txts[i], 'r', errors='ignore') as f: text = f.read().strip() train_texts_orig.append(text) f.close()""" 分词和tokenize"""train_tokens = []for text in train_texts_orig: # 去掉标点 text = re.sub("[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&amp;*（）]+", "", text) # 结巴分词 cut = jieba.cut(text) # 结巴分词的输出结果为一个生成器 # 把生成器转换为list cut_list = [i for i in cut] for i, word in enumerate(cut_list): try: # 将词转换为索引index cut_list[i] = cn_model.vocab[word].index except KeyError: # 如果词不在字典中，则输出0 cut_list[i] = 0 train_tokens.append(cut_list)""" 索引长度标准化"""num_tokens = [len(tokens) for tokens in train_tokens]num_tokens = np.array(num_tokens)max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)max_tokens = int(max_tokens)""" 准备Embedding Matrix"""num_words = 200000embedding_matrix = np.zeros((num_words, embedding_dim))for i in range(num_words): embedding_matrix[i, :] = cn_model[cn_model.index2word[i]]embedding_matrix = embedding_matrix.astype('float32')""" padding（填充）和truncating（修剪）"""train_pad = pad_sequences(train_tokens, maxlen=max_tokens, padding='pre', truncating='pre')train_pad[train_pad &gt;= num_words] = 0# 准备target向量（训练集label），前2000样本为1，后2000为0train_target = np.concatenate((np.ones(2000), np.zeros(2000)))X_train, X_test, y_train, y_test = train_test_split(train_pad, train_target, test_size=0.1, random_state=12)""" 构建网络"""model = Sequential([ Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=max_tokens, trainable=False), Bidirectional(LSTM(units=32, return_sequences=True)), LSTM(units=16, return_sequences=False), Dense(1, activation='sigmoid')])optimizer = Adam(lr=1e-3)model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])""" 绘制网络结构"""plot_model(model, to_file="model.png", show_shapes=True, show_layer_names="False", rankdir="TB")img = plt.imread("model.png")plt.imshow(img)plt.show()""" 存储"""# 建立一个权重的存储点path_checkpoint = 'sentiment_checkpoint.keras'checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True)# 尝试加载已训练模型try: model.load_weights(path_checkpoint)except Exception as e: print(e)""" 训练"""# 定义early stoping如果3个epoch内validation loss没有改善则停止训练earlystopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)# 自动降低learning ratelr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.1, min_lr=1e-5, patience=0, verbose=1)# 定义callback函数callbacks = [earlystopping, checkpoint, lr_reduction]# 开始训练model.fit(X_train, y_train, validation_split=0.1, epochs=10, batch_size=128, callbacks=callbacks)""" 评估"""# 用模型评估 训练集loss, accuracy = model.evaluate(X_train, y_train)print('train loss: ', loss)print('train accuracy: ', accuracy)# 用模型评估 测试集loss, accuracy = model.evaluate(X_test, y_test)print('test loss: ', loss)print('test accuracy: ', accuracy)""" 预测样例"""def predict_sentiment(text): print(text) # 去标点 text = re.sub("[\s+\.\!\/_,$%^*(+\"\']+|[+——！，。？、~@#￥%……&amp;*（）]+", "", text) # 分词 cut = jieba.cut(text) cut_list = [i for i in cut] # tokenize for i, word in enumerate(cut_list): try: cut_list[i] = cn_model.vocab[word].index if cut_list[i] &gt;= num_words: cut_list[i] = 0 except KeyError: cut_list[i] = 0 # padding tokens_pad = pad_sequences([cut_list], maxlen=max_tokens, padding='pre', truncating='pre') # 预测 result = model.predict(x=tokens_pad) coef = result[0][0] if coef &gt;= 0.5: print('是一例正面评价', 'output=%.2f' % coef) else: print('是一例负面评价', 'output=%.2f' % coef)test_list = [ '酒店设施不是新的，服务态度很不好', '酒店卫生条件非常不好', '床铺非常舒适', '房间很凉，不给开暖气', '房间很凉爽，空调冷气很足', '酒店环境不好，住宿体验很不好', '房间隔音不到位', '晚上回来发现没有打扫卫生', '因为过节所以要我临时加钱，比团购的价格贵']for text in test_list[-2:]: predict_sentiment(text)]]></content>
      <tags>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras 绘制网络结构]]></title>
    <url>%2F2019%2F04%2F20%2FDeepLearning%2FKeras%2FKeras-%E7%BB%98%E5%88%B6%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[先下载graphviz https://www.graphviz.org/download/ ，可以下载zip绿色版的 解压之后，将bin目录添加到PATH变量中 再执行以下代码1234567891011121314151617181920212223242526272829303132""" 导包"""import matplotlib.pyplot as pltfrom keras.layers import Dense, Dropout, Convolution2D, MaxPool2D, Flattenfrom keras.models import Sequentialfrom keras.utils.vis_utils import plot_model""" 构建网络"""model = Sequential([ Convolution2D(input_shape=(28, 28, 1), filters=32, kernel_size=1, strides=1, padding='same', activation='relu'), MaxPool2D(pool_size=2, strides=2, padding='same'), Convolution2D(filters=64, kernel_size=5, strides=1, padding='same', activation='relu'), MaxPool2D(pool_size=2, strides=2, padding='same'), Flatten(), Dense(units=1024, activation='relu'), Dropout(0.5), Dense(units=10, activation='softmax'),])""" 绘制网络"""# rankdir="TB": T代表top，B代表bottom，"TB"就是从上到下绘制网络结构plot_model(model, to_file="model.png", show_shapes=True, show_layer_names="False", rankdir="TB")img = plt.imread("model.png")plt.imshow(img)plt.show()]]></content>
      <tags>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras 手写数字识别]]></title>
    <url>%2F2019%2F04%2F20%2FDeepLearning%2FKeras%2FKeras-%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[1. 普通神经网络 手写数字识别123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354""" 导包"""import matplotlib.pyplot as pltimport numpy as npfrom keras.layers import Dense, Activation, Dropoutfrom keras.models import Sequentialfrom keras.optimizers import SGDfrom tensorflow.examples.tutorials.mnist import input_datafrom keras.regularizers import l2""" 读取数据"""mnist = input_data.read_data_sets("./data/mnist/input_data/", one_hot=True)x_train, y_train = mnist.train.images, mnist.train.labelsx_test, y_test = mnist.test.images, mnist.test.labels""" 构建网络"""model = Sequential([ # 输入784个神经元，输出10个神经元 # bias_initializer='one'初始化偏置为1，activation='softmax'激活函数将输出转为概率 Dense(input_dim=784, units=10, bias_initializer='one', activation='softmax')])# 自定义优化器，lr=0.2 学习率sgd = SGD(lr=0.2)model.compile( optimizer=sgd, # 设置优化器 loss='mse', # 设置损失函数，mse是均方误差 metrics=['accuracy'] # 计算准确率)""" 训练"""# batch_size=32每次取32个样本进行训练。所有样本训练一次将一个周期（epochs），epochs=10指定训练10个周期model.fit(x_train, y_train, batch_size=32, epochs=10)""" 评估"""# 用模型预测 训练集loss, accuracy = model.evaluate(x_train, y_train)print('train loss: ', loss)print('train accuracy: ', accuracy)# 用模型预测 测试集loss, accuracy = model.evaluate(x_test, y_test)print('test loss: ', loss)print('test accuracy: ', accuracy) 1.1. 使用交叉熵损失model.compile之前用的loss函数是mse均方误差，现在修改为categorical_crossentropy交叉熵，再训练，你会发现效果稍微好一点12345model.compile( optimizer=sgd, loss='categorical_crossentropy', # 设置交叉熵损失函数categorical_crossentropy， metrics=['accuracy']) 1.2. 构建多层网络构建3层，效果更好12345678model = Sequential([ # 784 -&gt; 200 Dense(input_dim=784, units=200, bias_initializer='one', activation='tanh'), # 200 -&gt; 100，"input_dim=200"可以直接省略 Dense(units=100, bias_initializer='one', activation='tanh'), # 100 -&gt; 10 Dense(units=10, bias_initializer='one', activation='softmax'),]) 结果12345655000/55000 [==============================] - 1s 17us/steptrain loss: 0.007714528205446814train accuracy: 0.998563636363636310000/10000 [==============================] - 0s 17us/steptest loss: 0.07307906513710914test accuracy: 0.9797 1.3. 添加Dropout防止过拟合123456789from keras.layers import Dropoutmodel = Sequential([ Dense(input_dim=784, units=200, bias_initializer='one', activation='tanh'), Dropout(0.4), # 失活40%的神经元，防止过拟合 Dense(units=100, bias_initializer='one', activation='tanh'), Dropout(0.4), # 失活40%的神经元，防止过拟合 Dense(units=10, bias_initializer='one', activation='softmax'),]) 可以看到，加了Dropout，准确率反而下降了。在这个例子中，仅仅是了解一下如何使用Dropout，目的不是为了提高准确率12345655000/55000 [==============================] - 1s 18us/steptrain loss: 0.07450573552753777train accuracy: 0.977181818181818210000/10000 [==============================] - 0s 20us/steptest loss: 0.10471451835799962test accuracy: 0.9694 1.4. 正则化防止过拟合1234567from keras.regularizers import l2model = Sequential([ Dense(input_dim=784, units=200, bias_initializer='one', activation='tanh', kernel_regularizer=l2(0.0003)), Dense(units=100, bias_initializer='one', activation='tanh', kernel_regularizer=l2(0.0003)), Dense(units=10, bias_initializer='one', activation='softmax', kernel_regularizer=l2(0.0003)),]) 结果12345655000/55000 [==============================] - 1s 18us/steptrain loss: 0.13682303088795056train accuracy: 0.987418181818181910000/10000 [==============================] - 0s 19us/steptest loss: 0.16329636447429657test accuracy: 0.9764 1.5. 使用Adam优化器123456789from keras.optimizers import Adamadam = Adam(lr=0.001)model.compile( optimizer=adam, # 设置Adam优化器 loss='categorical_crossentropy', metrics=['accuracy']) 1.6. 保存和加载模型在训练完之后，可以保存模型1234""" 保存模型"""model.save("mymodel") 之后可以直接加载模型，进行预测123456789101112131415161718192021222324252627282930from keras.models import load_modelfrom tensorflow.examples.tutorials.mnist import input_data""" 读取数据"""mnist = input_data.read_data_sets("./data/mnist/input_data/", one_hot=True)x_train, y_train = mnist.train.images, mnist.train.labelsx_test, y_test = mnist.test.images, mnist.test.labelsx_train = x_train.reshape([-1, 28, 28])x_test = x_test.reshape([-1, 28, 28])""" 加载模型"""model = load_model("mymodel")""" 评估"""# 用模型预测 训练集loss, accuracy = model.evaluate(x_train, y_train)print('train loss: ', loss)print('train accuracy: ', accuracy)# 用模型预测 测试集loss, accuracy = model.evaluate(x_test, y_test)print('test loss: ', loss)print('test accuracy: ', accuracy) 2. CNN 手写数字识别1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192""" 导包"""import matplotlib.pyplot as pltimport numpy as npfrom keras.layers import Dense, Activation, Dropout, Convolution2D, MaxPool2D, Flattenfrom keras.models import Sequentialfrom keras.optimizers import SGD, Adamfrom tensorflow.examples.tutorials.mnist import input_datafrom keras.regularizers import l2""" 读取数据"""mnist = input_data.read_data_sets("./data/mnist/input_data/", one_hot=True)x_train, y_train = mnist.train.images, mnist.train.labelsx_test, y_test = mnist.test.images, mnist.test.labels# CNN要求数据是4维格式，各维度是 [样本数，图片长，图片宽，通道数]x_train = x_train.reshape([-1, 28, 28, 1])x_test = x_test.reshape([-1, 28, 28, 1])""" 构建网络"""model = Sequential([ # 第1个卷积层 Convolution2D( input_shape=(28, 28, 1), # 输入平面 filters=32, # 卷积核（滤波器）个数 kernel_size=1, # 卷积窗口大小（通道数） strides=1, # 步长 padding='same', # padding方式 activation='relu' # 激活函数 ), # 第1个池化层 MaxPool2D( pool_size=2, strides=2, padding='same' ), # 第2个卷积层，不用再设置input_shape，因为经过上一次卷积，平面形状是已知的 Convolution2D( filters=64, kernel_size=5, strides=1, padding='same', activation='relu' ), # 第2个池化层 MaxPool2D( pool_size=2, strides=2, padding='same' ), # 将第2个池化层的输出结果扁平化为1维 Flatten(), # 第1个全连接层 Dense(units=1024, activation='relu'), # Dropout 失活50%的神经元 Dropout(0.5), # 第2个全连接层， Dense(units=10, activation='softmax'),])adam = Adam(lr=1e-4)model.compile( optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])""" 训练"""model.fit(x_train, y_train, batch_size=64, epochs=10)""" 评估"""# 用模型预测 训练集loss, accuracy = model.evaluate(x_train, y_train)print('train loss: ', loss)print('train accuracy: ', accuracy)# 用模型预测 测试集loss, accuracy = model.evaluate(x_test, y_test)print('test loss: ', loss)print('test accuracy: ', accuracy) 结果12345655000/55000 [==============================] - 19s 351us/steptrain loss: 0.02459700823045251train accuracy: 0.992490909090909110000/10000 [==============================] - 4s 353us/steptest loss: 0.04115021191320848test accuracy: 0.9863 3. RNN 手写数字识别12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364""" 导包"""import matplotlib.pyplot as pltimport numpy as npfrom keras.layers import Dense, Activation, Dropout, Convolution2D, MaxPool2D, Flattenfrom keras.models import Sequentialfrom keras.optimizers import SGD, Adamfrom tensorflow.examples.tutorials.mnist import input_datafrom keras.regularizers import l2from keras.layers.recurrent import SimpleRNN""" 读取数据"""mnist = input_data.read_data_sets("./data/mnist/input_data/", one_hot=True)x_train, y_train = mnist.train.images, mnist.train.labelsx_test, y_test = mnist.test.images, mnist.test.labels# 将图片看作序列化数据。N个样本就是N行数据，每个样本28x28，即每个样本28个序列，每个序列是28个数x_train = x_train.reshape([-1, 28, 28])x_test = x_test.reshape([-1, 28, 28])""" 构建网络"""input_size = 28 # 一行有28个像素time_steps = 28 # 序列（图像）共有28行cell_size = 50 # 隐藏层神经元数model = Sequential([ # RNN SimpleRNN( units=cell_size, input_shape=(time_steps, input_size) # 28x28 ), # 全连接层 Dense(units=10, activation='softmax')])adam = Adam(lr=1e-4)model.compile( optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])""" 训练"""model.fit(x_train, y_train, batch_size=64, epochs=10)""" 评估"""# 用模型预测 训练集loss, accuracy = model.evaluate(x_train, y_train)print('train loss: ', loss)print('train accuracy: ', accuracy)# 用模型预测 测试集loss, accuracy = model.evaluate(x_test, y_test)print('test loss: ', loss)print('test accuracy: ', accuracy) 结果如下，显然RNN不如CNN效果好。CNN多用于处理图像，RNN多用于处理序列式问题，如文本12345655000/55000 [==============================] - 2s 45us/steptrain loss: 0.33378106603405694train accuracy: 0.902563636363636310000/10000 [==============================] - 0s 40us/steptest loss: 0.3353205518245697test accuracy: 0.9036 4. 激活函数和损失函数的选择]]></content>
      <tags>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Keras 线性回归]]></title>
    <url>%2F2019%2F04%2F19%2FDeepLearning%2FKeras%2FKeras-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1. 实现线性回归1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162""" 导包"""import matplotlib.pyplot as pltimport numpy as np# Dense 是全连接层from keras.layers import Dense# Sequential 模型是多个网络层的线性堆叠from keras.models import Sequential""" 生成数据"""# 随机生成100个数x_data = np.random.rand(100)# 正态生成100个噪音，第1个参数是均值，第2个参数是方差，第3个参数是形状noise = np.random.normal(0, 0.01, x_data.shape)# 生成 y = 0.1x + 0.2 + 噪音抖动y_data = 0.1 * x_data + 0.2 + noise""" 构建顺序模型（定义网络）"""model = Sequential()# 在模型中添加全连接层（FC层） units=输出值维度，input_dim=输入值维度。因为x和y都是1维的，所以设置为1model.add(Dense(units=1, input_dim=1))# 编译模型，optimizer指定优化器，sgd代表随机梯度下降；loss指定损失函数，"mse"代表均方误差model.compile(optimizer='sgd', loss='mse')""" 训练"""for step in range(30000): # train_on_batch指定要训练的批次数据集，因为我们的数据很小，所以整个数据作为一个批次传入 cost = model.train_on_batch(x_data, y_data) # 每训练500，输出cost if step % 500 == 0: print('step = %s, cost: %s' % (step, cost))""" 获取训练得到的权值和偏置"""# 获取权值和偏置。因为只有1层，所以取layers[0]即可w, b = model.layers[0].get_weights()print('w = %s, b = %s' % (w, b))""" 预测"""y_predict = model.predict(x_data)""" 绘图"""# 显示原数据随机点，以散点图形式显示plt.scatter(x_data, y_data)# 显示预测结果，画一条直线。颜色'r'即red，lw=3代表线宽为3像素plt.plot(x_data, y_predict, 'r', lw=3)plt.show() 2. 实现非线性回归123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657""" 导包"""import matplotlib.pyplot as pltimport numpy as npfrom keras.layers import Dense, Activationfrom keras.models import Sequentialfrom keras.optimizers import SGD""" 生成数据"""# 在[-0.5,0.5]之间生成200个均匀分布的点x_data = np.linspace(-0.5, 0.5, 200)# 正态生成噪音点，第1个参数是均值，第2个参数是方差，第3个参数是形状noise = np.random.normal(0, 0.02, x_data.shape)# 生成 y = x^2 + 噪音y_data = x_data ** 2 + noise""" 构建顺序模型（定义网络） 1 - 10 - 1"""model = Sequential()# 添加 1 --&gt; 10model.add(Dense(input_dim=1, units=10))model.add(Activation('tanh')) # 添加非线性激活函数# 添加 10 --&gt; 1，"input_dim=10"可以省略，keras可以推断上一层的输出维度model.add(Dense(input_dim=10, units=1))model.add(Activation('tanh')) # 添加非线性激活函数# 自定义优化器SGD，lr（learning rate）设置学习率，默认0.01sgd = SGD(lr=0.3)# 编译模型model.compile(optimizer=sgd, loss='mse')""" 训练"""for step in range(30000): # train_on_batch指定要训练的批次数据集，因为我们的数据很小，所以整个数据作为一个批次传入 cost = model.train_on_batch(x_data, y_data) # 每训练500，输出cost if step % 500 == 0: print('step = %s, cost: %s' % (step, cost))""" 预测"""y_predict = model.predict(x_data)""" 绘图"""plt.scatter(x_data, y_data)plt.plot(x_data, y_predict, 'r', lw=3)plt.show() 2.1. 添加激活函数的两种方式方式1：1234model.add(Dense(input_dim=1, units=10))model.add(Activation('tanh'))model.add(Dense(input_dim=10, units=1))model.add(Activation('tanh')) 方式1：12model.add(Dense(input_dim=1, units=10, activation='tanh'))model.add(Dense(input_dim=10, units=1, activation='tanh'))]]></content>
      <tags>
        <tag>Keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC源码分析 DispatcherServlet]]></title>
    <url>%2F2019%2F04%2F18%2FSpringMVC%2FSpringMVC%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90-DispatcherServlet%2F</url>
    <content type="text"><![CDATA[1. DispatcherServlet UML Diagram]]></content>
      <tags>
        <tag>SpringMVC源码分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC 文件上传]]></title>
    <url>%2F2019%2F04%2F18%2FSpringMVC%2FSpringMVC-%E6%96%87%E4%BB%B6%E4%B8%8A%E4%BC%A0%2F</url>
    <content type="text"><![CDATA[1. 文件上传实现流程1.1. 依赖123456789101112&lt;!-- SpringMVC --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 文件上传 --&gt;&lt;dependency&gt; &lt;groupId&gt;commons-fileupload&lt;/groupId&gt; &lt;artifactId&gt;commons-fileupload&lt;/artifactId&gt; &lt;version&gt;1.4&lt;/version&gt;&lt;/dependency&gt; 1.2. 配置CommonsMultipartResolver1234&lt;bean id="multipartResolver" class="org.springframework.web.multipart.commons.CommonsMultipartResolver"&gt; &lt;property name="maxUploadSize" value="#&#123;1024 * 1024 * 10&#125;"/&gt; &lt;property name="defaultEncoding" value="utf-8"/&gt;&lt;/bean&gt; 1.3. 处理文件上传请求123456789101112131415@Controllerpublic class HelloController &#123; @PostMapping("/file/upload") public void fileUpload(MultipartFile file, HttpServletRequest request) &#123; try &#123; InputStream in = file.getInputStream(); String filename = file.getOriginalFilename(); System.out.println("filename = " + filename); // 复制到本地 FileUtils.copyInputStreamToFile(in, new File("C:\\" + filename)); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 1.4. 测试Postman向发起请求：localhost:8080/file/upload Headers设置 Content-Type为multipart/form-data Body将选中数据格式为form-data，添加一对key-value，key为file，value选择文件类型，上传文件 2. 多文件同步上传前面的步骤不改变，只要修改一下Controller 12345678910111213@PostMapping("/upload")public void upload(MultipartFile[] file) &#123; for (MultipartFile multipartFile : file) &#123; if (!multipartFile.isEmpty()) &#123; try &#123; // 保存文件到本地 multipartFile.transferTo(new File("C:\\" + multipartFile.getOriginalFilename())); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125; Postman向发起请求：localhost:8080/upload Headers设置 Content-Type为multipart/form-data，没有变化 Body将选中数据格式为form-data，添加多对key-value，key全都为file，value选择文件类型，上传文件]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet 相对路径和绝对路径]]></title>
    <url>%2F2019%2F04%2F17%2FJava%2FServlet%2FServlet-%E7%9B%B8%E5%AF%B9%E8%B7%AF%E5%BE%84%E5%92%8C%E7%BB%9D%E5%AF%B9%E8%B7%AF%E5%BE%84%2F</url>
    <content type="text"><![CDATA[1. 客户端请求路径1.1. 超链接跳转当前路径：http://localhost:8080/webapp/html/hello.html1234&lt;!-- 目标URL：http://localhost:8080/webapp/index.html --&gt;&lt;a href="http://localhost:8080/webapp/index.html"&gt;URL绝对路径，要带上虚拟目录&lt;/a&gt;&lt;a href="/webapp/index.html"&gt;URI绝对路径，要带上虚拟目录&lt;/a&gt;&lt;a href="../index.html"&gt;相对路径&lt;/a&gt; 1.2. 302重定向路径重定向是在客户端执行的，所以要带上虚拟目录getContextPath()1resp.sendRedirect(req.getContextPath() + "/hello"); 2. 服务器内部路径总之使用绝对路径时 客户端请求路径，要带上虚拟目录 服务器内部路径，不需要虚拟目录 2.1. 服务器转发当前路径：/aa/servlet11234// 目标Servlet绝对路径：/helloreq.getRequestDispatcher("/hello").forward(req, resp);// 目标Servlet相对路径：../helloreq.getRequestDispatcher("../hello").forward(req, resp);]]></content>
      <tags>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java工具 Bean数据封装]]></title>
    <url>%2F2019%2F04%2F17%2FJava%2F%E5%B7%A5%E5%85%B7%2FJava%E5%B7%A5%E5%85%B7-Bean%E6%95%B0%E6%8D%AE%E5%B0%81%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1. Apache BeanUtils1.1. 依赖12345&lt;dependency&gt; &lt;groupId&gt;commons-beanutils&lt;/groupId&gt; &lt;artifactId&gt;commons-beanutils&lt;/artifactId&gt; &lt;version&gt;1.9.3&lt;/version&gt;&lt;/dependency&gt;]]></content>
      <tags>
        <tag>Java工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet 中文乱码]]></title>
    <url>%2F2019%2F04%2F17%2FJava%2FServlet%2FServlet-%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%2F</url>
    <content type="text"><![CDATA[1. 乱码原因简单地讲，编码（encode）和解码（decode）使用的字符集不一致，就会导致乱码 服务器接收请求参数乱码原因：服务器接收数据解码时使用的字符集，与客户端发送数据编码时使用的字符集不一致 客户端接收响应数据乱码原因：客户端接收数据解码时使用的字符集，与服务器发送数据编码时使用的字符集不一致 2. 服务器获取GET请求queryString乱码解决方案从Tomcat8开始，GET乱码问题已经由Tomcat自己解决了 如果是Tomcat7及之前，可以编辑conf/server.xml12345&lt;!-- 找到设置port="8080"的那个标签，添加URIEncoding="UTF-8"即可 --&gt;&lt;Connector port="8080" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" URIEncoding="UTF-8" /&gt; 如果使用的是Maven Tomcat插件，在&lt;configuration&gt;标签中设置&lt;uriEncoding&gt;属性即可123456789101112131415&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;port&gt;8080&lt;/port&gt; &lt;path&gt;/&lt;/path&gt; &lt;!-- 设置uriEncoding --&gt; &lt;uriEncoding&gt;UTF-8&lt;/uriEncoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 3. 服务器获取POST请求体乱码解决方案123// 在获取请求参数之前，先通过setCharacterEncoding设置编码req.setCharacterEncoding("UTF-8");String username = req.getParameter("username"); 4. 客户端接收响应体乱码解决方案在响应头不指定ContentType时，浏览器器默认使用的字符集与系统环境有关，例如Windows中文环境下，默认字符集是GBK（GB2312） resp.getWriter() 获取输出流对象。Response是Tomcat实现的，编写Tomcat的不是中国人，他们指定了输出流的的默认字符集是IOS-8859-1。所以你用PrintWriter输出中文，使用IOS-8859-1，但是浏览器用GBK解码，肯定会乱码 解决方案如下： 1234resp.setCharacterEncoding("UTF-8"); // 在获取输入流之前，设置响应体的编码resp.setContentType("text/html;charset=utf-8"); // 设置响应头ContentType，告诉浏览器使用指定字符集进行解码PrintWriter out = resp.getWriter();out.print("&lt;h1&gt;你好&lt;/h1&gt;"); 事实上，setCharacterEncoding()可以进一步省略，因为Tomcat在实现setContentType()时，不仅仅设置响应头了的ContentType，还同时设置了响应体的编码123resp.setContentType("text/html;charset=utf-8"); // 只要这一句就够了PrintWriter out = resp.getWriter();out.print("&lt;h1&gt;你好&lt;/h1&gt;");]]></content>
      <tags>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet url-pattern]]></title>
    <url>%2F2019%2F04%2F17%2FJava%2FServlet%2FServlet-url-pattern%2F</url>
    <content type="text"><![CDATA[1. 多映射一个Servlet可以配置多个url-pattern1234567891011121314&lt;servlet&gt; &lt;servlet-name&gt;myServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;ServletDemo&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;!-- 映射1 --&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;myServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/demo&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;!-- 映射2 --&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;myServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/haha&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 2. 映射规则通配映射只有以下3种，注意/aa*和/*aa这两种通配映射是不存在的，不要使用 url-pattern 描述 匹配示例 不匹配示例 *.xxx 匹配扩展名 /aa.xxx/aa/bb.xxx /aaxxx /* 匹配任何路径 /aa/aa.jsp/aa/bb/cc /aa/* 匹配前缀路径 /aa/aa/bb/aa/bb/cc/aa/bb/cc.jsp /aabb 优先级从高到低 精确匹配 /aa/bb 前缀匹配 /aa/* 任意匹配 /* 后缀匹配 *.xxx 示例： 访问/aa/bb，会优先匹配/aa/bb，而不会匹配/aa/*和/* 访问/aa/cc，会优先匹配/aa/*，而不会匹配/* 访问/bb，会匹配/* 访问/aa.xxx，会匹配/*，而不会匹配*.xxx。也就是说，如果你配置了/*，其它任何后缀匹配都是没有意义的]]></content>
      <tags>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet 三种创建方式]]></title>
    <url>%2F2019%2F04%2F17%2FJava%2FServlet%2FServlet-%E4%B8%89%E7%A7%8D%E5%88%9B%E5%BB%BA%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1. 三种创建方式1.1. 方式1：实现Servlet接口123456789101112131415161718192021222324252627public class MyServlet implements Servlet &#123; @Override public void init(ServletConfig config) throws ServletException &#123; &#125; @Override public ServletConfig getServletConfig() &#123; return null; &#125; @Override public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException &#123; System.out.println("hello"); &#125; @Override public String getServletInfo() &#123; return null; &#125; @Override public void destroy() &#123; &#125;&#125; 1.2. 方式2：继承GenericServlet123456public class MyServlet extends GenericServlet &#123; @Override public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException &#123; System.out.println("hello"); &#125;&#125; 1.3. 方式3：继承HttpServlet1234567public class MyServlet extends HttpServlet &#123; @Override protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("hello"); &#125;&#125; 1.4. 三种方式对比实现Servlet接口：要实现一大堆没有用到的方法，类显得太臃肿 为了解决实现Servlet接口导致类过于臃肿的问题，就有了继承抽象类GenericServlet的方式。抽象GenericServlet对Servlet的所有方法进行了空实现（即实现了方法，但是没有做任何事），除了service方法需要你手动去实现 HttpServlet在GenericServlet的基础上，添加了对HTTP协议的封装，可以处理各种HTTP请求。实际开发中，我们就使用继承HttpServlet的方式]]></content>
      <tags>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet 概述]]></title>
    <url>%2F2019%2F04%2F17%2FJava%2FServlet%2FServlet-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1. 什么是ServletServlet是运行在服务端的小程序]]></content>
      <tags>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet Response对象]]></title>
    <url>%2F2019%2F04%2F17%2FJava%2FServlet%2FServlet-Response%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[1. 设置响应行响应行格式：协议/版本 状态码 状态码描述1HTTP/1.1 200 OK 1.1. 设置状态码1resp.setStatus(302); 2. 设置响应头1resp.setHeader("location", "http://www.baidu.com"); 3. 设置响应体设置响应体步骤： 获取输出流 输出数据 注意输出流不必flush或者close，因为写完之后，response对象会被自动销毁，输出流会被自动关闭 3.1. 字符输出12PrintWriter out = resp.getWriter(); // 获取字符输出流out.print("&lt;h1&gt;你好&lt;/h1&gt;"); // 输出数据 3.2. 字节输出12ServletOutputStream out = resp.getOutputStream(); // 获取字节输出流out.write("&lt;h1&gt;HelloWorld&lt;/h1&gt;".getBytes()); 4. 重定向重定向需要设置状态码，以及location12resp.setStatus(302); // 302重定向resp.setHeader("location", "http://www.baidu.com"); // 要重定向的目标 也可以用sendRedirect(location)进行简化1resp.sendRedirect("http://www.baidu.com"); 重定向的特点：redirect 地址栏发生变化 重定向可以访问其他站点(服务器)的资源 重定向是两次请求。不能使用request对象来共享数据 转发的特点：forward 转发地址栏路径不变 转发只能访问当前服务器下的资源 转发是一次请求，可以使用request对象来共享数据]]></content>
      <tags>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet Request对象]]></title>
    <url>%2F2019%2F04%2F16%2FJava%2FServlet%2FServlet-Request%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[1. request对象和response对象的原理每向Tomcat发一次请求，Tomcat就会创建一个request和一个response对象，将本次请求的所有数据封装到request对象中，然后将request和response作为实参传递给对应Servlet的service方法，来处理本次请求 2. request对象继承体系结构12345ServletRequest -- 接口 | 继承HttpServletRequest -- 接口 | 实现org.apache.catalina.connector.RequestFacade 类(tomcat) 3. 获取请求行请求行格式如下：1GET /webapp/index.html?username=abc HTTP/1.1 3.1. 获取请求方式getMethod()返回的值只可能是以下几种： &quot;DELETE&quot; &quot;HEAD&quot; &quot;GET&quot; &quot;OPTIONS&quot; &quot;POST&quot; &quot;PUT&quot; &quot;TRACE&quot; 1234String method = req.getMethod();if (method.equals("GET")) &#123; System.out.println("...");&#125; 3.2. 获取虚拟目录1String contextPath = req.getContextPath(); 3.3. 获取ServletPath1String servletPath = req.getServletPath(); 3.4. 获取请求URL和URI例如访问http://localhost:8080/webapp/index.html?username=abc1234StringBuffer url = req.getRequestURL(); // 获取请求URL http://localhost:8080/webapp/index.htmlString uri = req.getRequestURI(); // 获取请求URI /webapp/index.htmlSystem.out.println("url = " + url.toString());System.out.println("uri = " + uri); 3.5. 获取URL中的请求参数（GET请求的参数）1String queryString = req.getQueryString(); 例如访问http://localhost:8080/hello?username=mike&amp;password=123456，得到的queryString是username=mike&amp;password=123456 3.6. 获取请求协议12String scheme = req.getScheme(); // 获取请求的协议：如http/https/ftp String protocol = req.getProtocol(); // 获取请求的协议（含版本号） 如 HTTP/1.1 4. 获取请求头4.1. 获取Content-Type1String contentType = req.getContentType(); 4.2. 根据名称获取String getHeader(String name) 注意如果不存在，则返回null12// 注意：请求头的name不区分大小写String userAgent = req.getHeader("User-Agent"); 4.3. 遍历请求头123456789// 获取所有header的名称Enumeration&lt;String&gt; headers = req.getHeaderNames();// 遍历所有headerwhile (headers.hasMoreElements()) &#123; // 得到该header对应的value String key = headers.nextElement(); String value = req.getHeader(key); System.out.println(key + " --- " + value);&#125; 5. 获取客户端信息1234String remoteUser = req.getRemoteUser(); // 获取当前缓存的用户，比如Spring Security做权限控制后就会将用户登录名缓存到这里 String remoteAddr = req.getRemoteAddr(); // 获取客户端IPString remoteHost = req.getRemoteHost(); // 获取客户端的主机名int remotePort = req.getRemotePort(); // 获取客户端端口 6. 获取服务器信息123String localAddr = req.getLocalAddr(); // 获取服务器IPString localName = req.getLocalName(); // 获取服务器主机名int localPort = req.getLocalPort(); // 获取服务器端口 7. 通用获取请求参数获取请求参数不区分GET/POST，可以统一用以下几种方式来获取参数的值 7.1. 根据name获取单个值1String username = req.getParameter("username"); 注意以下几种情况参数的值：1234http://localhost:8080/hello?aaa=123 username为nullhttp://localhost:8080/hello?username&amp;aaa=3 username为空字符串http://localhost:8080/hello?username=&amp;aaa=3 username为空字符串http://localhost:8080/hello?username=123&amp;aaa=3 username为&quot;123&quot; 7.2. 根据name获取多个值有时候同1个name，可能有多个value，例如复选框123&lt;input type="checkbox" name="check" value="1"&gt;&lt;input type="checkbox" name="check" value="2"&gt;&lt;input type="checkbox" name="check" value="3"&gt; 勾选之后GET提交的URL如下1http://localhost:8080/hello?check=1&amp;check=2&amp;check=3 req.getParameter(name)只能获取第1个value，如果想获取多个，则使用req.getParameterValues(name)1234String[] checks = req.getParameterValues("check");for (String value : checks) &#123; System.out.println(value);&#125; 要注意，没有对应参数时，req.getParameterValues(name)也是返回null，在用for遍历数组时，会出现空指针异常，所以要先判断是否为null123456String[] checks = req.getParameterValues("check");if (checks != null) &#123; for (String value : checks) &#123; System.out.println(value); &#125;&#125; 7.3. 遍历所有请求参数getParameterNames() 获取所有请求参数的名称123456Enumeration&lt;String&gt; params = req.getParameterNames();while (params.hasMoreElements()) &#123; String name = params.nextElement(); String value = req.getParameter(name); System.out.println(name + " === " + value);&#125; getParameterMap() 获取所有请求参数以及值。key是参数名，value是参数的值，因为值可能有多个，所以是数组类型1234Map&lt;String, String[]&gt; map = req.getParameterMap();map.forEach((name, values) -&gt; &#123; System.out.println(name + " === " + Arrays.toString(values));&#125;); 8. 获取请求体只有POST请求，才有请求体。Request将请求体封装成流，想获取请求体，你首先要获取流对象，再从流对象中取数据。 你可以从Request对象获取两种流，如果数据是字符，就用字符流，如果是字节，就用字节流 BufferedReader getReader() 获取字符流 ServletInputStream getInputStream() 获取字符流 读取完之后，流不必close()，因为流是从request中获取的，本次请求结束后，request对象会自动被销毁 读取字符示例123456BufferedReader br = req.getReader();String line = null;while ((line = br.readLine()) != null) &#123; line = br.readLine(); System.out.println(line);&#125; 9. 请求转发请求转发：一种在服务器内部的资源跳转方式 转发步骤： 通过request对象获取请求转发器对象：RequestDispatcher getRequestDispatcher(String path) 使用RequestDispatcher对象来进行转发：forward(ServletRequest request, ServletResponse response) 12345678910111213141516@WebServlet("/hello")public class MyServlet1 extends HttpServlet &#123; @Override protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("hello"); req.getRequestDispatcher("/world").forward(req, resp); // 转发请求 &#125;&#125;@WebServlet("/world")public class MyServlet2 extends HttpServlet &#123; @Override protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("world"); &#125;&#125; 转发特点： 浏览器地址栏路径不发生变化 只能转发到当前服务器内部资源中。 转发是一次请求 10. 共享数据共享数据： 域对象：一个有作用范围的对象，可以在范围内共享数据 request域：代表一次请求的范围，一般用于请求转发的多个资源中共享数据 方法： void setAttribute(String name,Object obj) 存储数据 Object getAttitude(String name) 通过键获取值 void removeAttribute(String name) 通过键移除键值对 12345678910111213141516171819@WebServlet("/hello")public class MyServlet1 extends HttpServlet &#123; @Override protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("hello"); req.setAttribute("age", 100); // 转发前存储数据 req.getRequestDispatcher("/world").forward(req, resp); // 转发请求 &#125;&#125;@WebServlet("/world")public class MyServlet2 extends HttpServlet &#123; @Override protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; int age = (int) req.getAttribute("age"); System.out.println("age = " + age); // 获取数据 System.out.println("world"); &#125;&#125;]]></content>
      <tags>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet 处理各种类型请求]]></title>
    <url>%2F2019%2F04%2F16%2FJava%2FServlet%2FServlet-%E5%A4%84%E7%90%86%E5%90%84%E7%A7%8D%E7%B1%BB%E5%9E%8B%E8%AF%B7%E6%B1%82%2F</url>
    <content type="text"><![CDATA[1. Servlet处理各种请求 service：处理所有请求 doGet：处理GET请求 doPost：处理POST请求 doPut：处理PUT请求 doDelete：处理DELETE请求 doHead：处理HEAD请求 doOptions：处理OPTIONS请求 12345678910111213141516171819202122232425262728293031public class MyServlet extends HttpServlet &#123; @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("doGet"); &#125; @Override protected void doPost(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("doPost"); &#125; @Override protected void doDelete(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("doDelete"); &#125; @Override protected void doPut(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("doPut"); &#125; @Override protected void doHead(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("doHead"); &#125; @Override protected void doOptions(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("doOptions"); &#125;&#125; 如果你重写了service，又重写了doXxx，那么只有service会被调用12345678910/** 只调用service */@Overrideprotected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("service");&#125;@Overrideprotected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("doGet");&#125; 但是如果你在service中调用父类的service，则会发现，doXxx也会被调用。这说明，父类的service会根据请求的类型，调用对应的doXxx12345678910@Overrideprotected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("service"); super.service(req, resp);&#125;@Overrideprotected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("doGet");&#125; 2. 405错误：请求方式不支持如果调用了父类的service，你又没有重写相应的doXxx，则发起请求时，就返回HTTP Status 405 – Method Not Allowed，表示服务器无法处理该请求12345@Overrideprotected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; System.out.println("service"); super.service(req, resp);&#125;]]></content>
      <tags>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet 生命周期]]></title>
    <url>%2F2019%2F04%2F16%2FJava%2FServlet%2FServlet-%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[1. 生命周期方法默认情况下，当用户第一次发起请求，Tomcat会根据URI创建对应的Servlet，将Servlet加载到堆内存中，执行构造方法和init() 之后，Servlet一直驻留在堆内存中，直到当前应用被关闭，执行destory() 12345678910111213141516171819202122232425public class MyServlet extends HttpServlet &#123; /** Servlet被实例化时调用 */ public MyServlet() &#123; System.out.println("MyServlet()"); &#125; /** 实例化之后，执行init */ @Override public void init() throws ServletException &#123; System.out.println("init"); &#125; /** Servlet销毁之前，执行destory */ @Override public void destroy() &#123; System.out.println("destory"); &#125; /** 访问当前Servlet时，执行service */ @Override protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; resp.getWriter().write("&lt;h1&gt;HelloWorld&lt;/h1&gt;"); &#125;&#125; 2. 实例化Servlet的时机可以通过配置loadOnStartup，来改变Servlet实例化的时机，分以下3种情况 loadOnStartup &lt; 0：即负数的情况下，web容器启动的时候不做实例化处理，servlet首次被调用时做实例化。这种情况和没有设置loadOnStartup是一样的 loadOnStartup &gt; 0：web容器启动的时候做实例化处理，顺序是由小到大，正整数小的先被实例化 loadOnStartup = 0：web容器启动的时候做实例化处理，相当于是最大整数，因此web容器启动时，最后被实例 12345&lt;servlet&gt; &lt;servlet-name&gt;haha&lt;/servlet-name&gt; &lt;servlet-class&gt;demo.servlet.MyServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt; 3. Servlet线程安全问题从Servlet的生命周期可以看出，Servlet是单例的 这说明多个用户同时访问时，可能存在线程安全问题 为了避免发生线程安全问题，要使用变量时，尽量定义局部变量，而不要使用成员变量。因为局部变量在栈中，多个线程的栈是不共享的。 如果定义了成员变量，仅仅是读取，那是可以的，但是千万不要去修改123456789public class MyServlet extends HttpServlet &#123; private Integer age = 100; // 尽量不定义成员变量 @Override protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; int age = 100; // 尽量使用局部变量 &#125;&#125;]]></content>
      <tags>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Servlet HelloWorld]]></title>
    <url>%2F2019%2F04%2F16%2FJava%2FServlet%2FServlet-HelloWorld%2F</url>
    <content type="text"><![CDATA[1. Servlet HelloWorld Maven环境1.1. 依赖123456&lt;dependency&gt; &lt;groupId&gt;javax.servlet&lt;/groupId&gt; &lt;artifactId&gt;servlet-api&lt;/artifactId&gt; &lt;version&gt;2.5&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 1.2. 编写Servlet继承HttpServlet，重写service方法12345678public class MyServlet extends HttpServlet &#123; @Override protected void service(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException &#123; // 输出响应的内容 resp.getWriter().write("&lt;h1&gt;HelloWorld&lt;/h1&gt;"); &#125;&#125; 1.3. 配置Serlvet编辑web.xml，配置Servlet123456789101112&lt;servlet&gt; &lt;!-- 给servlet取名，名称随意 --&gt; &lt;servlet-name&gt;haha&lt;/servlet-name&gt; &lt;!-- 指定Servlet类 --&gt; &lt;servlet-class&gt;demo.servlet.MyServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;!-- 与&lt;servlet&gt;的&lt;servlet-name&gt;对应 --&gt; &lt;servlet-name&gt;haha&lt;/servlet-name&gt; &lt;!-- 指定URL --&gt; &lt;url-pattern&gt;/hello&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 1.4. 测试运行tomcat，访问/hello 2. Servlet运行原理浏览器访问 http://localhost:8080/webapp1/hello 前面的localhost:8080，就指定了要访问哪个Tomcat Tomcat接受到请求，分析URL，根据虚拟目录webapp1找到对应的web应用 找到该web应用的web.xml，根据URI/hello找到对应的&lt;servlet-mapping&gt;，根据&lt;servlet-name&gt;找到关联的&lt;servlet&gt; 找到&lt;servlet-class&gt;，得到对应Servlet的全类名。Tomcat就会通过Class.forName()和cls.newInstance()反射创建Servlet，再执行service()处理本次请求 12345678&lt;servlet&gt; &lt;servlet-name&gt;haha&lt;/servlet-name&gt; &lt;servlet-class&gt;demo.servlet.MyServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;haha&lt;/servlet-name&gt; &lt;url-pattern&gt;/hello&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;]]></content>
      <tags>
        <tag>Servlet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interview 数据库]]></title>
    <url>%2F2019%2F04%2F16%2FInterview%2FInterview-%E6%95%B0%E6%8D%AE%E5%BA%93%2F</url>
    <content type="text"><![CDATA[1. 数据库理论1.1. 简单介绍一下关系数据库三范式？参考 https://blog.csdn.net/asdfsadfasdfsa/article/details/83963046 https://www.cnblogs.com/gdwkong/p/9012262.html 首先讲一下什么是范式，范式就是规范，就是设计关系（表）时要遵循的规范 第一范式（1NF）：字段（列）具有原子性，不可再分123456789101112131415161718192021有一列可以拆分为firstName和lastName，不满足1NF+---------+-----------------------+| id | firstName , lastName |+---------+-----------------------+| 1 | Abbot , aa1 || 2 | Doris , aa2 || 3 | Emerson , aa3 || 4 | Green , aa4 || 5 | Jeames , aa5 |+---------+-----------|-----------+解决办法是，拆分为多列，就满足1NF了+---------+-----------------------+| id | firstName | lastName |+---------+-----------------------+| 1 | Abbot | aa1 || 2 | Doris | aa2 || 3 | Emerson | aa3 || 4 | Green | aa4 || 5 | Jeames | aa5 |+---------+-----------|-----------+ 二范式（2NF）：满足1NF，且非主属性完全依赖于主键，消除了部分依赖 三范式（3NF）：满足2NF，不存在传递依赖 反三范式：有的时候为了效率，可以设置重复或者可以推导出的字段 2. 数据库优化2.1. 有没有做过数据库优化方面的事情？做过MySQL数据库的优化 定位：查找、定位慢查询 优化手段： 创建索引：创建合适的索引，我们就可以现在索引中查询，查询到以后直接找对应的记录。 分表：当一张表的数据比较多或者一张表的某些字段的值比较多并且很少使用时，采用水平分表和垂直分表来优化 读写分离：当一台服务器不能满足需求时，采用读写分离的方式进行集群。 缓存：使用Redis来进行缓存 2.2. 你是怎么查询并定位慢查询的？在项目自验项目转测试之前，在启动mysql数据库时开启慢查询，并且把执行慢的语句写到日志中，在运行一定时间后。通过查看日志找到慢查询语句 3. Redis整理3.1. 有没有使用过Redis？Redis是什么？]]></content>
      <tags>
        <tag>Interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interview Linux]]></title>
    <url>%2F2019%2F04%2F15%2FInterview%2FInterview-Linux%2F</url>
    <content type="text"><![CDATA[1. 有没有用过Linux？你都用它来做什么？ 因为Linux能长时间比较稳定地运行，所以我一般拿它来做服务器。比如部署tomcat/mysql/hadoop/nginx Linux具有C的编译环境，一些软件是没有包的（如redis、nginx等等），需要在Linux环境下编译得到软件包 2. 説一下Linux下面的常用命令计划任务我会用。。 部署爬虫我会用。。 文件操作。。。 fdisk。。 性能优化命令 top/ 3. 你是使用什么来连接远程Linux服务器的？首先在Linux服务器安装ssh服务，开放22端口 然后使用ssh客户端连接Linux服务器，使用命令行来操作。 使用sftp客户端上传下载文件。]]></content>
      <tags>
        <tag>Interview</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM 简介]]></title>
    <url>%2F2019%2F04%2F15%2FJava%2FJVM%2FJVM-%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"></content>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Interview JavaSE 基础]]></title>
    <url>%2F2019%2F04%2F15%2FInterview%2FJavaSE%2FInterview-JavaSE-%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[1. 数据类型专题1.1. 为什么Java没有无符号整型？ Java主要是用来处理业务逻辑的，无符号数的用处少 简化开发 1.2. Java中有没有指针？有指针，但是隐藏了，开发人员无法直接操作指针，由jvm来操作指针 1.3. String是基本数据类型吗？答：不是。Java中的基本数据类型只有8个：byte、short、int、long、float、double、char、boolean；除了基本类型（primitive type），剩下的都是引用类型（reference type）。 1.4. char类型变量能不能储存一个中文的汉字，为什么？答：char用来储存一个Unicode编码字符，Unicode字符集包含了汉字，所以char类型自然就能存储汉字，但是在某些特殊情况下某个生僻汉字可能没有包含在 Unicode 编码字符集中，这种情况下 char 类型就不能存储该生僻汉字了 1.5. double能赋值给float吗（代码）以下代码是否正确？1float f = 3.4; 答：不正确。3.4是双精度数，将双精度型（double）赋值给浮点型（float）属于下转型（down-casting，也称为窄化）会造成精度损失，因此需要强制类型转换float f = (float)3.4; 或者写成float f = 3.4F; 1.6. int能赋值给short吗？（代码）以下代码是否正确？12short s = 1; // 正确short s = 65536; // 错误。Incompatible types, Required short, Found int short s = 1 是正确的。虽然说常量1是int类型，原则上int不能赋值给short，但是编译器看到常量1在short的范围内，所以会将常量1自动转换为short类型（注意这不叫隐式转换） short s = 65536 是错误的。因为65536超过short的范围，编译器无法将65536自动转换为short，只好视为int，因为int无法赋值给short，所以报错 以下代码是否正确？12short s = 1;s = s + 1; // 错误。Incompatible types, Required short, Found int s = s + 1 是错误的。s + 1隐式转换为int，int无法赋值给short 以下代码是否正确？12short s = 1;s += 1; // 正确，相当于 s = (short)(s + 1) https://blog.csdn.net/weixin_42405708/article/details/81069625 有人从JVM反汇编的角度，论证s += 1与s = (short)(s + 1)完全一致 1.7. Integer和int有什么区别？答：int 是 java 内置基本数据类型之一，java 为每个基本类型都提供了一个封装类，Integer 就是 int 的封装类（也叫包装类型）；int 变量的默认值为 0，Integer 变量的默认值为 null，所以 Integer 可以区分出未赋值和值为 0 的区别；Integer 类内部提供了一些关于整数操作的方法，例如上文用到的表示整数的最大值和最小值。 1.8. 有了基本数据类型，为什么还要包装类型？基本类型并不具有对象的性质，为了让基本类型也具有对象的特征，就有了包装类。包装类为其添加了属性和方法，丰富了基本类型的操作。这样在程序中，我们就可以统一以面向对象的方式来操作数据 包装类能区分0和null，但是基本类型不能区分。比如Student有一个score字段，代表分数。你用基本数据类型，就不能分区这个学生得了0分，还是没有成绩 1.9. 什么是装箱和拆箱？装箱：把基本数据类型转换成对应的包装类型拆箱：把包装类型转换为对应的基本数据类型 1.10. 手动装箱拆箱和自动装箱拆箱的主要区别？手动装箱：显式调用包装类的valueOf，将基本类型转为包装类自动装箱：编译器自动调用valueOf12Integer a = Integer.valueOf(1); // 手动装箱Integer b = 1; // 自动装箱，编译器会自动调用Integer.valueof(1) 手动拆箱：显式调用包装类的intValue，将包装类转为基本类型自动拆箱：编译器自动调用intValue123Integer a = 1;int b = a.intValue(); // 手动拆箱int c = a; // 自动拆箱 1.11. boolean占几个字节？boolean不可能占1bit。因为计算机处理数据的最小单元是1byte=8bit boolean单独使用时，相当于int，占4字节。《Java虚拟机规范》中提到，虽然Java定义了boolean数据类型，但JVM没有任何操作boolean专用的字节码指令。故编译之后用int来取代boolean。为什么用int而不是byte或short？因为int类型存取更高效。 boolean[]数组编译后会被byte[]数组代替，此时的boolean占1字节 但是也不能绝对说boolean就是占4字节或者1字节，因为这只是JVM规范。具体各个JVM实现（OracleJDK、OpenJDK）不一定会按照这个规范来 1.12. String的字面量和new创建方式一样吗？1234567String s1 = "aaa"; // 通过字面量创建String s2 = new String("aaa"); // 通过new创建String s3 = "aaa";System.out.println(s1 == s2); // falseSystem.out.println(s1.equals(s2)); // trueSystem.out.println(s1 == s3); // trueSystem.out.println(s1.equals(s3)); // true 字符串有new和字面量2种创建方式。但是字面量使用了缓存技术，故一般情况下均使用字面量方式创建。 new创建：若常量区没有&quot;abc&quot;，则先在常量池创建&quot;abc&quot;，再在堆中创建&quot;abc&quot;。若常量池已有&quot;abc&quot;，则只用在堆中创建&quot;abc&quot;。最后栈中开辟s1，指向堆中的&quot;abc&quot;对象 字面量创建：若常量池没有&quot;abc&quot;，则先在常量池创建&quot;abc&quot;，最后栈中开辟s1，指向常量池中的&quot;abc&quot;对象。若常量 池已经有&quot;abc&quot;，则直接开辟s1，指向已存在的&quot;abc&quot; 2. 集合框架专题2.1. 讲一下Java中的集合?Java中的集合分为value和key-value两种，或者说分为Collection和Map两种 存储value（Collection）分为List和Set List是有序的（有序是指存储的顺序与添加时的顺序一致），可以重复的 Set是无序的（无序是指存储的顺序与添加时的顺序不一致，LinkedHashSet是特例），不可以重复的。根据equals和hashcode判断，也就是如果一个对象要存储在Set中，必须重写equals和hashCode方法。 存储key-value的为Map. 2.2. ArrayList和LinkedList的区别?ArrayList底层使用时数组。LinkedList使用的是链表。 数组查询具有所有查询特定元素比较快。而插入和删除和修改比较慢(数组在内存中是一块连续的内存，如果插入或删除是需要移动内存)。 链表不要求内存是连续的，在当前元素中存放下一个或上一个元素的地址。查询时需要从头部开始，一个一个的找。所以查询效率低。插入时不需要移动内存，只需改变引用指向即可。所以插入或者删除的效率高。 ArrayList使用在查询比较多，但是插入和删除比较少的情况，而LinkedList使用在查询比较少而插入和删除比较多的情况。 3. IO流整理3.1. Java的IO流分为哪两种？按功能来分：输入流(input)，输出流(output)按类型来分：字节流，字符流 3.2. 字节流与字符流的区别以字节为单位输入输出数据，字节流按照8位传输以字符为单位输入输出数据，字符流按照16位传输 3.3. 实现一个拷贝文件的工具类使用字节流还是字符流?我们拷贝的文件不确定是只包含字符流，有可以能有字节流(图片、声音、图像等)，为考虑到通用性，要使用字节流。 4. 基础语法整理4.1. Java有没有goto？答：goto 是Java中的保留字，在目前版本的Java中没有使用 4.2. “==”与equals有什么区别？首先讲一下==的作用： 对于基本数据类型，判断值是否相等 对于引用类型，判断堆内存地址是否相等（是不是指向同一个对象） equals是Object对象的一个方法，默认行为与与==是一致的。String以及所有的包装类都重写了该方法 4.3. String/StringBuilder/StringBuffer的区别String/StringBuillder/StringBuffer都可以表示和操作字符串，字符串就是多个字符的集合 在内容上： String是不可变的，底层使用final char value[] StringBuilder/StringBuffer是可变的，底层使用char[] value 在操作字符串时（如拼接操作），尽量使用StringBuilder/StringBuffer，而不去使用String。因为String不可变，每操作一次就要生成新的对象 StringBuffer与StringBuilder的区别： StringBuffer是线程安全的，底层所有方法都使用用synchronized加锁，但加锁之后效率就变低 StringBuilder是线程不安全的，没有加锁，所以相对StringBuffer，效率较高 4.4. final关键字有哪些用法final可以修饰类、方法、变量。 final类：该类不能被继承，比如String final方法：子类不能重写该方法 final变量：常量 4.5. static关键字有哪些用法static可以修饰内部类、方法、变量、代码块 static修饰的类是静态内部类（外部类不能用static修饰） static修饰的方法是静态方法，表示该方法属于当前类的，而不属于某个对象 static修饰变量是静态变量或者叫类变量 static修饰的代码块叫静态代码块，当类被加载类会被执行一次 4.6. Java中操作字符串使用哪些类？String，StringBuffer，StringBuilder 4.7. ++i与i++的区别++i：先赋值，后计算i++：先计算，后赋值 4.8. Java常用包有那些java.langjava.iojava.sqljava.utiljava.netjava.math 4.9. Java最顶级的父类是哪个Object 4.10. Object类常用方法有那些？equalshashcodetoStringwaitnotifyclonegetClass 4.11. java函数传参是值传递和引用传递Java只有值传递（这里的引用传递是C++中的引用） 对于基本数据类型，传递的是它的副本 对于引用类型，传递的是引用的副本，还是按值传递 4.12. 实例化数组后，能不能改变数组长度呢？不能，数组一旦实例化，它的长度就是固定的 4.13. 构造方法能不能重写？能不能重载？可以重写，也可以重载 4.14. 什么是方法重载？存在两个方法，方法名相同，但是参数类型或者参数个数不同 4.15. override、overwrite、overload的区别override：覆盖overwrite：重写overload：重载 重写和覆盖是等价的，重载是指函数重载 4.16. 如何将字符串反转？StringBuilder或者StringBuffer的reverse方法 1System.out.println(new StringBuilder("213").reverse()); 4.17. 面向对象的语言有那些特征？封装、继承、多态 4.18. Java中的继承是单继承还是多继承Java中既有单继承，又有多继承。对于java类来说只能有一个父类，对于接口来说可以同时继承多个接口 4.19. 如果一个类中有抽象方法，那么这个一定是抽象类？包含抽象方法的类一定是抽象类 4.20. 抽象类可以使用final修饰吗？不可以。定义抽象类就是让其他继承的，而final修饰类表示该类不能被继承，与抽象类的理念违背了 4.21. 普通类与抽象类有什么区别？普通类不能包含抽象方法，抽象类可以包含抽象方法抽象类不能直接实例化，普通类可以直接实例化 4.22. 除了使用new创建对象之外，还可以用什么方法创建对象？Java反射 4.23. Java反射创建对象效率高还是通过new创建对象的效率高？通过new创建对象的效率比较高。通过反射时，先找查找类资源，使用类加载器创建，过程比较繁琐，所以效率较低 4.24. Java中集合框架的有几个？Coillection、Map 5. JDBC整理5.1. JDBC操作的步骤 加载数据库驱动类 打开数据库连接 执行sql语句 处理返回结果 关闭资源 5.2. 在使用jdbc的时候，如何防止出现sql注入的问题使用PreparedStatement类，而不是使用Statement类 5.3. 怎么在JDBC内调用一个存储过程使用CallableStatement 5.4. 是否了解连接池，使用连接池有什么好处？数据库连接是非常消耗资源的，影响到程序的性能指标。连接池是用来分配、管理、释放数据库连接的，可以使应用程序重复使用同一个数据库连接，而不是每次都创建一个新的数据库连接。通过释放空闲时间较长的数据库连接避免数据库因为创建太多的连接而造成的连接遗漏问题，提高了程序性能。 6. 异常整理6.1. Java中异常分为哪两种？编译时异常运行时异常 6.2. 说几个常见的编译时异常类？NullPointerException：空指针异常ArrayIndexOutOfBoundsException：数组下标越界NumberFormatException：数字转换异常IllegalArgumentException：参数不匹配异常InstantiationException：对象初始化异常ArithmeticException：算术异常 6.3. 如何自定义一个异常继承一个异常类，通常是RumtimeException或者Exception 6.4. try.catch.finally是必须要存在的吗？try块必须存在，catch和finally可以不存在，但不能同时不存在 6.5. throw与throws区别throw 表示抛出异常。写在代码块内，throw后面跟的是一个具体的异常实例 throws 声明该方法会出现哪些异常。写在方法后面，throws后面跟的是异常类，异常类可以出现多个 6.6. Error与Exception区别？Error和Exception都是java错误处理机制的一部分，都继承了Throwable类。 Exception表示的异常，异常可以通过程序来捕捉，或者优化程序来避免。 Error表示的是系统错误，不能通过程序来进行错误处理。 7. JVM专题整理7.1. JDK和JRE的区别JRE（java runtime environment）是指Java运行环境 JDK（java development kit）是Java开发工具包。JDK包含了JRE 7.2. 简单讲一下Java跨平台的原理由于各操作系统（windows,liunx等）支持的指令集，不是完全一致的。就会让我们的程序在不同的操作系统上要执行不同程序代码。Java开发了适用于不同操作系统及位数的java虚拟机来屏蔽个系统之间的差异，提供统一的接口。对于我们java开发者而言，你只需要在不同的系统上安装对应的不同java虚拟机、这时你的java程序只要遵循java规范，就可以在所有的操作系统上面运行java程序了 8. 知识面扩展整理8.1. Java之父是？詹姆斯·高斯林（James Gosling）]]></content>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 627 交换工资]]></title>
    <url>%2F2019%2F04%2F15%2FProblem%2FLeetCode%2FLeetCode-Database-627-%E4%BA%A4%E6%8D%A2%E5%B7%A5%E8%B5%84%2F</url>
    <content type="text"><![CDATA[1. 题目描述给定一个 salary 表，如下所示，有 m=男性 和 f=女性 的值 。交换所有的 f 和 m 值（例如，将所有 f 值更改为 m，反之亦然）。要求使用一个更新（Update）语句，并且没有中间临时表。 请注意，你必须编写一个 Update 语句，不要编写任何 Select 语句。 例如: 123456| id | name | sex | salary ||----|------|-----|--------|| 1 | A | m | 2500 || 2 | B | f | 1500 || 3 | C | m | 5500 || 4 | D | f | 500 | 运行你所编写的更新语句之后，将会得到以下表: 123456| id | name | sex | salary ||----|------|-----|--------|| 1 | A | f | 2500 || 2 | B | m | 1500 || 3 | C | f | 5500 || 4 | D | m | 500 | 2. 题解2.1. MySQL（IF函数）IF(expr,v1,v2) 如果表达式 expr 成立，返回结果 v1；否则，返回结果 v2。 12UPDATE salary SET sex=IF(sex='m', 'f', 'm') 2.2. MySQL（CASE语句）123456789101112131415-- 形式1UPDATE salary SET sex = CASE WHEN sex = 'm' THEN 'f' ELSE 'm' END-- 形式2UPDATE salary SET sex = CASE sex WHEN 'm' THEN 'f' ELSE 'm' END]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 626 换座位]]></title>
    <url>%2F2019%2F04%2F15%2FProblem%2FLeetCode%2FLeetCode-Database-626-%E6%8D%A2%E5%BA%A7%E4%BD%8D%2F</url>
    <content type="text"><![CDATA[1. 题目描述小美是一所中学的信息科技老师，她有一张 seat 座位表，平时用来储存学生名字和与他们相对应的座位 id。 其中纵列的 id 是连续递增的 小美想改变相邻俩学生的座位。 你能不能帮她写一个 SQL query 来输出小美想要的结果呢？ 示例： 123456789+---------+---------+| id | student |+---------+---------+| 1 | Abbot || 2 | Doris || 3 | Emerson || 4 | Green || 5 | Jeames |+---------+---------+ 假如数据输入的是上表，则输出结果如下： 123456789+---------+---------+| id | student |+---------+---------+| 1 | Doris || 2 | Abbot || 3 | Green || 4 | Emerson || 5 | Jeames |+---------+---------+ 注意： 如果学生人数是奇数，则不需要改变最后一个同学的座位。 2. 题解2.1. MySQL题目没有描述换座位的规则，从input/output可以看出，是一对一对相邻的两个换座位，最后有多余的人就不换座位 分3种情况 id为奇数，且不是最后1条记录，则id = id + 1 id为奇数，且是最后1条记录，则id不变 id为偶数，id = id - 1 123456789SELECT ( CASE WHEN id MOD 2 = 1 AND id &lt; cnt THEN id + 1 -- id为奇数，且不是最后1条记录，则id = id + 1 WHEN id MOD 2 = 1 AND id = cnt THEN id -- id为奇数，且是最后1条记录，则id不变 ELSE id - 1 -- id为偶数，id = id - 1 END) AS id, student -- 小技巧：在FROM处添加临时统计结果COUNTFROM seat, (SELECT COUNT(*) AS cnt FROM seat) AS cntORDER BY id -- 修改完ID，顺序已经乱了，所以要ORDER BY]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 620 有趣的电影]]></title>
    <url>%2F2019%2F04%2F15%2FProblem%2FLeetCode%2FLeetCode-Database-620-%E6%9C%89%E8%B6%A3%E7%9A%84%E7%94%B5%E5%BD%B1%2F</url>
    <content type="text"><![CDATA[1. 题目描述某城市开了一家新的电影院，吸引了很多人过来看电影。该电影院特别注意用户体验，专门有个 LED显示板做电影推荐，上面公布着影评和相关电影描述。 作为该电影院的信息部主管，您需要编写一个 SQL查询，找出所有影片描述为非 boring (不无聊) 的并且 id 为奇数 的影片，结果请按等级 rating 排列。 例如，下表 cinema: 123456789+---------+-----------+--------------+-----------+| id | movie | description | rating |+---------+-----------+--------------+-----------+| 1 | War | great 3D | 8.9 || 2 | Science | fiction | 8.5 || 3 | irish | boring | 6.2 || 4 | Ice song | Fantacy | 8.6 || 5 | House card| Interesting| 9.1 |+---------+-----------+--------------+-----------+ 对于上面的例子，则正确的输出是为： 123456+---------+-----------+--------------+-----------+| id | movie | description | rating |+---------+-----------+--------------+-----------+| 5 | House card| Interesting| 9.1 || 1 | War | great 3D | 8.9 |+---------+-----------+--------------+-----------+ 2. 题解2.1. MySQL取模有两种形式 a MOD b MOD(a, b) 12345SELECT id, movie, description, ratingFROM cinemaWHERE id MOD 2 = 1 -- 奇数AND description != 'boring' -- 非boringORDER BY rating DESC -- 结果请按等级 rating 排列]]></content>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 601 体育馆的人流量]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-601-%E4%BD%93%E8%82%B2%E9%A6%86%E7%9A%84%E4%BA%BA%E6%B5%81%E9%87%8F%2F</url>
    <content type="text"><![CDATA[1. 题目描述X 市建了一个新的体育馆，每日人流量信息被记录在这三列信息中：序号 (id)、日期(date)、 人流量 (people)。 请编写一个查询语句，找出高峰期时段，要求连续三天及以上，并且每天人流量均不少于100。 例如，表 stadium： 123456789101112+------+------------+-----------+| id | date | people |+------+------------+-----------+| 1 | 2017-01-01 | 10 || 2 | 2017-01-02 | 109 || 3 | 2017-01-03 | 150 || 4 | 2017-01-04 | 99 || 5 | 2017-01-05 | 145 || 6 | 2017-01-06 | 1455 || 7 | 2017-01-07 | 199 || 8 | 2017-01-08 | 188 |+------+------------+-----------+ 对于上面的示例数据，输出为： 12345678+------+------------+-----------+| id | date | people |+------+------------+-----------+| 5 | 2017-01-05 | 145 || 6 | 2017-01-06 | 1455 || 7 | 2017-01-07 | 199 || 8 | 2017-01-08 | 188 |+------+------------+-----------+ Note:每天只有一行记录，日期随着 id 的增加而增加。 2. 题解2.1. MySQLNote提及：每天只有一行记录，日期随着 id 的增加而增加。所以相邻3天不用必DATEDIFF来判断，直接用3条记录的id值递增1就可以来判断 注意：题目描述有问题，一查测试数据，发现date字段名是visit_date 12345678SELECT DISTINCT tb1.id, tb1.visit_date, tb1.people -- 要去重FROM stadium AS tb1, stadium AS tb2, stadium AS tb3WHERE tb1.people &gt;= 100 AND tb2.people &gt;= 100 AND tb3.people &gt;= 100 -- 每天人流量均不少于100-- 3种情况都在考虑，即这一天是3天中的第1天，还是第2天，还是第3天AND ((tb1.id + 1 = tb2.id AND tb2.id + 1 = tb3.id) OR (tb2.id + 1 = tb1.id AND tb1.id + 1 = tb3.id) OR (tb2.id + 1 = tb3.id AND tb3.id + 1 = tb1.id))ORDER BY tb1.id]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 596 超过5名学生的课]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-596-%E8%B6%85%E8%BF%875%E5%90%8D%E5%AD%A6%E7%94%9F%E7%9A%84%E8%AF%BE%2F</url>
    <content type="text"><![CDATA[1. 题目描述有一个courses 表 ，有: student (学生) 和 class (课程)。 请列出所有超过或等于5名学生的课。 例如,表: 12345678910111213+---------+------------+| student | class |+---------+------------+| A | Math || B | English || C | Math || D | Biology || E | Math || F | Computer || G | Math || H | Math || I | Math |+---------+------------+ 应该输出: 12345+---------+| class |+---------+| Math |+---------+ 2. 题解2.1. MySQL注意：这表里有重复记录的，比如A学生选了Math的记录可以有多条，所以COUNT要用DISTINCT去重 12SELECT class FROM coursesGROUP BY class HAVING COUNT(DISTINCT(student)) &gt;= 5]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 595 大的国家]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-595-%E5%A4%A7%E7%9A%84%E5%9B%BD%E5%AE%B6%2F</url>
    <content type="text"><![CDATA[1. 题目描述这里有张 World 表 123456789+-----------------+------------+------------+--------------+---------------+| name | continent | area | population | gdp |+-----------------+------------+------------+--------------+---------------+| Afghanistan | Asia | 652230 | 25500100 | 20343000 || Albania | Europe | 28748 | 2831741 | 12960000 || Algeria | Africa | 2381741 | 37100000 | 188681000 || Andorra | Europe | 468 | 78115 | 3712000 || Angola | Africa | 1246700 | 20609294 | 100990000 |+-----------------+------------+------------+--------------+---------------+ 如果一个国家的面积超过300万平方公里，或者人口超过2500万，那么这个国家就是大国家。 编写一个SQL查询，输出表中所有大国家的名称、人口和面积。 例如，根据上表，我们应该输出: 123456+--------------+-------------+--------------+| name | population | area |+--------------+-------------+--------------+| Afghanistan | 25500100 | 652230 || Algeria | 37100000 | 2381741 |+--------------+-------------+--------------+ 2. 题解2.1. MySQL水题 123SELECT name, population, areaFROM WorldWHERE area &gt; 3000000 OR population &gt; 25000000]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 262 行程和用户]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-262-%E8%A1%8C%E7%A8%8B%E5%92%8C%E7%94%A8%E6%88%B7%2F</url>
    <content type="text"><![CDATA[1. 题目描述Trips 表中存所有出租车的行程信息。每段行程有唯一键 Id，Client_Id 和 Driver_Id 是 Users 表中 Users_Id 的外键。Status 是枚举类型，枚举成员为 (‘completed’, ‘cancelled_by_driver’, ‘cancelled_by_client’)。 1234567891011121314+----+-----------+-----------+---------+--------------------+----------+| Id | Client_Id | Driver_Id | City_Id | Status |Request_at|+----+-----------+-----------+---------+--------------------+----------+| 1 | 1 | 10 | 1 | completed |2013-10-01|| 2 | 2 | 11 | 1 | cancelled_by_driver|2013-10-01|| 3 | 3 | 12 | 6 | completed |2013-10-01|| 4 | 4 | 13 | 6 | cancelled_by_client|2013-10-01|| 5 | 1 | 10 | 1 | completed |2013-10-02|| 6 | 2 | 11 | 6 | completed |2013-10-02|| 7 | 3 | 12 | 6 | completed |2013-10-02|| 8 | 2 | 12 | 12 | completed |2013-10-03|| 9 | 3 | 10 | 12 | completed |2013-10-03| | 10 | 4 | 13 | 12 | cancelled_by_driver|2013-10-03|+----+-----------+-----------+---------+--------------------+----------+ Users 表存所有用户。每个用户有唯一键 Users_Id。Banned 表示这个用户是否被禁止，Role 则是一个表示（‘client’, ‘driver’, ‘partner’）的枚举类型。 123456789101112+----------+--------+--------+| Users_Id | Banned | Role |+----------+--------+--------+| 1 | No | client || 2 | Yes | client || 3 | No | client || 4 | No | client || 10 | No | driver || 11 | No | driver || 12 | No | driver || 13 | No | driver |+----------+--------+--------+ 写一段 SQL 语句查出 2013年10月1日 至 2013年10月3日 期间非禁止用户的取消率。基于上表，你的 SQL 语句应返回如下结果，取消率（Cancellation Rate）保留两位小数。 1234567+------------+-------------------+| Day | Cancellation Rate |+------------+-------------------+| 2013-10-01 | 0.33 || 2013-10-02 | 0.00 || 2013-10-03 | 0.50 |+------------+-------------------+ 2. 题解2.1. MySQL取消率由Status字段决定。Status字段的值可以为completed、cancelled_by_driver、cancelled_by_client，只有completed代表不取消，其它两个值就代表取消。那么取消率就是cancelled_by_driver和cancelled_by_client所占的比例 这里可以用CASE语句来统计，只有completed状态视为0，其它状态视为1，累加起来，最除以总数COUNT(*)，就是取消率1234CASE WHEN tb1.Status = 'completed' THEN 0 ELSE 1END ROUND(value, n) 表示对value四舍五入保留N位小数 12345678910111213-- 正确做法SELECT tb1.Request_at AS Day, ( ROUND(SUM(CASE WHEN tb1.Status = 'completed' THEN 0 ELSE 1 END) / COUNT(*), 2)) AS 'Cancellation Rate'FROM Trips tb1JOIN Users tb2ON tb1.Client_Id = tb2.Users_Id AND tb2.Banned = 'No' -- Client是非禁止用户JOIN Users tb3ON tb1.Driver_id = tb3.Users_Id AND tb3.Banned = 'No' -- Driver是非禁止用户WHERE tb1.Request_at BETWEEN '2013-10-01' AND '2013-10-03' -- 2013年10月1日 至 2013年10月3日GROUP BY tb1.Request_at -- 分组统计ORDER BY Day -- 日期从小到大]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 197 上升的温度]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-197-%E4%B8%8A%E5%8D%87%E7%9A%84%E6%B8%A9%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[1. 题目描述给定一个 Weather 表，编写一个 SQL 查询，来查找与之前（昨天的）日期相比温度更高的所有日期的 Id。 12345678+---------+------------------+------------------+| Id(INT) | RecordDate(DATE) | Temperature(INT) |+---------+------------------+------------------+| 1 | 2015-01-01 | 10 || 2 | 2015-01-02 | 25 || 3 | 2015-01-03 | 20 || 4 | 2015-01-04 | 30 |+---------+------------------+------------------+ 例如，根据上述给定的 Weather 表格，返回如下 Id: 123456+----+| Id |+----+| 2 || 4 |+----+ 2. 题解2.1. MySQL 上升的温度DATEDIFF(date1, date2) 返回date1-date2的日期天数差 12345SELECT DISTINCT(tb1.Id) FROM Weather tb1JOIN Weather tb2 ON DATEDIFF(tb1.RecordDate, tb2.RecordDate) = 1AND tb1.Temperature &gt; tb2.Temperature]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 196 删除重复的电子邮箱]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-196-%E5%88%A0%E9%99%A4%E9%87%8D%E5%A4%8D%E7%9A%84%E7%94%B5%E5%AD%90%E9%82%AE%E7%AE%B1%2F</url>
    <content type="text"><![CDATA[1. 题目说明编写一个 SQL 查询，来删除 Person 表中所有重复的电子邮箱，重复的邮箱里只保留 Id 最小 的那个。 12345678+----+------------------+| Id | Email |+----+------------------+| 1 | john@example.com || 2 | bob@example.com || 3 | john@example.com |+----+------------------+Id 是这个表的主键。 例如，在运行你的查询语句之后，上面的 Person 表应返回以下几行: 123456+----+------------------+| Id | Email |+----+------------------+| 1 | john@example.com || 2 | bob@example.com |+----+------------------+ 2. 题解2.1. MySQL（DELETE自连接）123456789-- 方式1DELETE tb1FROM Person tb1, Person tb2WHERE tb1.Email = tb2.Email AND tb1.Id &gt; tb2.Id-- 方式2DELETE FROM p1 USING Person p1,Person p2 WHERE p1.Email = p2.Email and p1.Id &gt; p2.Id]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 185 部门工资前三高的员工]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-185-%E9%83%A8%E9%97%A8%E5%B7%A5%E8%B5%84%E5%89%8D%E4%B8%89%E9%AB%98%E7%9A%84%E5%91%98%E5%B7%A5%2F</url>
    <content type="text"><![CDATA[1. 题目描述Employee 表包含所有员工信息，每个员工有其对应的 Id, salary 和 department Id 。 12345678910+----+-------+--------+--------------+| Id | Name | Salary | DepartmentId |+----+-------+--------+--------------+| 1 | Joe | 70000 | 1 || 2 | Henry | 80000 | 2 || 3 | Sam | 60000 | 2 || 4 | Max | 90000 | 1 || 5 | Janet | 69000 | 1 || 6 | Randy | 85000 | 1 |+----+-------+--------+--------------+ Department 表包含公司所有部门的信息。 123456+----+----------+| Id | Name |+----+----------+| 1 | IT || 2 | Sales |+----+----------+ 编写一个 SQL 查询，找出每个部门工资前三高的员工。例如，根据上述给定的表格，查询结果应返回： 123456789+------------+----------+--------+| Department | Employee | Salary |+------------+----------+--------+| IT | Max | 90000 || IT | Randy | 85000 || IT | Joe | 70000 || Sales | Henry | 80000 || Sales | Sam | 60000 |+------------+----------+--------+ 2. 题解2.1. MySQL（ORDER + LIMIT）12345678SELECT tb2.Name AS Department, tb1.Name AS Employee, tb1.SalaryFROM Employee tb1JOIN Department tb2 ON tb1.DepartmentId = IdWHERE tb1.Salary IN ( SELECT tb3.DepartmentId, tb3.Salary FROM Employee tb3 WHERE tb1.DepartmentId = tb3.DepartmentId ORDER BY tb3.Salary DESC LIMIT 3) 出错信息：IN/ALL/ANY/SOME子查询不支持LIMIT1This version of MySQL doesn&apos;t yet support &apos;LIMIT &amp; IN/ALL/ANY/SOME subquery&apos; 2.2. MySQL（COUNT统计TON N）123456789SELECT tb2.Name AS Department, tb1.Name AS Employee, tb1.SalaryFROM Employee tb1JOIN Department tb2 ON tb1.DepartmentId = tb2.IdWHERE ( -- COUNT统计该部门下，比当前员工Salary大的不同的Salary有多少个，如果小于3，说明当前员工的Salary属于前3 SELECT COUNT(DISTINCT Salary) FROM Employee tb3 WHERE tb1.DepartmentId = tb3.DepartmentId AND tb1.Salary &lt; tb3.Salary) &lt; 3]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 184 部门工资最高的员工]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-184-%E9%83%A8%E9%97%A8%E5%B7%A5%E8%B5%84%E6%9C%80%E9%AB%98%E7%9A%84%E5%91%98%E5%B7%A5%2F</url>
    <content type="text"><![CDATA[1. 题目描述Employee 表包含所有员工信息，每个员工有其对应的 Id, salary 和 department Id。 12345678+----+-------+--------+--------------+| Id | Name | Salary | DepartmentId |+----+-------+--------+--------------+| 1 | Joe | 70000 | 1 || 2 | Henry | 80000 | 2 || 3 | Sam | 60000 | 2 || 4 | Max | 90000 | 1 |+----+-------+--------+--------------+ Department 表包含公司所有部门的信息。 123456+----+----------+| Id | Name |+----+----------+| 1 | IT || 2 | Sales |+----+----------+ 编写一个 SQL 查询，找出每个部门工资最高的员工。例如，根据上述给定的表格，Max 在 IT 部门有最高工资，Henry 在 Sales 部门有最高工资。 123456+------------+----------+--------+| Department | Employee | Salary |+------------+----------+--------+| IT | Max | 90000 || Sales | Henry | 80000 |+------------+----------+--------+ 2. 题解2.1. MySQL（MAX子查询）子查询，找到当前员工所在部门的最高工资，如果当前员工的工资就是最高工资，则输出 1234567SELECT tb2.Name AS Department, tb1.Name AS Employee, tb1.SalaryFROM Employee tb1JOIN Department tb2 ON tb1.DepartmentId = tb2.IdWHERE tb1.Salary = ( -- 找出当前DepartmentId下，最高的工资 SELECT MAX(Salary) FROM Employee tb3 WHERE tb1.DepartmentId = tb3.DepartmentId) 2.2. MySQL（GROUP BY MAX）123456SELECT tb2.Name AS Department, tb1.Name AS Employee, tb1.SalaryFROM Employee tb1JOIN Department tb2 ON tb1.DepartmentId = tb2.IdWHERE (tb1.DepartmentId, tb1.Salary) IN ( SELECT DepartmentId, MAX(Salary) FROM Employee GROUP BY DepartmentId)]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 183 从不订购的客户]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-183-%E4%BB%8E%E4%B8%8D%E8%AE%A2%E8%B4%AD%E7%9A%84%E5%AE%A2%E6%88%B7%2F</url>
    <content type="text"><![CDATA[1. 题目描述某网站包含两个表，Customers 表和 Orders 表。编写一个 SQL 查询，找出所有从不订购任何东西的客户。 Customers 表： 12345678+----+-------+| Id | Name |+----+-------+| 1 | Joe || 2 | Henry || 3 | Sam || 4 | Max |+----+-------+ Orders 表： 123456+----+------------+| Id | CustomerId |+----+------------+| 1 | 3 || 2 | 1 |+----+------------+ 例如给定上述表格，你的查询应返回： 123456+-----------+| Customers |+-----------+| Henry || Max |+-----------+ 2. 题解2.1. MySQL (NOT IN)1234SELECT tb1.Name AS Customers FROM Customers tb1WHERE tb1.Id NOT IN ( SELECT DISTINCT(tb2.CustomerId) FROM Orders tb2) MySQL（LEFT JOIN）1234SELECT tb1.Name AS Customers FROM Customers tb1LEFT JOIN Orders tb2ON tb1.Id = tb2.CustomerIdWHERE tb2.Id IS NULL]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 182 查找重复的电子邮箱]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-182-%E6%9F%A5%E6%89%BE%E9%87%8D%E5%A4%8D%E7%9A%84%E7%94%B5%E5%AD%90%E9%82%AE%E7%AE%B1%2F</url>
    <content type="text"><![CDATA[1. 题目描述编写一个 SQL 查询，查找 Person 表中所有重复的电子邮箱。 示例： 1234567+----+---------+| Id | Email |+----+---------+| 1 | a@b.com || 2 | c@d.com || 3 | a@b.com |+----+---------+ 根据以上输入，你的查询应返回以下结果： 12345+---------+| Email |+---------+| a@b.com |+---------+ 说明：所有电子邮箱都是小写字母。 2. 题解2.1. MySQL（GROUP BY + HAVING）12SELECT Email FROM PersonGROUP BY Email HAVING COUNT(*) &gt; 1 2.2. MySQL（自连接）注意别忘了DISTINCT 123SELECT DISTINCT(tb1.Email) FROM Person tb1JOIN Person tb2ON tb1.Email = tb2.Email AND tb1.Id != tb2.Id]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 181 超过经理收入的员工]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-181-%E8%B6%85%E8%BF%87%E7%BB%8F%E7%90%86%E6%94%B6%E5%85%A5%E7%9A%84%E5%91%98%E5%B7%A5%2F</url>
    <content type="text"><![CDATA[1. 题目描述Employee 表包含所有员工，他们的经理也属于员工。每个员工都有一个 Id，此外还有一列对应员工的经理的 Id。 12345678+----+-------+--------+-----------+| Id | Name | Salary | ManagerId |+----+-------+--------+-----------+| 1 | Joe | 70000 | 3 || 2 | Henry | 80000 | 4 || 3 | Sam | 60000 | NULL || 4 | Max | 90000 | NULL |+----+-------+--------+-----------+ 给定 Employee 表，编写一个 SQL 查询，该查询可以获取收入超过他们经理的员工的姓名。在上面的表格中，Joe 是唯一一个收入超过他的经理的员工。 12345+----------+| Employee |+----------+| Joe |+----------+ 2. 题解2.1. MySQL 题解考点：JOIN自连接 1234SELECT tb1.Name AS Employee FROM Employee tb1 JOIN Employee tb2ON tb1.ManagerId = tb2.IdAND tb1.Salary &gt; tb2.Salary]]></content>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 180 连续出现的数字]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-180-%E8%BF%9E%E7%BB%AD%E5%87%BA%E7%8E%B0%E7%9A%84%E6%95%B0%E5%AD%97%2F</url>
    <content type="text"><![CDATA[1. 题目描述编写一个 SQL 查询，查找所有至少连续出现三次的数字。 1234567891011+----+-----+| Id | Num |+----+-----+| 1 | 1 || 2 | 1 || 3 | 1 || 4 | 2 || 5 | 1 || 6 | 2 || 7 | 2 |+----+-----+ 例如，给定上面的 Logs 表， 1 是唯一连续出现至少三次的数字。 12345+-----------------+| ConsecutiveNums |+-----------------+| 1 |+-----------------+ 2. 题解如果题目要求离散出现至少3次，则用GROUP BY + HAVING，但是注意现在题目要求连续出现， 可以通过3表连接，描述连续出现3次的情况1234-- 返回的Num必须用DISTINCT去重，因为连续出现3次以上，同一个数字就会返回多次SELECT DISTINCT(t1.Num) AS ConsecutiveNumsFROM Logs t1, Logs t2, Logs t3 -- 3表自连接WHERE t1.id + 1 = t2.id AND t2.id + 1 = t3.id AND t1.Num = t2.Num AND t2.Num = t3.Num -- 连续出现3个相同的数字]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 178 分数排名]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-178-%E5%88%86%E6%95%B0%E6%8E%92%E5%90%8D%2F</url>
    <content type="text"><![CDATA[1. 题目描述编写一个 SQL 查询来实现分数排名。如果两个分数相同，则两个分数排名（Rank）相同。请注意，平分后的下一个名次应该是下一个连续的整数值。换句话说，名次之间不应该有“间隔”。 12345678910+----+-------+| Id | Score |+----+-------+| 1 | 3.50 || 2 | 3.65 || 3 | 4.00 || 4 | 3.85 || 5 | 4.00 || 6 | 3.65 |+----+-------+ 例如，根据上述给定的 Scores 表，你的查询应该返回（按分数从高到低排列）： 12345678910+-------+------+| Score | Rank |+-------+------+| 4.00 | 1 || 4.00 | 1 || 3.85 | 2 || 3.65 | 3 || 3.65 | 3 || 3.50 | 4 |+-------+------+ 2. 题解2.1. MySQL 题解计算出比当前Score大的不同的Score有多少个，再加1，就是对应的排名 123456SELECT tb1.Score, ( SELECT COUNT(DISTINCT Score) FROM Scores AS tb2 WHERE tb2.Score &gt; tb1.Score) + 1 AS RankFROM Scores AS tb1ORDER BY tb1.Score DESC]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 177 第N个最高工资]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-177-%E7%AC%ACN%E4%B8%AA%E6%9C%80%E9%AB%98%E5%B7%A5%E8%B5%84%2F</url>
    <content type="text"><![CDATA[1. 题目描述编写一个 SQL 查询，获取 Employee 表中第 n 高的薪水（Salary）。 1234567+----+--------+| Id | Salary |+----+--------+| 1 | 100 || 2 | 200 || 3 | 300 |+----+--------+ 例如上述 Employee 表，n = 2 时，应返回第二高的薪水 200。如果不存在第 n 高的薪水，那么查询应返回 null。 12345+------------------------+| getNthHighestSalary(2) |+------------------------+| 200 |+------------------------+ 2. 题解2.1. MySQL 题解（ORDER BY + LIMIT）首先，本题要求的第N个，是去重排序第N个，题目没有描述清楚。面试也一样，让你求TOP N，先问清楚要不要去重 ORDER BY + LIMIT 可以解决TOPN问题 12345678910-- 正确做法CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INTBEGIN SET N = N - 1; RETURN ( SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT N, 1 );END GROUP BY可以替换DISTINCT1234567891011-- 正确做法CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INTBEGIN SET N = N - 1; RETURN ( SELECT Salary FROM Employee GROUP BY Salary ORDER BY Salary DESC LIMIT N, 1 );END 要注意的是，在MySQL语法上要求RETURN 返回SELECT语句时，要加上()，而且内部，不能直接引用函数参数 12345678CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INTBEGIN RETURN ( SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT N - 1, 1 -- 语法错误，不能直接引用N );END 如果想引用函数参数，必须先SET，或者再声明其它变量，引用函数参数12345678910111213141516171819202122-- 方式1CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INTBEGIN SET N = N - 1; -- 先SET RETURN ( SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT N, 1 );END-- 方式2CREATE FUNCTION getNthHighestSalary(N INT) RETURNS INTBEGIN DECLARE m INT; SET m = N - 1; RETURN ( SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT m, 1 );END]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 176 第二高的薪水]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-176-%E7%AC%AC%E4%BA%8C%E9%AB%98%E7%9A%84%E8%96%AA%E6%B0%B4%2F</url>
    <content type="text"><![CDATA[1. 题目描述编写一个 SQL 查询，获取 Employee 表中第二高的薪水（Salary） 。 1234567+----+--------+| Id | Salary |+----+--------+| 1 | 100 || 2 | 200 || 3 | 300 |+----+--------+ 例如上述 Employee 表，SQL查询应该返回 200 作为第二高的薪水。如果不存在第二高的薪水，那么查询应返回 null。 12345+---------------------+| SecondHighestSalary |+---------------------+| 200 |+---------------------+ 2. 题解2.1. MySQL 题解（ORDER BY + LIMIT）一开始我想到先用ORDER BY排序，再LIMIT 1,1取第2个 123SELECT Salary AS SecondHighestSalaryFROM Employee ORDER BY Salary DESC LIMIT 1,1 提交出错。当表中只有一条记录，要求输出结果为null，但是我的输出为空字符串。 123Input: &#123;&quot;headers&quot;: &#123;&quot;Employee&quot;: [&quot;Id&quot;, &quot;Salary&quot;]&#125;, &quot;rows&quot;: &#123;&quot;Employee&quot;: [[1, 100]]&#125;&#125;Output: &#123;&quot;headers&quot;:[&quot;SecondHighestSalary&quot;],&quot;values&quot;:[]&#125;Expected: &#123;&quot;headers&quot;:[&quot;SecondHighestSalary&quot;],&quot;values&quot;:[[null]]&#125; 我感觉很奇怪，到MySQL做了一下测试，发现记录只有一条时，就是返回null。没办法，暂时认为是系统的BUG吧。于是我修改了一下SQL，在外面套了一层SELECT 12SELECT (SELECT Salary FROM Employee ORDER BY Salary DESC LIMIT 1,1) AS SecondHighestSalary 提交还是出错，但是是新的错误，说明刚才的样例通过了。现在的错误是：表中有2条Salary相同的记录，要求返回null，但是我的输出的100。就是说，系统要求Salary的排序是经过去重的123输入 &#123;&quot;headers&quot;: &#123;&quot;Employee&quot;: [&quot;Id&quot;, &quot;Salary&quot;]&#125;, &quot;rows&quot;: &#123;&quot;Employee&quot;: [[1, 100], [2, 100]]&#125;&#125;输出 &#123;&quot;headers&quot;:[&quot;SecondHighestSalary&quot;],&quot;values&quot;:[[100]]&#125;预期结果 &#123;&quot;headers&quot;:[&quot;SecondHighestSalary&quot;],&quot;values&quot;:[[null]]&#125; 将排序修改为去重排序，提交成功，以下是正确做法：12SELECT (SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT 1,1) AS SecondHighestSalary 题解中，看到有的人使用了IFNULL(expr1, expr2)，这个函数的作用是，如果expr1不为NULL，则返回expr1，否则返回expr2。他们是这样做的，显然这里使用的IFNULL是多余的，去掉就是上面的正确做法，还是对的12SELECT IFNULL((SELECT DISTINCT Salary FROM Employee ORDER BY Salary DESC LIMIT 1,1), NULL) AS SecondHighestSalary 2.2. MySQL 题解（MAX嵌套）用两层MAX查询。第1次查出最大的Salary，第2次在去掉最大值的记录中，查询最大的Salary。使用MAX查询的好处是，已经是去重的结果，不需要使用DISTINCT123456-- 形式1：MAX嵌套查询，使用"等号"SELECT MAX(Salary) AS SecondHighestSalary FROM EmployeeWHERE Salary != (SELECT MAX(Salary) FROM Employee)-- 形式2：MAX嵌套查询，使用"小于号"SELECT MAX(Salary) AS SecondHighestSalary FROM EmployeeWHERE Salary &lt; (SELECT MAX(Salary) FROM Employee) 第1次查询最大值也可以不用MAX，改为ORDER BY + LIMIT，但是实现的思路还是没有变化。但是你要知道，MAX的运行效率要比ORDER BY高，但是面试时可能会要求你给出多种解法，所以还是记录一下。123SELECT MAX(Salary) AS SecondHighestSalary FROM EmployeeWHERE Salary != (SELECT Salary FROM Employee ORDER BY Salary LIMIT 1)]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode Database 175 组合两个表]]></title>
    <url>%2F2019%2F04%2F14%2FProblem%2FLeetCode%2FLeetCode-Database-175-%E7%BB%84%E5%90%88%E4%B8%A4%E4%B8%AA%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[1. 题目描述表1: Person 12345678+-------------+---------+| 列名 | 类型 |+-------------+---------+| PersonId | int || FirstName | varchar || LastName | varchar |+-------------+---------+PersonId 是上表主键 表2: Address 123456789+-------------+---------+| 列名 | 类型 |+-------------+---------+| AddressId | int || PersonId | int || City | varchar || State | varchar |+-------------+---------+AddressId 是上表主键 编写一个 SQL 查询，满足条件：无论 person 是否有地址信息，都需要基于上述两表提供 person 的以下信息： 1FirstName, LastName, City, State 2. 题解无论person是否有地址信息，都要提供person的信息。根据这一要求，可知使用LEFT JOIN 2.1. MySQL Solution1234SELECT FirstName, LastName, City, State FROM Person AS tb1LEFT JOIN Address AS tb2ON tb1.PersonId = tb2.PersonId; 注意：SQL语句最后的分号不要落了，一落可能面试就挂了]]></content>
      <tags>
        <tag>LeetCode Database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 生成随机测试数据]]></title>
    <url>%2F2019%2F04%2F14%2FMySQL%2FMySQL-%E7%94%9F%E6%88%90%E9%9A%8F%E6%9C%BA%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[1. 随机生成各种类型的数据1.1. 生成[a,b)范围内的浮点数RAND()生成[0,1)范围内的浮点数1SELECT RAND() RAND()*n生成[0,n)范围内的浮点数1SELECT RAND() * 100 RAND()*(b-a)+a生成[a,b)范围内的浮点数1SELECT RAND() * (100-60) + 60 1.2. 生成[a,b)范围内的整数FLOOR(RAND()*n)生成[0,n)范围内的整数1SELECT RAND() * 100 FLOOR(RAND()*(b-a))+a生成[a,b)范围内的整数1SELECT FLOOR(RAND()*(100-60)) + 60 SELECT FLOOR(RAND()*3) 1.3. 生成随机字符串SELECT MD5(RAND()) 生成32位随机的数字0~9和小写字母a~z的组合123456mysql&gt; SELECT MD5(RAND());+----------------------------------+| MD5(RAND()) |+----------------------------------+| d799e1b276adebb6e572ba6964731806 |+----------------------------------+ SELECT SHA(RAND()) 或者 SELECT SHA1(RAND()) 生成40位随机的数字0~9和小写字母a~z的组合。目前MySQL的SHA就是采用SHA-1123456mysql&gt; SELECT SHA(RAND());+------------------------------------------+| SHA(RAND()) |+------------------------------------------+| 0ba19012421eaf2c325f1c6371187f5aa8a5c9ee |+------------------------------------------+ 1.4. 生成随机日期MAKEDATE(year, dayOfYear) 指定年份，以及该年的第几天，得到对应日期12345678910111213141516171819202122232425262728mysql&gt; SELECT MAKEDATE(2018,1);+------------------+| MAKEDATE(2018,1) |+------------------+| 2018-01-01 |+------------------+mysql&gt; SELECT MAKEDATE(2018,300);+--------------------+| MAKEDATE(2018,300) |+--------------------+| 2018-10-27 |+--------------------+mysql&gt; SELECT MAKEDATE(2018,3660); -- 可以跨年+---------------------+| MAKEDATE(2018,3660) |+---------------------+| 2028-01-08 |+---------------------+1 row in set (0.05 sec)mysql&gt; SELECT MAKEDATE(2018,-1); -- 不可以为负数+-------------------+| MAKEDATE(2018,-1) |+-------------------+| NULL |+-------------------+ 注意年份可以用2位或4位数字来表示，2位数，70~99对应1970~1999，00~69对应2000~2069123456789101112mysql&gt; SELECT MAKEDATE(70,2);+----------------+| MAKEDATE(70,2) |+----------------+| 1970-01-02 |+----------------+mysql&gt; SELECT MAKEDATE(69,2);+----------------+| MAKEDATE(69,2) |+----------------+| 2069-01-02 |+----------------+ 指定年份，随机生成日期1SELECT MAKEDATE(2018,FLOOR(RAND()*365)) 生成随机日期1SELECT MAKEDATE(FLOOR(RAND()*20)+2000,FLOOR(RAND()*365)) 1.5. 生成随机时间MAKETIME(hour, minute, second) 生成时分秒 123456789101112mysql&gt; SELECT MAKETIME(1,1,1);+-----------------+| MAKETIME(1,1,1) |+-----------------+| 01:01:01 |+-----------------+mysql&gt; SELECT MAKETIME(10,20,30);+--------------------+| MAKETIME(10,20,30) |+--------------------+| 10:20:30 |+--------------------+ 生成随机时间1SELECT MAKETIME(FLOOR(RAND()*24),FLOOR(RAND()*60),FLOOR(RAND()*60)); 生成随机日期和时间123SELECT CONCAT( MAKEDATE(FLOOR(RAND()*20)+2000, FLOOR(RAND()*365)), ' ', MAKETIME(FLOOR(RAND()*24),FLOOR(RAND()*60),FLOOR(RAND()*60))) 2. 随机生成记录随机生成N条记录 12345678910111213141516171819202122232425262728293031323334-- 删除存储过程DROP PROCEDURE IF EXISTS generate_random_record;-- 临时修改SQL语句结束符，防止与存储过程的语句结束符冲突DELIMITER $$ -- 创建存储过程，用 "IN &lt;形参名&gt; 数据类型" 定义一个形参 CREATE PROCEDURE generate_random_record(IN n INT)BEGIN DECLARE age INT DEFAULT 0; DECLARE name VARCHAR(40) DEFAULT ''; DECLARE birthday DATETIME; -- 声明一个整数变量 DECLARE i INT DEFAULT 0; -- 开启事务 SET AUTOCOMMIT = 0; -- 循环N次，生成N条记录 WHILE i &lt; n DO SET age = FLOOR(RAND()*90) + 10; SET name = MD5(RAND()); SET birthday = MAKEDATE(FLOOR(RAND()*20+2000),FLOOR(RAND()*365)); INSERT INTO person (age, name, birthday) VALUES (age, name, birthday); SET i = i + 1; END WHILE; -- 提交事务 SET AUTOCOMMIT = 1;END $$-- 恢复SQL语句的默认结束符（分号）DELIMITER ; -- 调用存储过程CALL generate_random_record(20); 特别注意，设置开启事务和提交事务，实现批量插入的效果，能大幅提高效率]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 包管理]]></title>
    <url>%2F2019%2F04%2F12%2FLinux%2FUbuntu-%E5%8C%85%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[1. 错误 Could not get lock /var/lib/dpkg/lock 的解决方案在使用Ubuntu的过程中，这个问题我遇到N遍了，每次解决之后就忘了当时是如何解决的，所以这次就记录下来 我在使用apt-get安装软件包时，遇到以下错误123$ apt-get install -y treeE: Could not get lock /var/lib/dpkg/lock - open (11: Resource temporarily unavailable)E: Unable to lock the administration directory (/var/lib/dpkg/), is another process using it? 查了一下网上的资料，得知出现这个问题的原因可能是：apt-get进程可能没有结束，此时你再次运行apt-get，就会出现上述错误。即有另一个程序正在占用apt-get install进程，由于它在运行时，会占用软件源更新时的系统锁（简称系统更新锁），此时资源被锁。 查看apt相关进程，发现确实上锁了123$ ps -ef | grep aptroot 2939 1 0 17:47 ? 00:00:00 /bin/sh /usr/lib/apt/apt.systemd.daily installroot 2943 2939 0 17:47 ? 00:00:00 /bin/sh /usr/lib/apt/apt.systemd.daily lock_is_held install 解决方法12rm -rf /var/lib/dpkg/lockapt-get update]]></content>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC mvc:annotation-driven]]></title>
    <url>%2F2019%2F04%2F12%2FSpringMVC%2FSpringMVC-mvc-annotation-driven%2F</url>
    <content type="text"><![CDATA[在实际开发中，&lt;mvc:annotation-driven/&gt;是一定要加上的，因为它帮我们做了好多好多事情。没有&lt;mvc:annotation-driven/&gt;，你使用的只是低配的SpringMVC，一旦加上，就是高配的SpringMVC，开启了SpringMVC的开挂模式 &lt;mvc:annotation-driven/&gt; 会自动注册以下3个Bean： RequestMappingHandlerMapping RequestMappingHandlerAdapter ExceptionHandlerExceptionResolver 还将提供以下支持： 支持使用 ConversionService 实例对表单参数进行类型转换 支持使用 @NumberFormat annotation、@DateTimeFormat 注解完成数据类型的格式化 支持使用 @Valid 注解对 JavaBean 实例进行 JSR 303 验证 支持使用 @RequestBody 和 @ResponseBody 注解 1. 几个需要&lt;mvc:annotation-driven/&gt;支持的场景说明1.1. &lt;mvc:default-servlet-handler/&gt;单独使用&lt;mvc:default-servlet-handler/&gt;，会导致@RequestMapping处理器失效。 既没有配置&lt;mvc:default-servlet-handler/&gt;也没有配置&lt;mvc:annotation-driven/&gt;，Spring容器会注册以下Bean HttpRequestHandlerAdapter SimpleControllerHandlerAdapter RequestMappingHandlerAdapter（Spring旧版本中叫做AnnotationMethodHandlerAdapter）可以处理@RequestMapping 配置了&lt;mvc:default-servlet-handler/&gt;但没有配置&lt;mvc:annotation-driven/&gt;，Spring容器会注册以下Bean。你会发现RequestMappingHandlerAdapter这个组件消失了，所以@RequestMapping就无法处理 HttpRequestHandlerAdapter SimpleControllerHandlerAdapter 配置了&lt;mvc:default-servlet-handler/&gt;也配置了&lt;mvc:annotation-driven/&gt;，Spring容器会注册以下Bean。RequestMappingHandlerAdapter又有了 HttpRequestHandlerAdapter SimpleControllerHandlerAdapter RequestMappingHandlerAdapter 因此，&lt;mvc:default-servlet-handler/&gt;必须要有&lt;mvc:annotation-driven/&gt;的支持12&lt;mvc:annotation-driven/&gt;&lt;mvc:default-servlet-handler/&gt;]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu Python环境配置]]></title>
    <url>%2F2019%2F04%2F12%2FLinux%2FUbuntu-Python%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Ubuntu16.04 自带Python2和Python31234$ python --versionPython 2.7.12$ python3 --versionPython 3.5.2 1. pip环境1.1. 安装pip默认pip没有安装，输入pip和pip3，会提示你要安装对应的包123456$ pipThe program 'pip' is currently not installed. You can install it by typing:apt install python-pip$ pip3The program 'pip3' is currently not installed. You can install it by typing:apt install python3-pip 安装pip12apt-get install python-pipapt-get install python3-pip 检查是否安装成功1234$ pip --versionpip 8.1.1 from /usr/lib/python2.7/dist-packages (python 2.7)$ pip3 --versionpip 8.1.1 from /usr/lib/python3/dist-packages (python 3.5) 1.2. 更新pip12python -m pip install --upgrade pippython3 -m pip install --upgrade pip 1.2.1. 错误 Traceback (most recent call last): File “/usr/bin/pip”更新pip之后，可能会出现以下问题，这是pip的一个BUG12345678910$ pipTraceback (most recent call last): File "/usr/bin/pip3", line 9, in &lt;module&gt; from pip import mainImportError: cannot import name 'main'$ pip3Traceback (most recent call last): File "/usr/bin/pip3", line 9, in &lt;module&gt; from pip import mainImportError: cannot import name 'main' 编辑 /usr/bin/pip 和 /usr/bin/pip312345678# ============== 以下是原本的内容from pip import mainif __name__ == '__main__': sys.exit(main())# ============== 以下是修改之后的内容from pip import __main__ # 修改if __name__ == '__main__': sys.exit(__main__._main()) # 修改 2. 默认PythonUbuntu下，python命令对应py2，python3命令对应py3 2.1. 利用alternatives机制修改默认python修改python3为默认，数字大的优先级高12update-alternatives --install /usr/bin/python python /usr/bin/python2 100update-alternatives --install /usr/bin/python python /usr/bin/python3 150 设置之后，python和pip都默认对应py3 2.2. 修改命令文件修改默认python12rm -rf /usr/bin/python # 删除python命令ln -s /usr/bin/python3 /usr/bin/python # 设置软链接 python ==&gt; python3]]></content>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC 数据校验]]></title>
    <url>%2F2019%2F04%2F12%2FSpringMVC%2FSpringMVC-%E6%95%B0%E6%8D%AE%E6%A0%A1%E9%AA%8C%2F</url>
    <content type="text"><![CDATA[参考 IBM Developer：JSR 303 - Bean Validation 介绍及最佳实践 https://www.ibm.com/developerworks/cn/java/j-lo-jsr303/index.html JCP官方 JSR 303说明文档 https://jcp.org/en/jsr/detail?id=303 1. 数据校验只做前端校验是不安全的，因为可以通过很多种方式绕过浏览器，向服务器发请请求。因此重要数据一定要在后端加上验证 在后端验证，我们可以这样做：对重要数据进行校验，如果失败则直接跳转到原来的表单页面，提示用户重新填写表单 但是这样做，有不少缺点： 显然，这样做有点麻烦 在通常的情况下，应用程序是分层的，不同的层由不同的开发人员来完成。很多时候同样的数据验证逻辑会出现在不同的层，这样就会导致代码冗余和一些管理的问题，比如说语义的一致性等 2. JSR 303 - Bean Validation3. Hibernate ValidatorJDBC是规范，其实现就是各个厂商的驱动包JSR303也是规范，Hibernate Validator是该标准的一个较好的实现 3.1. 扩展注解Hibernate Validator 除了支持所有的JSR 303标准校验注解外，它还支持以下的扩展注解 4. 使用流程4.1. 依赖]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC CORS]]></title>
    <url>%2F2019%2F04%2F11%2FSpringMVC%2FSpringMVC-CORS%2F</url>
    <content type="text"><![CDATA[spring MVC自4.2开始添加了CORS的支持 1. 实现CORS1.1. 方式一：在Bean配置文件中配置所有默认使用默认值，对所有请求开放123&lt;mvc:cors&gt; &lt;mvc:mapping path="/**"/&gt;&lt;/mvc:cors&gt; 这是完整的配置1234567891011&lt;mvc:cors&gt; &lt;mvc:mapping path="/api/**" allowed-origins="http://domain1.com, http://domain2.com" allowed-methods="GET, PUT" allowed-headers="header1, header2, header3" exposed-headers="header1, header2" allow-credentials="false" max-age="123"/&gt; &lt;mvc:mapping path="/resources/**" allowed-origins="http://domain1.com"/&gt;&lt;/mvc:cors&gt;]]></content>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC 拦截器]]></title>
    <url>%2F2019%2F04%2F11%2FSpringMVC%2FSpringMVC-%E6%8B%A6%E6%88%AA%E5%99%A8%2F</url>
    <content type="text"><![CDATA[1. 拦截器简介SpringMVC提供了拦截器机制；允许运行目标方法之前进行一些拦截工作，或者目标方法运行之后进行一些其他处理。]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[商城项目 前端]]></title>
    <url>%2F2019%2F04%2F11%2Fproject%2F%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE%2F%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE-%E5%89%8D%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[1. 基本配置1.1. 关闭eslint编辑 config/index.js1useEslint: false 2. 品牌管理 参考Example 2.1. 添加路由和侧边栏编辑 router/index.js 12345678910111213141516171819202122232425262728293031&#123; path: '/brand', component: Layout, redirect: '/brand/list', name: 'Brand', meta: &#123; title: '品牌管理', icon: 'example' &#125;, children: [ &#123; path: 'create', component: () =&gt; import('@/views/brand/create'), name: 'CreateBrand', meta: &#123; title: '创建品牌', icon: 'edit' &#125; &#125;, &#123; path: 'edit/:id(\\d+)', component: () =&gt; import('@/views/brand/edit'), name: 'EditBrand', meta: &#123; title: '编辑品牌', noCache: true &#125;, hidden: true &#125;, &#123; path: 'list', component: () =&gt; import('@/views/brand/list'), name: 'BrandList', meta: &#123; title: '品牌列表', icon: 'list' &#125; &#125; ]&#125;, 2.2. 添加组件文件复制@/views/example目录，得到@/views/brand 2.3. 添加API接口文件复制@/api/article.js，得到@/api/brand.js 2.4. list.vue 品牌列表页]]></content>
      <tags>
        <tag>商城项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis MBG 通用Mapper插件]]></title>
    <url>%2F2019%2F04%2F10%2FMyBatis%2FMyBatis-MBG-%E9%80%9A%E7%94%A8Mapper%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[1. MBG 通用Mapper插件 With Java 环境搭建1.1. 依赖在原MBG的基础上，多加一个通用mapper依赖123456789101112131415161718192021222324252627&lt;!-- mysql --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt;&lt;/dependency&gt;&lt;!-- mybatis MBG生成代码，本身并不需要MyBatis依赖的支持。 但是有时你会使用MBG生成MyBatis注解，这是需要MyBatis依赖的，为了生成后的文件不报错，就加上该依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.6&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 通用mapper --&gt;&lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper&lt;/artifactId&gt; &lt;version&gt;4.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- generator --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;/artifactId&gt; &lt;version&gt;1.3.7&lt;/version&gt;&lt;/dependency&gt; 1.2. generatorConfig.xml添加通用Mapper插件 12345678910&lt;!-- 指定Java文件的编码 --&gt;&lt;property name="javaFileEncoding" value="UTF-8"/&gt;&lt;!-- 是否使用通用 Mapper 提供的注释工具 --&gt;&lt;property name="useMapperCommentGenerator" value="false"/&gt;&lt;plugin type="tk.mybatis.mapper.generator.MapperPlugin"&gt; &lt;property name="mappers" value="tk.mybatis.mapper.common.Mapper"/&gt; &lt;property name="caseSensitive" value="true"/&gt; &lt;property name="forceAnnotation" value="true"/&gt;&lt;/plugin&gt; 1.3. 生成代码没有变化123456789101112131415161718@Testpublic void test() throws Exception &#123; // MBG执行过程中的警告信息 List&lt;String&gt; warnings = new ArrayList&lt;String&gt;(); // 覆盖原文件 boolean overwrite = true; // 读取配置 InputStream in = this.getClass().getClassLoader().getResourceAsStream("generatorConfig.xml"); ConfigurationParser cp = new ConfigurationParser(warnings); Configuration config = cp.parseConfiguration(in); DefaultShellCallback callback = new DefaultShellCallback(overwrite); // 创建MBG MyBatisGenerator myBatisGenerator = new MyBatisGenerator(config, callback, warnings); // 执行生成代码 myBatisGenerator.generate(null); // 输出警告信息 warnings.forEach(System.out::println);&#125; 2. QBC说明MBG&lt;context&gt;的&lt;targetRuntime&gt;主要有两种取值 MyBatis3Simple: 生成不带example的查询 MyBatis3：生成带example的查询 因为通用Mapper已经带有QBC的功能，所以MBG设置为MyBatis3Simple即可]]></content>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis 通用Mapper入门]]></title>
    <url>%2F2019%2F04%2F10%2FMyBatis%2FMyBatis-%E9%80%9A%E7%94%A8Mapper%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[1. 通用Mapper简介1.1. 项目地址MyBatis分页插件和通用Mapper都托管在码云和Github，作者是同一个人。 https://github.com/abel533/Mapper 作者是这样介绍的： 通用Mapper都可以极大的方便开发人员。可以随意的按照自己的需要选择通用方法，还可以很方便的开发自己的通用方法。 极其方便的使用MyBatis单表的增删改查。 支持单表操作，不支持通用的多表联合查询 1.2. 为什么要使用通用Mapper有了MBG，已经极大地简化了MyBatis开发。MBG生成的方法都是单表查询，通过Mapper也是单表查询，为什么还要使用通用Mapper呢？ MBG只是帮我们快速生成Mapper接口和xml。当实体类发生修改，你还要重新运行逆向工程，覆盖原来的Mapper接口和xml，十分麻烦。 使用通用Mapper，可以动态生成SQL，也就可以省略xml。实体类发生修改，生成的SQL跟着就修改了，不需要人为地做其它修改，给开发带来很多便利的好处。 通用Mapper还支持批量操作，而MBG不支持 1.3. 对实体类的要求考虑到基本数据类型在Java 类中都有默认值，会导致MyBatis 在执行相关操作时很难判断当前字段是否为null，所以在MyBatis 环境下使用Java 实体类时尽量不要使用基本数据类型，都使用对应的包装类型 2. Spring整合通用Mapper参考： Github官方项目wiki: 与Spring集成 https://github.com/abel533/Mapper/wiki/1.2-spring 2.1. 依赖正常情况下，Spring 和 MyBatis 的集成环境中，应该已经存在下面的依赖12345678910111213141516171819202122232425&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;版本号&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;版本号&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;版本号&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;版本号&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;版本号&lt;/version&gt;&lt;/dependency&gt; 集成通用 Mapper 在上面的基础上添加下面的依赖：12345&lt;dependency&gt; &lt;groupId&gt;tk.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mapper&lt;/artifactId&gt; &lt;version&gt;最新版本&lt;/version&gt;&lt;/dependency&gt; 2.2. 实体类1234567891011121314@Data@Builder@NoArgsConstructor@AllArgsConstructorpublic class Brand &#123; private Long id; private String name; private String firstChar; private String logoUrl; private String siteUrl; private Byte sortOrder; private Boolean isShow; private String brandDesc;&#125; 2.3. Mapper接口相较于原生MyBatis，通用Mapper省略了xml，也没有使用注解开发，就能实现增删查改。这不是平白无故就能省略的，这要求Mapper接口继承Mapper&lt;T&gt;，泛型参数就是实体类12public interface BrandMapper extends Mapper&lt;Brand&gt; &#123; &#125; 2.4. jdbc.properties1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/mall?useUnicode=true&amp;characterEncoding=utf-8jdbc.username=rootjdbc.password=123456 2.5. MyBatis配置文件mybatis-config.xml1234567&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt;&lt;/configuration&gt; 2.6. applicationContext.xml在Spring和MyBatis集成配置的基础上，只需要将org.mybatis.spring.mapper.MapperScannerConfigurer修改为tk.mybatis.spring.mapper.MapperScannerConfigurer即可 123456789101112131415161718192021222324252627282930&lt;context:property-placeholder location="classpath:jdbc.properties"/&gt;&lt;!-- 数据源配置 --&gt;&lt;bean id="dataSource" class="com.alibaba.druid.pool.DruidDataSource" destroy-method="close"&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;"/&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;"/&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;"/&gt; &lt;property name="driverClassName" value="$&#123;jdbc.driver&#125;"/&gt; &lt;property name="maxActive" value="10"/&gt; &lt;property name="minIdle" value="5"/&gt;&lt;/bean&gt;&lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;!-- 引用数据源 --&gt; &lt;property name="dataSource" ref="dataSource"/&gt; &lt;!-- 指定MyBatis配置文件 --&gt; &lt;property name="configLocation" value="classpath:mybatis-config.xml"/&gt;&lt;/bean&gt;&lt;!-- 原生MyBatis使用org.mybatis.spring.mapper.MapperScannerConfigurer进行包扫描&lt;bean class="org.mybatis.spring.mapper.MapperScannerConfigurer"&gt; &lt;property name="basePackage" value="demo.mybatis.mapper"/&gt;&lt;/bean&gt;--&gt;&lt;!-- 通用Mapper使用tk.mybatis.spring.mapper.MapperScannerConfigurer进行包扫描--&gt;&lt;bean class="tk.mybatis.spring.mapper.MapperScannerConfigurer"&gt; &lt;property name="basePackage" value="demo.mybatis.mapper"/&gt;&lt;/bean&gt; 2.7. 测试1234567891011@ContextConfiguration("classpath:applicationContext.xml")@RunWith(SpringRunner.class)public class SpringTest &#123; @Autowired private BrandMapper brandMapper; @Test public void test() &#123; brandMapper.selectAll().forEach(System.out::println); &#125;&#125;]]></content>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 整合MyBatis]]></title>
    <url>%2F2019%2F04%2F10%2FSpring%2FSpring-%E6%95%B4%E5%90%88MyBatis%2F</url>
    <content type="text"><![CDATA[参考 官方mybatis-spring参考文档 http://www.mybatis.org/spring/zh/getting-started.html 1. MyBatis-Spring简介1.1. 什么是 MyBatis-SpringMyBatis-Spring 会帮助你将 MyBatis 代码无缝地整合到 Spring 中。它将允许 MyBatis 参与到 Spring 的事务管理之中，创建映射器 mapper 和 SqlSession 并注入到 bean 中，以及将 Mybatis 的异常转换为 Spring 的 DataAccessException。最终，可以做到应用代码不依赖于 MyBatis，Spring 或 MyBatis-Spring。 1.2. MyBatis-Spring版本注意MyBatis-Spring和Spring之间版本的对应关系 MyBatis-Spring MyBatis Spring 框架 Spring Batch Java 2.0 3.5+ 5.0+ 4.0+ Java 8+ 1.3 3.4+ 3.2.2+ 2.1+ Java 6+ 2. 基本环境搭建2.1. 依赖12345678910111213141516171819202122232425262728293031323334353637383940&lt;!-- Spring --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;!-- org.mybatis.spring.SqlSessionFactoryBean使用了spring-jdbc，所以必须要导入这个依赖 --&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-tx&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;!-- MySQL --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt;&lt;/dependency&gt;&lt;!-- Druid --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.16&lt;/version&gt;&lt;/dependency&gt;&lt;!-- MyBatis --&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.6&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;2.0.1&lt;/version&gt;&lt;/dependency&gt; 2.2. jdbc.properties1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/mall?useUnicode=true&amp;characterEncoding=utf-8jdbc.username=rootjdbc.password=123456 2.3. applicationContext.xml以下配置是必须的 配置数据源 配置SqlSessionFactoryBean，并引用数据源 1234567891011121314151617&lt;context:property-placeholder location="classpath:jdbc.properties"/&gt;&lt;!-- 数据源配置 --&gt;&lt;bean id="dataSource" class="com.alibaba.druid.pool.DruidDataSource" destroy-method="close"&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;"/&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;"/&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;"/&gt; &lt;property name="driverClassName" value="$&#123;jdbc.driver&#125;"/&gt; &lt;property name="maxActive" value="10"/&gt; &lt;property name="minIdle" value="5"/&gt;&lt;/bean&gt;&lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;!-- 引用数据源 --&gt; &lt;property name="dataSource" ref="dataSource"/&gt;&lt;/bean&gt; 2.4. Bean/Mapper接口/映射文件这里就简单列出listBrands的配置1234&lt;!-- List&lt;Brand&gt; listBrands() --&gt;&lt;select id="listBrands" resultType="demo.mybatis.entity.Brand"&gt; SELECT * FROM brand&lt;/select&gt; 3. 整合方式一：使用SqlSessionTemplate3.1. 编辑Bean配置文件 sqlSessionFactory要指定Mapper映射文件的路径 配置SqlSessionTemplate，引用sqlSessionFactory 123456789101112131415&lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;!-- 引用数据源 --&gt; &lt;property name="dataSource" ref="dataSource"/&gt; &lt;!-- 指定Mapper映射文件 --&gt; &lt;property name="mapperLocations"&gt; &lt;list&gt; &lt;value&gt;classpath:demo/mybatis/mapper/**/*.xml&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id="sessionTemplate" class="org.mybatis.spring.SqlSessionTemplate"&gt; &lt;!-- 引用sqlSessionFactory --&gt; &lt;constructor-arg name="sqlSessionFactory" ref="sqlSessionFactory"/&gt;&lt;/bean&gt; 3.2. 测试12345678910111213141516@ContextConfiguration("classpath:applicationContext.xml")@RunWith(SpringRunner.class)public class SpringTest &#123; @Autowired private SqlSessionTemplate sqlSessionTemplate; @Test public void test() &#123; // selectList("命名空间.id") 执行对应的SQL语句 List&lt;Brand&gt; objects = sqlSessionTemplate.selectList("demo.mybatis.mapper.BrandMapper.listBrands"); System.out.println(objects); // getMapper() 获取Mapper，再执行相应方法 BrandMapper mapper = sqlSessionTemplate.getMapper(BrandMapper.class); mapper.listBrands().forEach(System.out::println); &#125;&#125; 4. 整合方式二：SqlSessionDaoSupportTODO 5. 整合方式三：MapperFactoryBeanTODO 6. 整合方式四：扫描Mapper6.1. 配置MapperScannerConfigurer配置MapperScannerConfigurer，指定扫描指定包下所有的Mapper接口，自动创建代理并且在Spring容器中注册，遵循以下规范 Mapper接口与映射文件在同一级目录 注册的Mapper，其id是Mapper接口类名首字母小写 12345678&lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;!-- 引用数据源 --&gt; &lt;property name="dataSource" ref="dataSource"/&gt;&lt;/bean&gt;&lt;bean class="org.mybatis.spring.mapper.MapperScannerConfigurer"&gt; &lt;!-- 扫描Mapper所在包 --&gt; &lt;property name="basePackage" value="demo.mybatis.mapper"/&gt;&lt;/bean&gt; 6.2. 测试12345678910111213@ContextConfiguration("classpath:applicationContext.xml")@RunWith(SpringRunner.class)public class SpringTest &#123; // 扫描到Mapper就会被注册到IoC容器中，所以直接装配使用Mapper即可 @Autowired private BrandMapper brandMapper; @Test public void test() &#123; brandMapper.listBrands().forEach(System.out::println); &#125;&#125; 7. 几种方式的对比显然扫描Mapper最方便啊，配置少，还是批量自动注册的]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[商城项目 后台——工程搭建]]></title>
    <url>%2F2019%2F04%2F10%2Fproject%2F%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE%2F%E5%95%86%E5%9F%8E%E9%A1%B9%E7%9B%AE-%E5%90%8E%E5%8F%B0%E2%80%94%E2%80%94%E5%B7%A5%E7%A8%8B%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1. mall-parent Maven父工程所有工程都继承父工程。父工程只编写pom.xml 1.1. pom.xmlpom.xml主要的配置如下，但是没有列出完整的依赖123456789101112131415161718192021222324252627282930313233343536373839404142434445&lt;!-- 声明子项目公用的配置属性 --&gt;&lt;properties&gt; &lt;junit.version&gt;4.12&lt;/junit.version&gt; &lt;spring.version&gt;5.1.5.RELEASE&lt;/spring.version&gt; &lt;mysql.version&gt;5.1.32&lt;/mysql.version&gt;&lt;/properties&gt;&lt;!-- 声明并引入子项目共有的依赖 --&gt;&lt;dependencies&gt; &lt;!-- junit --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;$&#123;junit.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;!-- 仅声明子项目共有的依赖，如果子项目需要此依赖，那么子项目需要声明 --&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;$&#123;mysql.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;!-- 声明构建信息 --&gt;&lt;build&gt; &lt;!-- 声明并引入子项目共有的插件（插件就是附着到Maven各个声明周期的具体实现） --&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.6.1&lt;/version&gt; &lt;!-- 统一按照JDK1.8编译 --&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;]]></content>
      <tags>
        <tag>商城项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vue vue-element-admin]]></title>
    <url>%2F2019%2F04%2F10%2FVue%2FVue-vue-element-admin%2F</url>
    <content type="text"><![CDATA[参考 vue-element-admin官网 https://panjiachen.gitee.io/vue-element-admin-site/zh/ 1.]]></content>
      <tags>
        <tag>Vue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 架构]]></title>
    <url>%2F2019%2F04%2F10%2FMySQL%2FMySQL-%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[1. MySQL 架构和其它数据库相比，MySQL有点与众不同，它的架构可以在多种不同场景中应用并发挥良好作用。主要体现在存储引擎的架构上，插件式的存储引擎架构将查询处理和其它的系统任务以及数据的存储提取相分离。这种架构可以根据业务的需求和实际需要选择合适的存储引擎。 1.1. Connectors 连接层最上层是一些客户端和连接服务，包含本地sock通信和大多数基于客户端/服务端工具实现的类似于tcp/ip的通信。主要完成一些类似于连接处理、授权认证、及相关的安全方案。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。 1.2. 服务层第二层架构主要完成大多少的核心服务功能，如SQL接口，并完成缓存的查询，SQL的分析和优化及部分内置函数的执行。所有跨存储引擎的功能也在这一层实现，如过程、函数等。在该层，服务器会解析查询并创建相应的内部解析树，并对其完成相应的优化如确定查询表的顺序，是否利用索引等，最后生成相应的执行操作。如果是select语句，服务器还会查询内部的缓存。如果缓存空间足够大，这样在解决大量读操作的环境中能够很好的提升系统的性能。]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS RPM包管理]]></title>
    <url>%2F2019%2F04%2F10%2FLinux%2FCentOS-RPM%E5%8C%85%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[1. 配置yum网络源1.1. 配置阿里源进入 https://opsx.alibaba.com/mirror ，找到CentOS，点击Help，找到CentOS 6对应的命令，执行1wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-6.repo 更新yum缓存12yum clean allyum makecache]]></content>
      <tags>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS 静态NAT配置]]></title>
    <url>%2F2019%2F04%2F10%2FLinux%2FCentOS-%E9%9D%99%E6%80%81NAT%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1. CentOS 6 静态NAT配置编辑 /etc/sysconfig/network-scripts/ifcfg-eth0123456789101112DEVICE=eth0HWADDR=00:0C:29:81:E7:5BTYPE=EthernetUUID=5e19ab91-63d8-4e04-86ef-2a702a6bd136NM_CONTROLLED=yes# 主要是以下部分ONBOOT=yes # 开机激活网卡BOOTPROTO=static # 静态IPIPADDR=192.168.57.131 # 指定IPNETMASK=255.255.255.0 # 指定子网掩码GATEWAY=192.168.57.2 # 指定网关DNS1=114.114.114.114 # 指定DNS 重启网络服务 service network restart 检验配置1234ifconfig # 查看网关ping 192.168.57.2 # ping网关ping 114.114.114.114ping www.baidu.com]]></content>
      <tags>
        <tag>CentOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 安装]]></title>
    <url>%2F2019%2F04%2F09%2FMySQL%2FMySQL-%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1. CentOS 6 RPM方式安装 MySQL71.1. 下载 设置选项 选择版本 5.7 选择系统 Red Hat 选择系统版本 Linux6 x86-64 在列表中下载以下几项 RPM Package, MySQL Configuration (mysql-community-common-5.7.25-1.el6.x86_64.rpm) RPM Package, Shared Libraries (mysql-community-libs-5.7.25-1.el6.x86_64.rpm) RPM Package, MySQL Server (mysql-community-server-5.7.25-1.el6.x86_64.rpm) RPM Package, Client Utilities (mysql-community-client-5.7.25-1.el6.x86_64.rpm) 1.2. 卸载自带的mysql一开始系统自带mysql-libs12$ rpm -qa | grep -i mysql # grep 选项-i表示不区分大小写mysql-libs-5.1.73-8.el6_8.x86_64 将其卸载123# -e: 卸载rpm# --nodeps 卸载时不检查依赖关系rpm -e --nodeps mysql-libs-5.1.73-8.el6_8.x86_64 按照依赖关系依次安装MySQL的rpm包，安装顺序为common-&gt;libs-&gt;client-&gt;server1234567# -i: install安全# -v: verbose显示进度# -h: hash进行哈希检验rpm -ivh mysql-community-common-5.7.25-1.el6.x86_64.rpmrpm -ivh mysql-community-libs-5.7.25-1.el6.x86_64.rpmrpm -ivh mysql-community-client-5.7.25-1.el6.x86_64.rpmrpm -ivh mysql-community-server-5.7.25-1.el6.x86_64.rpm 检验安装成功12345678910111213$ rpm -qa | grep mysql # 检验rpm是否安装mysql-community-common-5.7.25-1.el6.x86_64mysql-community-server-5.7.25-1.el6.x86_64mysql-community-client-5.7.25-1.el6.x86_64mysql-community-libs-5.7.25-1.el6.x86_64$ cat /etc/passwd | grep mysql # 检测是否生成mysql用户mysql:x:27:27:MySQL Server:/var/lib/mysql:/bin/false$ cat /etc/group | grep mysql # 检测是否生成mysql用户组mysql:x:27:$ mysql --versionmysql Ver 14.14 Distrib 5.7.25, for Linux (x86_64) using EditLine wrapper 1.3. 启动关闭MySQL服务12service mysqld startservice mysqld stop 1.4. 配置root密码一开始MySQL的root用户是没有密码的。这样不安全，所以要先设置 启动MySQL服务之后，用mysqladmin命令设置root密码。发现无权访问123$ mysqladmin -u root password 123456mysqladmin: connect to server at 'localhost' failederror: 'Access denied for user 'root'@'localhost' (using password: NO)' 解决方法如下：123service mysqld stop # 先关闭服务mysqld_safe --user=mysql --skip-grant-tables --skip-networking &amp; # 启动安全模式mysql -u root # 以root用户连接MySQL，没有密码 123456789101112131415mysql&gt; use mysql;mysql&gt; set PASSWORD = PASSWORD('123456'); -- 通过设置PASSWORD来修改密码，发现--skip-grant-tables模式下不能修改ERROR 1290 (HY000): The MySQL server is running with the --skip-grant-tables option so it cannot execute this statementmysql&gt; UPDATE user SET password=PASSWORD('123456') WHERE User='root'; -- 设置密码，发现MySQL 5.7的user将password字段修改为authentication_stringERROR 1054 (42S22): Unknown column 'Password' in 'field list'mysql&gt; UPDATE user SET authentication_string=PASSWORD('123456') WHERE User='root'; -- 再尝试设置密码mysql&gt; ALTER USER 'root'@'localhost' PASSWORD EXPIRE NEVER; -- 设置密码永久生效ERROR 1290 (HY000): The MySQL server is running with the --skip-grant-tables option so it cannot execute this statementmysql&gt; FLUSH PRIVILEGES; -- 使设置生效mysql&gt; exit; 关闭mysql服务，注意此时的服务是指mysqld_safe1234$ service mysqld stop # 关闭mysqld_safe2019-04-10T01:43:39.129247Z mysqld_safe mysqld from pid file /var/run/mysqld/mysqld.pid endedStopping mysqld: [ OK ][1]+ Done mysqld_safe --user=mysql --skip-grant-tables --skip-networking 再开启mysql服务，用root用户连接mysql，发现连接成功12service mysqld startmysql -uroot -p123456 1.5. 设置root密码永久有效之前在配置root密码时，尝试设置永久生效，但是由于处于--skip-grant-tables状态，所以无法设置12mysql&gt; ALTER USER 'root'@'localhost' PASSWORD EXPIRE NEVER; -- 设置密码永久生效ERROR 1290 (HY000): The MySQL server is running with the --skip-grant-tables option so it cannot execute this statement 连接mysql，发现做很多操作都提示以下信息，说明root密码已经失效了12mysql&gt; show databases;ERROR 1820 (HY000): You must reset your password using ALTER USER statement before executing this statement. 只要更新root密码，就能暂时使密码生效。但是发现另一个问题，由于MySQL存在密码策略的问题，当发现root密码过于简单，就会限制其操作。所以现在SET PASSWORD就无法执行了123mysql&gt; SET PASSWORD=PASSWORD(&apos;123456&apos;);ERROR 1819 (HY000): Your password does not satisfy the current policy requirementsmysql&gt; show variables like &apos;default_password_lifetime&apos;; 解决办法是，先修改密码策略12345mysql&gt; SET GLOBAL validate_password_policy=0;mysql&gt; SET GLOBAL validate_password_mixed_case_count=0;mysql&gt; SET GLOBAL validate_password_number_count=3;mysql&gt; SET GLOBAL validate_password_special_char_count=0;mysql&gt; SET GLOBAL validate_password_length=3; 再设置密码，设置永久生效123mysql&gt; SET PASSWORD=PASSWORD('123456'); -- 密码生效mysql&gt; ALTER USER 'root'@'localhost' PASSWORD EXPIRE NEVER; -- 密码永久有效mysql&gt; flush privileges; -- 配置立即生效 1.6. 设置MySQL服务开机自启动设置mysqld开机自启动1chkconfig mysqld on 查看运行级别12$ chkconfig --list | grep mysqlmysqld 0:off 1:off 2:on 3:on 4:on 5:on 6:off 0/1/6是off状态，2~5是on状态，说明就是开机自启动的。123456789$ cat /etc/inittab # 查看运行级别说明# ................# 0 - halt (Do NOT set initdefault to this)# 1 - Single user mode# 2 - Multiuser, without NFS (The same as 3, if you do not have networking)# 3 - Full multiuser mode# 4 - unused# 5 - X11# 6 - reboot (Do NOT set initdefault to this) 也可以通过ntsysv查看自启动的服务，前面是[*]的，代表已经设置为自启动1ntsysv Tab可以切换到Cancel退出 1.7. 查看MySQL相关目录查看mysql进程123$ ps -ef | grep mysqlroot 37916 1 0 09:44 pts/1 00:00:00 /bin/sh /usr/bin/mysqld_safe --datadir=/var/lib/mysql --socket=/var/lib/mysql/mysql.sock --pid-file=/var/run/mysqld/mysqld.pid --basedir=/usr --user=mysqlmysql 38110 37916 0 09:44 pts/1 00:00:00 /usr/sbin/mysqld --basedir=/usr --datadir=/var/lib/mysql --plugin-dir=/usr/lib64/mysql/plugin --user=mysql --log-error=/var/log/mysqld.log --pid-file=/var/run/mysqld/mysqld.pid --socket=/var/lib/mysql/mysql.sock 你会看到--datadir=/var/lib/mysql，这说明/var/lib/mysql就是mysql数据库文件的存放路径 总的来説，与MySQL相关的有这几个路径 路径 说明 /var/lib/mysql 存放数据库文件 /usr/share/mysql 存放配置文件 /usr/bin 存放相关命令，如mysql、mysqladmin /etc/init.d/mysqld 启动关闭服务的脚本 /var/run/mysqld/mysqld.pid pid文件 1.8. 设置字符集查看字符集变量，可以看到character_set_database和character_set_server默认都是latin1，这意味着，你往数据库中添加数据，如果是中文就会出现乱码 1234567891011121314mysql&gt; SHOW VARIABLES LIKE '%char%';+--------------------------------------+----------------------------+| Variable_name | Value |+--------------------------------------+----------------------------+| character_set_client | utf8 || character_set_connection | utf8 || character_set_database | latin1 || character_set_filesystem | binary || character_set_results | utf8 || character_set_server | latin1 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ || validate_password_special_char_count | 0 |+--------------------------------------+----------------------------+ 编辑配置文件/etc/my.cnf，在对应位置设置以下内容123456789101112[mysqld]# 忽略客户端字符集，只使用服务器的字符集配置character-set-client-handshake=FALSE# 服务器默认字符集character-set-server=utf8mb4# collation-server指定数据集如何排序，以及字符串的比对规则collation-server=utf8mb4_unicode_ci# init_connect指定连接到MySQL后就执行的SQL init_connect=’SET NAMES utf8mb4’[client]# 指定客户端默认字符集default-character-set=utf8mb4 重启mysql服务1service mysqld restart 连接数据库，可以看到字符集已经修改了1234567891011121314mysql&gt; SHOW VARIABLES LIKE '%char%';+--------------------------------------+----------------------------+| Variable_name | Value |+--------------------------------------+----------------------------+| character_set_client | utf8mb4 || character_set_connection | utf8mb4 || character_set_database | utf8mb4 || character_set_filesystem | binary || character_set_results | utf8mb4 || character_set_server | utf8mb4 || character_set_system | utf8 || character_sets_dir | /usr/share/mysql/charsets/ || validate_password_special_char_count | 1 |+--------------------------------------+----------------------------+ 这里说明一点，utf8mb4是utf8的扩展。支持更多的字符，如emoji等。更推荐使用utf8mb4]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL 简介]]></title>
    <url>%2F2019%2F04%2F09%2FMySQL%2FMySQL-%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[1. MySQL 简介MySQL是一个关系型数据库管理系统，由瑞典MySQL AB公司开发，目前属于Oracle公司： MySQL是一种关联数据库管理系统，将数据保存在不同的表中，而不是将所有数据放在一个大仓库内，这样就増加了速度并提高了灵活性。 MySQL是开源的，所以你不需要支付额外的费用。 MySQL支持大型的数据库，可以处理拥有上千万条记录的大型数据库。MySQL使用标准的SQL数据语言形式。 MySQL可以允许于多个系统上，并且支持多种语言。这些编程语言包括C、C++、Python、Java、Perl、PHP、Eiffel、Ruby和Tel等。 MySQL对PHP有很好的支持，PHP是目前最流行的Web开发语言。 MySQL支持大型数据库，支持5000万条记录的数据仓库，32位系统表文件最大可支持4GB，64位系统支持最大的表文件为8TB。 MySQL是可以定制的，采用了GPL协议，你可以修改源码来开发自己的MySQL系统。]]></content>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis PageHelper]]></title>
    <url>%2F2019%2F04%2F09%2FMyBatis%2FMyBatis-PageHelper%2F</url>
    <content type="text"><![CDATA[参考 PageHelper官网 https://pagehelper.github.io/ 1. PageHelper使用流程1.1. 依赖1234567891011121314151617181920212223242526&lt;dependencies&gt; &lt;!-- log4j 用于mybatis输出日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;!-- mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- pagehelper --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.pagehelper&lt;/groupId&gt; &lt;artifactId&gt;pagehelper&lt;/artifactId&gt; &lt;version&gt;5.1.8&lt;/version&gt; &lt;/dependency&gt; &lt;!-- mysql --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.2. 配置拦截器插件在MyBatis配置文件中配置拦截器插件12345&lt;plugins&gt; &lt;plugin interceptor="com.github.pagehelper.PageInterceptor"&gt; &lt;/plugin&gt;&lt;/plugins&gt; 1.3. Mapper和映射123public interface AuthorMapper &#123; List&lt;Author&gt; selectAll();&#125; 123&lt;select id="selectAll" resultMap="demo.mybatis.entity.Author"&gt; SELECT * FROM author&lt;/select&gt; 1.4. 测试分页查询123456789@Testpublic void test() &#123; int page = 2; // 页码，从1开始 int pageSize = 3; // 页大小 PageHelper.startPage(page, pageSize); // 在查询之前调用PageHelper.startPage AuthorMapper mapper = sqlSession.getMapper(AuthorMapper.class); List&lt;Author&gt; list = mapper.selectAll(); // 查询 list.forEach(System.out::println);&#125; 2. PageHelper 一次性原则注意，分页查询只对紧跟的首次查询有效，之后自动失效 12345678910111213@Testpublic void test() &#123; int page = 2; int pageSize = 3; PageHelper.startPage(page, pageSize); AuthorMapper mapper = sqlSession.getMapper(AuthorMapper.class); List&lt;Author&gt; list = mapper.selectAll(); // startPage()之后的首次查询，会分页 list.forEach(System.out::println); List&lt;Author&gt; list2 = mapper.selectAll(); // 首次查询之后，不再分页 list2.forEach(System.out::println);&#125; 之后的查询如果还想分页，则必须再调用PageHelper.startPage()1234567891011121314@Testpublic void test() &#123; int page = 2; int pageSize = 3; PageHelper.startPage(page, pageSize); AuthorMapper mapper = sqlSession.getMapper(AuthorMapper.class); List&lt;Author&gt; list = mapper.selectAll(); list.forEach(System.out::println); PageHelper.startPage(page, pageSize); // 再启用分页 List&lt;Author&gt; list2 = mapper.selectAll(); list2.forEach(System.out::println);&#125; 3. PageInfo的使用PageInfo是用来获取分页结果的额外信息的 12345678910111213141516171819202122232425@Testpublic void test() &#123; int page = 2; int pageSize = 3; // 分页查询 PageHelper.startPage(page, pageSize); AuthorMapper mapper = sqlSession.getMapper(AuthorMapper.class); List&lt;Author&gt; list = mapper.selectAll(); // 获取分页额外信息 PageInfo&lt;Author&gt; info = new PageInfo&lt;&gt;(list); System.out.println("info.getPageNum() = " + info.getPageNum()); // 当前页码 System.out.println("info.getPageSize() = " + info.getPageSize()); // 页大小 System.out.println("info.getStartRow() = " + info.getStartRow()); // 首条记录是第几行记录（从1开始） System.out.println("info.getEndRow() = " + info.getEndRow()); // 最后的记录是第几行记录（从1开始） System.out.println("info.getTotal() = " + info.getTotal()); // 总记录数（不开启分页，能查询到的记录数） System.out.println("info.getPages() = " + info.getPages()); // 总页数，根据总记录数和页大小可以算出 System.out.println("info.getSize() = " + info.getSize()); // 当前查询到多少条记录，如果当前页不是最后一页，则与getPageSize()大小相同，若是最后页，则不一定相同 System.out.println("info.getPrePage() = " + info.getPrePage()); // 上一页的页码 System.out.println("info.getNextPage() = " + info.getNextPage()); // 下一页的页码 System.out.println("info.getList() = " + info.getList()); // 查询结果 System.out.println("info.isIsFirstPage() = " + info.isIsFirstPage()); // 当前页是不是首页 System.out.println("info.isIsLastPage() = " + info.isIsLastPage()); // 当前页是不是最后一页&#125;]]></content>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis MBG逆向工程]]></title>
    <url>%2F2019%2F04%2F09%2FMyBatis%2FMyBatis-MBG%E9%80%86%E5%90%91%E5%B7%A5%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[参考资料 MBG官方文档地址 http://www.mybatis.org/generator/ MBG官方工程地址 https://github.com/mybatis/generator/releases 1. MBG简介MyBatis Generator：简称MBG，是一个专门为MyBatis框架使用者定制的代码生成器，可以快速的根据表生成对应的映射文件，接口，以及bean类。支持基本的增删改查，以及QBC风格的条件查询。但是表连接、存储过程等这些复杂sql的定义需要我们手工编写 2. MBG With Java 环境搭建官方参考文档：http://www.mybatis.org/generator/running/runningWithJava.html 2.1. 依赖12345678910111213141516171819202122232425262728&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;!-- mysql --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt; &lt;/dependency&gt; &lt;!-- mybatis MBG生成代码，本身并不需要MyBatis依赖的支持。 但是有时你会使用MBG生成MyBatis注解，这是需要MyBatis依赖的，为了生成后的文件不报错，就加上该依赖 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.6&lt;/version&gt; &lt;/dependency&gt; &lt;!-- generator --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis.generator&lt;/groupId&gt; &lt;artifactId&gt;mybatis-generator-core&lt;/artifactId&gt; &lt;version&gt;1.3.7&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.2. generatorConfig.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;!DOCTYPE generatorConfiguration PUBLIC "-//mybatis.org//DTD MyBatis Generator Configuration 1.0//EN" "http://mybatis.org/dtd/mybatis-generator-config_1_0.dtd"&gt;&lt;generatorConfiguration&gt; &lt;context id="Mysql" targetRuntime="MyBatis3Simple" defaultModelType="flat"&gt; &lt;commentGenerator&gt; &lt;!-- 去掉日期注释 --&gt; &lt;property name="suppressDate" value="true"/&gt; &lt;!-- 去掉所有注释 --&gt; &lt;property name="suppressAllComments" value="true"/&gt; &lt;/commentGenerator&gt; &lt;!-- 配置连接数据库的基本信息 --&gt; &lt;jdbcConnection driverClass="com.mysql.jdbc.Driver" connectionURL="jdbc:mysql://localhost:3306/test" userId="root" password="123456"&gt; &lt;/jdbcConnection&gt; &lt;!-- 指定生成的实体类放到哪里 targetProject: 指定根目录。"./src/main/java" --&gt; &lt;javaModelGenerator targetProject="./src/main/java" targetPackage="demo.mybatis.entity"/&gt; &lt;!-- 指定生成的映射文件XxxMapper.xml放到时哪里 --&gt; &lt;sqlMapGenerator targetProject="./src/main/java" targetPackage="demo.mybatis.mapper"/&gt; &lt;!-- 指定生成的Mapper接口放到哪里 --&gt; &lt;javaClientGenerator targetProject="./src/main/java" targetPackage="demo.mybatis.mapper" type="XMLMAPPER"/&gt; &lt;!-- table: 指定表和实体类之间的逆向生成规则 tableName: 指定表名。"%"表示数据库中所有表都参与逆向工程，此时使用默认规则 domainObjectName: 指定实体类的名称。 如果不指定，默认是下划线转大驼峰。例如tableName="hello_world"，那么实体类名就是"HelloWorld" --&gt; &lt;table tableName="book"/&gt; &lt;table tableName="author"/&gt; &lt;!--&lt;table tableName="book" domainObjectName="Employee"&gt;--&gt; &lt;!--&amp;lt;!&amp;ndash; 配置主键生成策略 &amp;ndash;&amp;gt;--&gt; &lt;!--&lt;generatedKey column="emp_id" sqlStatement="Mysql" identity="true"/&gt;--&gt; &lt;!--&lt;/table&gt;--&gt; &lt;/context&gt;&lt;/generatorConfiguration&gt; 2.3. 执行生成代码从类路径下读取123456789101112131415161718@Testpublic void test() throws Exception &#123; // MBG执行过程中的警告信息 List&lt;String&gt; warnings = new ArrayList&lt;String&gt;(); // 覆盖原文件 boolean overwrite = true; // 读取配置 InputStream in = this.getClass().getClassLoader().getResourceAsStream("generatorConfig.xml"); ConfigurationParser cp = new ConfigurationParser(warnings); Configuration config = cp.parseConfiguration(in); DefaultShellCallback callback = new DefaultShellCallback(overwrite); // 创建MBG MyBatisGenerator myBatisGenerator = new MyBatisGenerator(config, callback, warnings); // 执行生成代码 myBatisGenerator.generate(null); // 输出警告信息 warnings.forEach(System.out::println);&#125; 从本地磁盘读取1234567891011@Testpublic void test() throws Exception &#123; List&lt;String&gt; warnings = new ArrayList&lt;String&gt;(); boolean overwrite = true; File configFile = new File("generatorConfig.xml"); ConfigurationParser cp = new ConfigurationParser(warnings); Configuration config = cp.parseConfiguration(configFile); DefaultShellCallback callback = new DefaultShellCallback(overwrite); MyBatisGenerator myBatisGenerator = new MyBatisGenerator(config, callback, warnings); myBatisGenerator.generate(null);&#125; 3. MBG 配置文件你至少需要指定以下信息： &lt;jdbcConnection&gt; 指定连接数据库的配置信息 &lt;javaModelGenerator&gt; 指定生成的实体类放到哪里 &lt;sqlMapGenerator&gt; 指定生成的映射文件XxxMapper.xml放到哪里 &lt;table&gt; 至少要1个table配置 &lt;javaClientGenerator&gt; 可选配置。指定生成的XxxMapper接口放到哪里 3.1. contexttargetRuntime属性 MyBatis3Simple：只生成基本的CRUD，没有byExample方法 MyBatis3：生成CRUD，还有byExample方法 3.2. commentGenerator 去掉生成的注释123456&lt;commentGenerator&gt; &lt;!-- 去掉日期注释 --&gt; &lt;property name="suppressDate" value="true"/&gt; &lt;!-- 去掉所有注释 --&gt; &lt;property name="suppressAllComments" value="true"/&gt;&lt;/commentGenerator&gt; 3.3. jdbcConnection 数据库连接配置1234567&lt;!-- 配置连接数据库的基本信息 --&gt;&lt;jdbcConnection driverClass="com.mysql.jdbc.Driver" connectionURL="jdbc:mysql://localhost:3306/test" userId="root" password="123456"&gt;&lt;/jdbcConnection&gt; 3.4. javaModelGenerator 指定实体类生成路径123456&lt;!-- 指定生成的实体类放到哪里 targetProject: 指定根目录。"./src/main/java" --&gt;&lt;javaModelGenerator targetProject="./src/main/java" targetPackage="demo.mybatis.entity"/&gt; 3.5. sqlMapGenerator 指定映射文件生成路径1234&lt;!-- 指定生成的映射文件XxxMapper.xml放到时哪里 --&gt;&lt;sqlMapGenerator targetProject="./src/main/java" targetPackage="demo.mybatis.mapper"/&gt; 3.6. javaClientGenerator 指定生成Mapper接口的路径12345&lt;!-- 指定生成的Mapper接口放到哪里 --&gt;&lt;javaClientGenerator targetProject="./src/main/java" targetPackage="demo.mybatis.mapper" type="XMLMAPPER"/&gt; 3.7. table 指定表与实体类的映射规则12345678&lt;!-- table: 指定表和实体类之间的逆向生成规则 tableName: 指定表名。"%"表示数据库中所有表都参与逆向工程，此时使用默认规则 domainObjectName: 指定实体类的名称。 如果不指定，默认是下划线转大驼峰。例如tableName="hello_world"，那么实体类名就是"HelloWorld"--&gt;&lt;table tableName="book"/&gt;&lt;table tableName="author"/&gt; 4. 为生成的实体类实现序列化接口直接使用自带的插件即可1&lt;plugin type="org.mybatis.generator.plugins.SerializablePlugin"/&gt; 生效的效果如下：1234public class Author implements Serializable &#123; // ...... private static final long serialVersionUID = 1L;&#125; 5. 自动生成主键生成策略设置主键生成123456789&lt;table tableName="author"&gt; &lt;!-- 配置主键生成策略 column: 数据库中的主键列名 sqlStatement: 可选值是固定的。值为"MySql"时，代表采取SELECT LAST_INSERT_ID()获取主键 identity: true对应&lt;selectKey&gt;的order 如果 --&gt; &lt;generatedKey column="author_id" sqlStatement="MySql" identity="true"/&gt;&lt;/table&gt; 生成后的效果123&lt;selectKey keyProperty="authorId" order="BEFORE" resultType="java.lang.Long"&gt; SELECT LAST_INSERT_ID()&lt;/selectKey&gt; 6. 整合Lombok6.1. 自定义MBG插件12345678910111213141516171819202122232425262728293031323334353637383940414243444546package demo.mybatis.plugin;import org.mybatis.generator.api.IntrospectedColumn;import org.mybatis.generator.api.IntrospectedTable;import org.mybatis.generator.api.PluginAdapter;import org.mybatis.generator.api.dom.java.Method;import org.mybatis.generator.api.dom.java.TopLevelClass;import java.util.List;public class LombokPlugin extends PluginAdapter &#123; @Override public boolean validate(List&lt;String&gt; warnings) &#123; return true; &#125; @Override public boolean modelBaseRecordClassGenerated(TopLevelClass topLevelClass, IntrospectedTable introspectedTable) &#123; // 添加domain的import topLevelClass.addImportedType("lombok.Data"); topLevelClass.addImportedType("lombok.Builder"); topLevelClass.addImportedType("lombok.NoArgsConstructor"); topLevelClass.addImportedType("lombok.AllArgsConstructor"); // 添加domain的注解 topLevelClass.addAnnotation("@Data"); topLevelClass.addAnnotation("@Builder"); topLevelClass.addAnnotation("@NoArgsConstructor"); topLevelClass.addAnnotation("@AllArgsConstructor"); return true; &#125; @Override public boolean modelSetterMethodGenerated(Method method, TopLevelClass topLevelClass, IntrospectedColumn introspectedColumn, IntrospectedTable introspectedTable, ModelClassType modelClassType) &#123; // 不生成setter return false; &#125; @Override public boolean modelGetterMethodGenerated(Method method, TopLevelClass topLevelClass, IntrospectedColumn introspectedColumn, IntrospectedTable introspectedTable, ModelClassType modelClassType) &#123; // 不生成getter return false; &#125;&#125; 6.2. 配置自定义插件12&lt;plugin type="demo.mybatis.plugin.LombokPlugin"&gt;&lt;/plugin&gt;]]></content>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis 动态SQL]]></title>
    <url>%2F2019%2F04%2F09%2FMyBatis%2FMyBatis-%E5%8A%A8%E6%80%81SQL%2F</url>
    <content type="text"><![CDATA[1. 动态SQL简介MyBatis 的强大特性之一便是它的动态 SQL。如果你有使用 JDBC 或其它类似框架的经验，你就能体会到根据不同条件拼接 SQL 语句的痛苦。例如拼接时要确保不能忘记添加必要的空格，还要注意去掉列表最后一个列名的逗号。利用动态 SQL 这一特性可以彻底摆脱这种痛苦]]></content>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java NIO简介]]></title>
    <url>%2F2019%2F04%2F09%2FNIO%2FJava-NIO%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[1. NIO简介Java NIO（New IO）是从Java 1.4版本开始引入的一个新的IO API，可以替代标准的Java IO API。NIO与原来的IO有同样的作用和目的，但是使用的方式完全不同，NIO支持面向缓冲区的、基于通道的IO操作。NIO将以更加高效的方式进行文件的读写操作 2. NIO与传统IO的区别 IO NIO 面向流（Stream Oriented） 面向缓冲（Buffer Oriented） 阻塞IO（Blocking IO） 非阻塞IO（Non Blocking IO） 无 选择器（Selectors） 3. 通道和缓冲区Java NIO系统的核心在于：通道（Channel）和缓冲区（Buffer）。通道表示打开到IO设备（例如：文件、套接字）的连接。若需要使用NIO系统，需要获取用于连接IO设备的通道以及用于容纳数据的缓冲区。然后操作缓冲区，对数据进行处理 简而言之，Channel负责传输，Buffer负责存储]]></content>
      <tags>
        <tag>Java</tag>
        <tag>NIO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis 注解开发]]></title>
    <url>%2F2019%2F04%2F09%2FMyBatis%2FMyBatis-%E6%B3%A8%E8%A7%A3%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[1. 环境搭建1.1. 依赖12345678910111213141516171819202122232425262728293031&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.6&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- log4j 用于mybatis输出日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;!-- mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- mysql --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.2. log4j.properties用于打印MyBatis日志123456# Global logging configurationlog4j.rootLogger=DEBUG, stdout# Console output...log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%5p [%t] - %m%n 1.3. jdbc.properties1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/test?useSSL=truejdbc.username=rootjdbc.password=123456 1.4. MyBatis全局配置文件mybatis-config.xml1234567891011121314151617181920212223242526272829&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;properties resource="jdbc.properties"/&gt; &lt;settings&gt; &lt;setting name="mapUnderscoreToCamelCase" value="true"/&gt; &lt;/settings&gt; &lt;environments default="development"&gt; &lt;environment id="development"&gt; &lt;transactionManager type="JDBC"/&gt; &lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="$&#123;jdbc.driver&#125;"/&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;"/&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;"/&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;package name="demo.mybatis.mapper"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 1.5. 实体类12345678@Data@NoArgsConstructor@AllArgsConstructorpublic class Person &#123; private Long personId; private String firstName; private Date birthday;&#125; 1.6. Mapper1234567891011121314151617181920public interface PersonMapper &#123; @Select("SELECT * FROM person WHERE person_id = #&#123;personId&#125;") Person findById(Long personId); @Select("SELECT * FROM person") List&lt;Person&gt; listPerson(); @Select("SELECT * FROM person WHERE person_id = #&#123;personId&#125; AND first_name = #&#123;firstName&#125;") Person findByIdAndFirstName(@Param("personId") Long personId, @Param("firstName") String firstName); @Delete("DELETE FROM person WHERE person_id = #&#123;personId&#125;") int deleteById(Long personId); @Update("UPDATE person SET first_name = #&#123;firstName&#125;, birthday = #&#123;birthday&#125; " + "WHERE person_id = #&#123;personId&#125;") int updateById(Person person); @Update("INSERT INTO person (first_name, birthday) VALUES (#&#123;firstName&#125;, #&#123;birthday&#125;)") int insert(Person perosn);&#125; 1.7. 测试123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class MybatisTest &#123; SqlSessionFactory sqlSessionFactory = null; SqlSession sqlSession = null; @Before public void setup() throws IOException &#123; InputStream in = Resources.getResourceAsStream("mybatis-config.xml"); sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); sqlSession = sqlSessionFactory.openSession(); &#125; @After public void teardown() &#123; if (sqlSession != null) &#123; sqlSession.close(); &#125; &#125; @Test public void findById() throws IOException &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); Person person = mapper.findById(32L); System.out.println(person); &#125; @Test public void listPerson() &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); List&lt;Person&gt; list = mapper.listPerson(); list.forEach(System.out::println); &#125; @Test public void findByIdAndFirstName() throws IOException &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); Person person = mapper.findByIdAndFirstName(32L, "李四"); System.out.println(person); &#125; @Test public void deleteById() throws IOException &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); mapper.deleteById(3L); sqlSession.commit(); &#125; @Test public void insert() throws IOException &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); Person person = new Person(null, "张三", new Date()); mapper.insert(person); System.out.println(person); sqlSession.commit(); &#125; @Test public void updateById() throws IOException &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); Person person = new Person(32L, "李四", new Date()); mapper.updateById(person); sqlSession.commit(); &#125;&#125;]]></content>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis 多表联合查询]]></title>
    <url>%2F2019%2F04%2F09%2FMyBatis%2FMyBatis-%E5%A4%9A%E8%A1%A8%E8%81%94%E5%90%88%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[1. 一对一查询1.1. 表结构person表 字段 类型 person_id bigint name varchar first_name varchar birthday datetime card_id bigint card表 字段 类型 card_id bigint card_name varchar 1.2. 实体类12345678910@Data@NoArgsConstructor@AllArgsConstructorpublic class Person &#123; private Long personId; private String name; private String firstName; private Date birthday; private Card card;&#125; 1234567@Data@NoArgsConstructor@AllArgsConstructorpublic class Card &#123; private Long cardId; private String cardName;&#125; 1.3. Mapper123public interface PersonMapper &#123; Person findById(Long personId);&#125; 1.4. 映射文件方式1：级联属性封装1234567891011121314151617181920212223242526&lt;!-- type: 指定为哪个POJO自定义封装规则 id: 取个标识，可以被引用--&gt;&lt;resultMap id="person" type="demo.mybatis.entity.Person"&gt; &lt;!-- 已开启mapUnderscoreToCamelCase，可省略该配置。但是MyBatis官方説，指出ID有利于提高性能 &lt;id column="person_id" property="personId"/&gt; --&gt; &lt;id column="person_id" property="personId"/&gt; &lt;!-- &lt;result column="name" property="name"/&gt; column与property相同，可以省略该配置 &lt;result column="birthday" property="birthday"/&gt; column与property相同，可以省略该配置 &lt;result column="first_name" property="firstName"/&gt; 已开启mapUnderscoreToCamelCase，可省略该配置 --&gt; &lt;!-- 级联属性封装 --&gt; &lt;result column="card_id" property="card.cardId"/&gt; &lt;result column="card_name" property="card.cardName"/&gt;&lt;/resultMap&gt;&lt;!-- resultMap: 通过ID引用自定义的resultMap --&gt;&lt;select id="findById" resultMap="person"&gt; SELECT p.*, c.card_name FROM person p LEFT JOIN card c ON p.card_id = c.card_id WHERE p.person_id = #&#123;personId&#125;&lt;/select&gt; 方式2：使用association。要注意的是，使用了association之后，&lt;id&gt;和&lt;result&gt;都不要省略，即使column和property同名。如果省略，字段就不会被赋值12345678910111213141516&lt;resultMap id="person" type="demo.mybatis.entity.Person"&gt; &lt;id column="person_id" property="personId"/&gt; &lt;result column="name" property="name"/&gt; &lt;result column="birthday" property="birthday"/&gt; &lt;result column="first_name" property="firstName"/&gt; &lt;association property="card" javaType="demo.mybatis.entity.Card"&gt; &lt;id column="card_id" property="cardId"/&gt; &lt;result column="card_name" property="cardName"/&gt; &lt;/association&gt;&lt;/resultMap&gt;&lt;!-- resultMap: 通过ID引用自定义的resultMap --&gt;&lt;select id="findById" resultMap="person"&gt; SELECT p.*, c.card_name FROM person p LEFT JOIN card c ON p.card_id = c.card_id WHERE p.person_id = #&#123;personId&#125;&lt;/select&gt; 1.5. 测试运行测试方法，查询Person时，Card也会被封装1234567891011@Testpublic void test() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); Person person = mapper.findById(3L); System.out.println("person = " + person); &#125; finally &#123; sqlSession.close(); &#125;&#125; 2. 多对一查询多对一和一对一是一样的 2.1. 表结构one表 字段 描述 one_id bigint one_name varchar many表 字段 描述 many_id bigint many_name varchar one_id bigint 2.2. 实体类1234567@Data@NoArgsConstructor@AllArgsConstructorpublic class One &#123; private Long oneId; private String oneName;&#125; 12345678@Data@NoArgsConstructor@AllArgsConstructorpublic class Many &#123; private Long manyId; private String manyName; private One one;&#125; 2.3. Mapper123public interface ManyMapper &#123; Many findById(Long manyId);&#125; 2.4. 映射文件12345678910111213&lt;resultMap id="many" type="demo.mybatis.entity.Many"&gt; &lt;id column="many_id" property="manyId"/&gt; &lt;result column="many_name" property="manyName"/&gt; &lt;association property="one" javaType="demo.mybatis.entity.One"&gt; &lt;id column="one_id" property="oneId"/&gt; &lt;result column="one_name" property="oneName"/&gt; &lt;/association&gt;&lt;/resultMap&gt;&lt;select id="findById" resultMap="many"&gt; SELECT many_id, many_name, o.* FROM many m LEFT JOIN one o ON m.one_id = o.one_id WHERE many_id = #&#123;manyId&#125; &lt;/select&gt; 2.5. 测试1234567891011@Testpublic void test() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; ManyMapper mapper = sqlSession.getMapper(ManyMapper.class); Many many = mapper.findById(1L); System.out.println("many = " + many); &#125; finally &#123; sqlSession.close(); &#125;&#125; 3. 一对多查询3.1. 表结构同多对一 3.2. 实体类1234567@Data@NoArgsConstructor@AllArgsConstructorpublic class Many &#123; private Long manyId; private String manyName;&#125; 12345678@Data@NoArgsConstructor@AllArgsConstructorpublic class One &#123; private Long oneId; private String oneName; private List&lt;Many&gt; manyList;&#125; 3.3. Mapper123public interface OneMapper &#123; One findById(Long oneId);&#125; 3.4. 映射文件1234567891011121314151617&lt;resultMap id="one" type="demo.mybatis.entity.One"&gt; &lt;id column="one_id" property="oneId"/&gt; &lt;result column="one_name" property="oneName"/&gt; &lt;!-- property: 指定集合属性 ofType: 指定集合元素类型 --&gt; &lt;collection property="manyList" ofType="demo.mybatis.entity.Many"&gt; &lt;id column="many_id" property="manyId"/&gt; &lt;result column="many_name" property="manyName"/&gt; &lt;/collection&gt;&lt;/resultMap&gt;&lt;select id="findById" resultMap="one"&gt; SELECT many_id, many_name, o.* FROM one o LEFT JOIN many m ON o.one_id = m.one_id WHERE o.one_id = #&#123;oneId&#125;&lt;/select&gt; 3.5. 测试1234567891011@Testpublic void test() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; OneMapper mapper = sqlSession.getMapper(OneMapper.class); One one = mapper.findById(1L); System.out.println("one = " + one); &#125; finally &#123; sqlSession.close(); &#125;&#125;]]></content>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis SQL映射文件]]></title>
    <url>%2F2019%2F04%2F08%2FMyBatis%2FMyBatis-SQL%E6%98%A0%E5%B0%84%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[参考 官方中文文档 http://www.mybatis.org/mybatis-3/zh/sqlmap-xml.html 1. SQL映射文件 概述映射文件指导着MyBatis如何进行数据库增删改查，有着非常重要的意义。 MyBatis 的真正强大在于它的映射语句，这是它的魔力所在。由于它的异常强大，映射器的 XML 文件就显得相对简单。如果拿它跟具有相同功能的 JDBC 代码进行对比，你会立即发现省掉了将近 95% 的代码。MyBatis 为聚焦于 SQL 而构建，以尽可能地为你减少麻烦 SQL 映射文件只有很少的几个顶级元素（按照应被定义的顺序列出）： cache 命名空间的二级缓存配置 cache-ref 其他命名空间缓存配置的引用。 resultMap 自定义结果集映射 parameterMap 已废弃！iBatis老式风格的参数映射 sql 抽取可重用语句块。 insert 映射插入语句 update 映射更新语句 delete 映射删除语句 select 映射查询语句 2. insert、update、delete元素insert、update、delete元素有以下共同属性 属性 描述 id 命名空间中的唯一标识符，可被用来代表这条语句。 parameterType 将要传入语句的参数的完全限定类名或别名。这个属性是可选的，因为 MyBatis 可以通过类型处理器推断出具体传入语句的参数，默认值为未设置（unset）。 parameterMap 这是引用外部 parameterMap 的已经被废弃的方法。请使用内联参数映射和 parameterType 属性。 flushCache 将其设置为 true 后，只要语句被调用，都会导致本地缓存和二级缓存被清空，默认值：true（对于 insert、update 和 delete 语句）。 timeout 这个设置是在抛出异常之前，驱动程序等待数据库返回请求结果的秒数。默认值为未设置（unset）（依赖驱动）。 statementType STATEMENT，PREPARED 或 CALLABLE 的一个。这会让 MyBatis 分别使用 Statement，PreparedStatement 或 CallableStatement，默认值：PREPARED。 useGeneratedKeys （仅对 insert 和 update 有用）这会令 MyBatis 使用 JDBC 的 getGeneratedKeys 方法来取出由数据库内部生成的主键（比如：像 MySQL 和 SQL Server 这样的关系数据库管理系统的自动递增字段），默认值：false。 keyProperty （仅对 insert 和 update 有用）唯一标记一个属性，MyBatis 会通过 getGeneratedKeys 的返回值或者通过 insert 语句的 selectKey 子元素设置它的键值，默认值：未设置（unset）。如果希望得到多个生成的列，也可以是逗号分隔的属性名称列表。 keyColumn （仅对 insert 和 update 有用）通过生成的键值设置表中的列名，这个设置仅在某些数据库（像 PostgreSQL）是必须的，当主键列不是表中的第一列的时候需要设置。如果希望使用多个生成的列，也可以设置为逗号分隔的属性名称列表。 databaseId 如果配置了数据库厂商标识（databaseIdProvider），MyBatis 会加载所有的不带 databaseId 或匹配当前 databaseId 的语句；如果带或者不带的语句都有，则不带的会被忽略。 2.1. id属性与Mapper方法名绑定，一个方法对应一个id。id是不允许重复的，所以Mapper不要定义重载的方法 2.2. parameterType属性设置形参的全类名或者别名。这个属性是可选的，因为MyBatis可以通过TypeHandler推荐出形参类型，所以实际开发中，直接省略parameterType即可 123456789&lt;!-- Person findById(Long personId) --&gt;&lt;select id="findById" parameterType="java.lang.Long" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person WHERE person_id = #&#123;personId&#125;&lt;/select&gt;&lt;!-- 可以直接省略parameterType --&gt;&lt;select id="findById" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person WHERE person_id = #&#123;personId&#125;&lt;/select&gt; 2.3. timeout属性timeout属性可以忽略。之后会用Spring事务来控制 2.4. statementType属性可选值： STATEMENT：使用原生JDBC的Statement来执行该SQL PREPARED：使用原生JDBC的PreparedStatement来执行该SQL。相对Statement要更安全，而且预编译能加速执行 CallableStatement：使用原生JDBC的CallableStatement来执行该SQL，CallableStatement是用来执行存储过程的 statementType默认值就是PREPARED，一般都不必调整 2.5. keyColumn属性仅对 insert 和 update 有用，而且仅对特定数据库（像 PostgreSQL）才有用到。在开发中，该属性几乎不用 2.6. databaseId属性在数据库迁移时有用到，可参考全局配置文件中的databaseIdProvider 2.7. useGeneratedKeys和keyProperty属性insert 声明主键生成12345678910&lt;!-- useGeneratedKeys: 生成主键 keyProperty: 指定哪个属性对应数据库中的主键，这样生成主键后会自动赋值给该属性--&gt;&lt;insert id="insert" useGeneratedKeys="true" keyProperty="personId"&gt; INSERT INTO person (name, birthday) VALUES (#&#123;name&#125;, #&#123;birthday&#125;)&lt;/insert&gt; useGeneratedKeys=&quot;true&quot; 的原理是执行JDBC的Statement接口的getGeneratedKeys()，来获取生成的主键 3. SelectKey执行前生成主键1234567891011121314&lt;insert id="insert"&gt; &lt;!-- order="BEFORE": 在执行SQL之前，执行&lt;selectKey&gt;中的语句 resultType="long": 执行&lt;selectKey&gt;中的语句之后，将返回值转为Java中的Long类型 keyProperty="personId": 将返回值赋值给personId属性 --&gt; &lt;selectKey order="BEFORE" resultType="long" keyProperty="personId"&gt; SELECT MAX(person_id) + 1 FROM person &lt;/selectKey&gt; INSERT INTO person (name, birthday) VALUES (#&#123;name&#125;, #&#123;birthday&#125;)&lt;/insert&gt; 执行后生成主键 SELECT LAST_INSERT_ID()，效果和直接用useGeneratedKeys是一样的1234567891011121314&lt;insert id="insert"&gt; &lt;!-- order="AFTER": 在执行SQL之后，执行&lt;selectKey&gt;中的语句 resultType="long": 执行&lt;selectKey&gt;中的语句之后，将返回值转为Java中的Long类型 keyProperty="personId": 将返回值赋值给personId属性 --&gt; &lt;selectKey order="AFTER" resultType="long" keyProperty="personId"&gt; SELECT LAST_INSERT_ID() &lt;/selectKey&gt; INSERT INTO person (name, birthday) VALUES (#&#123;name&#125;, #&#123;birthday&#125;)&lt;/insert&gt; 4. 获取传递的参数4.1. 传递单个简单类型传递单个简单类型参数时，#{}内部可以填写任意名称，如#{hello}，#{value}等等，都可以引用参数。 1234&lt;!-- Person findById(Long personId) --&gt;&lt;select id="findById" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person WHERE person_id = #&#123;abc&#125;&lt;/select&gt; 虽然名字随意，但是最好还是取#{value}或#{参数名}，这样可读性更好一点 4.2. 传递多个简单类型直接用参数名引用会出错1234&lt;!-- Person findByIdAndName(Long personId, String name) --&gt;&lt;select id="findByIdAndName" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person WHERE person_id = #&#123;personId&#125; AND name = #&#123;name&#125;&lt;/select&gt; 运行测试方法1234567891011@Testpublic void test() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); Person person = mapper.findByIdAndName(3L, "张三"); System.out.println(person); &#125; finally &#123; sqlSession.close(); &#125;&#125; 会出错。提示你只能通过[arg1, arg0, param1, param2]这几个参数来引用。注意老版MyBatis会提示[0, 1, param1, param2]这几个参数，现在高版本已经发生变化1Cause: org.apache.ibatis.binding.BindingException: Parameter 'personId' not found. Available parameters are [arg1, arg0, param1, param2] 以下几种方式都可以。arg0和param1代表第1个参数，之后的类推1234567891011121314&lt;!-- Person findByIdAndName(Long personId, String name) --&gt;&lt;select id="findByIdAndName" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person WHERE person_id = #&#123;arg0&#125; AND name = #&#123;arg1&#125;&lt;/select&gt;&lt;!-- Person findByIdAndName(Long personId, String name) --&gt;&lt;select id="findByIdAndName" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person WHERE person_id = #&#123;param1&#125; AND name = #&#123;param2&#125;&lt;/select&gt;&lt;!-- Person findByIdAndName(Long personId, String name) --&gt;&lt;select id="findByIdAndName" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person WHERE person_id = #&#123;param1&#125; AND name = #&#123;arg1&#125;&lt;/select&gt; 但是上述方式可读性不高。此时可以结合MyBatis的@Param注解，给参数命名。实例开发时也推荐用这种方式 123public interface PersonMapper &#123; Person findByIdAndName(@Param("personId") Long personId, @Param("name") String name);&#125; 现在可以使用[personId, name, param1, param2]来引用对应的参数123456789&lt;!-- Person findByIdAndName(Long personId, String name) --&gt;&lt;select id="findByIdAndName" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person WHERE person_id = #&#123;personId&#125; AND name = #&#123;name&#125;&lt;/select&gt;&lt;!-- Person findByIdAndName(Long personId, String name) --&gt;&lt;select id="findByIdAndName" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person WHERE person_id = #&#123;param1&#125; AND name = #&#123;arg1&#125;&lt;/select&gt; 4.3. 传递POJO直接用 #{pojo对应的属性名} 就可以获取POJO对应的属性值 1234567&lt;!-- int insert(Person perosn) --&gt;&lt;insert id="insert"&gt; INSERT INTO person (name, birthday) VALUES (#&#123;name&#125;, #&#123;birthday&#125;)&lt;/insert&gt; 4.4. 传递MapMapper接口123public interface PersonMapper &#123; Person findByIdAndName(Map&lt;String,Object&gt; map);&#125; 测试类1234567891011121314@Testpublic void test() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); HashMap&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put("personId", 3L); map.put("name", "张三"); Person person = mapper.findByIdAndName(map); System.out.println(person); &#125; finally &#123; sqlSession.close(); &#125;&#125; 直接用 #{Map中的key} 就可以引用对应的value1234&lt;!-- Person findByIdAndName(Map&lt;String,Object&gt; map) --&gt;&lt;select id="findByIdAndName" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person WHERE person_id = #&#123;personId&#125; AND name = #&#123;name&#125;&lt;/select&gt; 4.5. 传递级联POJO12345678@Data@NoArgsConstructor@AllArgsConstructorpublic class Person &#123; private Long personId; private String name; private Date birthday;&#125; 123456@Data@NoArgsConstructor@AllArgsConstructorpublic class QueryVO &#123; private Person person;&#125; Mapper接口123public interface PersonMapper &#123; int insert(QueryVO queryVO);&#125; 测试方法1234567891011121314151617@Testpublic void insert() throws IOException &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); Person person = new Person(null, "李四", new Date()); QueryVO queryVO = new QueryVO(person); int result = mapper.insert(queryVO); System.out.println("result = " + result); System.out.println("person = " + person); sqlSession.commit(); &#125; finally &#123; sqlSession.close(); &#125;&#125; POJO可以直接用 #{属性名} 来得到对象。#{person}得到的是Person对象，可以继续用点操作符引用person的属性1234567&lt;!-- int insert(QueryVO queryVO) --&gt;&lt;insert id="insert"&gt; INSERT INTO person (name, birthday) VALUES (#&#123;person.name&#125;, #&#123;person.birthday&#125;)&lt;/insert&gt; 5. #{}和${}取值的区别在获取参数时，你会发现用#{}和${}好像没有什么区别。实际上，二者的区别还是存在的。下面做一个实验 Mapper接口123public interface PersonMapper &#123; Person findByIdAndName(@Param("personId") Long personId, @Param("name") String name);&#125; SQL映射文件。语句中，分别使用${personId}和#{name}取值1234&lt;!-- Person findByIdAndName(@Param("personId") Long personId, @Param("name") String name) --&gt;&lt;select id="findByIdAndName" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person WHERE person_id = $&#123;personId&#125; AND name = #&#123;name&#125;&lt;/select&gt; 配置好log4j输出日志，再运行以下测试方法1234567891011@Testpublic void test() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); Person person = mapper.findByIdAndName(3L, "张三"); System.out.println(person); &#125; finally &#123; sqlSession.close(); &#125;&#125; 运行之后，输出SQL如下。你会发现，${person_id}是直接以字符串的形式拼接到SQL语句中，而#{name}是以预编译占位符的方式，在预编译之后将值填充进去。其实这就是二者的区别123DEBUG [main] - ==&gt; Preparing: SELECT * FROM person WHERE person_id = 3 AND name = ? DEBUG [main] - ==&gt; Parameters: 张三(String)DEBUG [main] - &lt;== Total: 1 #{}是经过预编译的，安全性更高，可以防止SQL注入攻击，在获取参数时，使用#{}显然要比${}好 那么${}就没有用武之地了吗？其实${}还是有应用场景的。在JDBC中，预编译仅限于SQL语句中的参数，如以下几种形式12SELECT * FROM person WHERE name = ?SELECT * FROM person WHERE id = ? AND name = ? 但是不能对其它位置进行预编译，如表名12-- ERROR，不能对表名进行预编译SELECT * FROM ? WHERE name = "jack" 也就是説，MyBatis不能写以下语句1234&lt;!-- ERROR，因为#&#123;tableName&#125;表名不能预编译 --&gt;&lt;select id="findByIdAndName" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM #&#123;tableName&#125; WHERE person_id = #&#123;personId&#125; AND name = #&#123;name&#125;&lt;/select&gt; 此时，就可以用${}，传入表名，进行字符串动态拼接1234public interface PersonMapper &#123; // 加一个表名参数 Person findByIdAndName(@Param("personId") Long personId, @Param("name") String name, @Param("tableName") String tableName);&#125; 1234&lt;!-- Person findByIdAndName(@Param("personId") Long personId, @Param("name") String name, @Param("tableName") String tableName) --&gt;&lt;select id="findByIdAndName" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM $&#123;tableName&#125; WHERE person_id = #&#123;personId&#125; AND name = #&#123;name&#125;&lt;/select&gt; 运行测试方法，成功123456789101112@Testpublic void test() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); // 第3个参数代表person表 Person person = mapper.findByIdAndName(3L, "张三", "person"); System.out.println(person); &#125; finally &#123; sqlSession.close(); &#125;&#125; 这是日志输出123DEBUG [main] - ==&gt; Preparing: SELECT * FROM person WHERE person_id = ? AND name = ? DEBUG [main] - ==&gt; Parameters: 3(Long), 张三(String)DEBUG [main] - &lt;== Total: 1 综上可知，在支持参数预编译的位置取值，尽量使用#{}；在不支持的地方，只能通过${}实现字符串拼接 6. 返回值类型6.1. 返回简单类型123public interface PersonMapper &#123; Long count();&#125; resultType设置为返回值的类型1234&lt;!-- Long count() --&gt;&lt;select id="count" resultType="java.lang.Long"&gt; SELECT COUNT(*) FROM person&lt;/select&gt; 6.2. 返回POJO123public interface PersonMapper &#123; Person findById(Long personId);&#125; resultType设置为返回值的类型1234&lt;!-- Person findById(Long personId) --&gt;&lt;select id="findById" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person WHERE person_id = #&#123;personId&#125;&lt;/select&gt; 6.3. 返回ListMapper接口123public interface PersonMapper &#123; List&lt;Person&gt; listPerson();&#125; 映射文件。返回List，resultType设置为List的元素类型即可，MyBatis会自动判断，封装为List并返回1234&lt;!-- List&lt;Person&gt; listPerson() --&gt;&lt;select id="listPerson" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person&lt;/select&gt; 6.4. 返回Map单条记录返回单条记录时，可以用Map封装。key代表列名，value代表值 resultType设置为map1234&lt;!-- Map&lt;String, Object&gt; findById(Long personId) --&gt;&lt;select id="findById" resultType="map"&gt; SELECT * FROM person WHERE person_id = #&#123;value&#125;&lt;/select&gt; 6.5. 返回Map多条记录返回多条记录时，可以用Map&lt;主键类型, POJO&gt;，每一个key/value都代表一行记录 首先，要用@MapKey注解，声明Map的key要封装哪个属性1234public interface PersonMapper &#123; @MapKey("personId") Map&lt;Long, Person&gt; listPerson();&#125; resultType设置为POJO，即value的类型123&lt;select id="listPerson" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person&lt;/select&gt; 6.6. resultMap自定义封装类型当查询返回的列名与POJO属性名不一致时，就需要用resultMap自定义封装类型 但是有几种情况是可以避免使用resultMap的 下划线命名法与驼峰命名法不一致，这种情况可以直接用MyBatis自带的mapUnderscoreToCamelCase，不必使用resultMap 可以用SQL的AS对列名重命名，使得与POJO属性名一致，这样就不必使用resultMap了。 resultMap主要还是应用于多表联合查询]]></content>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis 全局配置文件]]></title>
    <url>%2F2019%2F04%2F08%2FMyBatis%2FMyBatis-%E5%85%A8%E5%B1%80%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[参考 MyBatis官方XML配置中文说明 http://www.mybatis.org/mybatis-3/zh/configuration.html#properties 1. 配置顺序configuration（配置顺序） properties（属性） settings（设置） typeAliases（类型别名） typeHandlers（类型处理器） objectFactory（对象工厂） plugins（插件） environments（环境配置） environment（环境变量） transactionManager（事务管理器） dataSource（数据源） databaseIdProvider（数据库厂商标识） mappers（映射器） 2. 属性（properties）2.1. 定义属性在 &lt;properties&gt; 内部可以通过 &lt;propertie&gt; 定义key/value，然后通过${var} 来引用属性 123456789101112131415161718192021&lt;properties&gt; &lt;!-- 定义属性 --&gt; &lt;property name="jdbc.driver" value="com.mysql.jdbc.Driver"/&gt; &lt;property name="jdbc.url" value="jdbc:mysql://localhost:3306/test?useSSL=true"/&gt; &lt;property name="jdbc.username" value="root"/&gt; &lt;property name="jdbc.password" value="123456"/&gt;&lt;/properties&gt;&lt;environments default="development"&gt; &lt;environment id="development"&gt; &lt;transactionManager type="JDBC"/&gt; &lt;dataSource type="POOLED"&gt; &lt;!-- 通过$&#123;var&#125;来引用 --&gt; &lt;property name="driver" value="$&#123;jdbc.driver&#125;"/&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;"/&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;"/&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;"/&gt; &lt;/dataSource&gt; &lt;/environment&gt;&lt;/environments&gt; 2.2. 引用外部配置文件外部配置文件 jdbc.properties 的内容如下：1234jdbc.driver=com.mysql.jdbc.Driverjdbc.url=jdbc:mysql://localhost:3306/test?useSSL=truejdbc.username=rootjdbc.password=123456 &lt;properties&gt; 的 resource 属性 可以引用classpath下的配置文件 1234567891011121314&lt;properties resource="jdbc.properties" /&gt;&lt;environments default="development"&gt; &lt;environment id="development"&gt; &lt;transactionManager type="JDBC"/&gt; &lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="$&#123;jdbc.driver&#125;"/&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;"/&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;"/&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;"/&gt; &lt;/dataSource&gt; &lt;/environment&gt;&lt;/environments&gt; &lt;properties&gt; 的 url 属性 可以引用网络中的配置文件，包括本地磁盘中的配置文件 1234567891011121314&lt;properties url="file:///c:/jdbc.properties" /&gt;&lt;environments default="development"&gt; &lt;environment id="development"&gt; &lt;transactionManager type="JDBC"/&gt; &lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="$&#123;jdbc.driver&#125;"/&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;"/&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;"/&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;"/&gt; &lt;/dataSource&gt; &lt;/environment&gt;&lt;/environments&gt; 引用外部配置文件，和直接定义属性，二者可以结合起来。配置加载顺序遵循就近原则。先加载外部，再加载内部。若有冲突，则内部会覆盖外部同名配置12345678&lt;!-- resource引用配置文件 --&gt;&lt;properties resource="file1.properties"&gt; &lt;!-- 直接定义属性 --&gt; &lt;property name="jdbc.driver" value="com.mysql.jdbc.Driver"/&gt; &lt;property name="jdbc.url" value="jdbc:mysql://localhost:3306/test?useSSL=true"/&gt; &lt;property name="jdbc.username" value="root"/&gt; &lt;property name="jdbc.password" value="123456"/&gt;&lt;/properties&gt; 3. 设置（settings）settings 是MyBatis中极为重要的调整设置，它们会改变 MyBatis 的运行时行为。 详细说明可以参考官方文档 http://www.mybatis.org/mybatis-3/zh/configuration.html#settings ，下面列举一些常用的配置 3.1. mapUnderscoreToCamelCase凡是使用Mybatis，一般都要设置mapUnderscoreToCamelCase为true123456&lt;!-- mapUnderscoreToCamelCase 描述：是否开启自动驼峰命名规则（camel case）映射， 即从经典数据库列名 A_COLUMN 到经典 Java 属性名 aColumn 的类似映射 有效值：true | false 默认值：false --&gt;&lt;setting name="mapUnderscoreToCamelCase" value="true"/&gt; 4. 类型别名（typeAliases）typeAliases可以给Java数据类型起一个别名，存在的意义仅在于用来减少类完全限定名的冗余 但是在实际开发中，不推荐使用typeAliases，而是推荐直接使用全类名。使用全类名的好处是，你可以很清晰地看出，这个类型属于哪个包。而且使用全类名，在IDE中，可以通过CTRL+左键直接定位到对应的类，便于查看 4.1. typeAlias起别名12345&lt;typeAliases&gt; &lt;!-- 省略alias属性时，别名默认就是简单类名，例如demo.mybatis.entity.Person的别名默认就是Person --&gt; &lt;typeAlias type="demo.mybatis.entity.Person" /&gt; &lt;typeAlias type="demo.mybatis.entity.Car" alias="car" /&gt;&lt;/typeAliases&gt; 在SQL映射文件中，都可以用别名替代全类名1234&lt;!-- resultType的值本来要写"demo.mybatis.entity.Person"，现在就可以用别名简写了 --&gt;&lt;select id="findById" resultType="Person"&gt; SELECT * FROM person WHERE person_id = #&#123;personId&#125;&lt;/select&gt; 需要注意的是，别名的使用是不区分大小写的。例如你给demo.mybatis.entity.Person设置别名为person，引用别名时，Person、perSON等等都是等价的 1234&lt;!-- 使用PERSON也是可以的 --&gt;&lt;select id="findById" resultType="PERSON"&gt; SELECT * FROM person WHERE person_id = #&#123;personId&#125;&lt;/select&gt; 4.2. package批量起别名假如有很多个类，你都想起别名，一个一个设置别名就会很累，此时可以使用&lt;package&gt;，mybatis会自动扫描该包及其子包（递归扫描）所有的类，将简单类名作为别名。 需要注意，这里的别名仍是不区分大小写的 1234567&lt;typeAliases&gt; &lt;!-- demo.entity.Person ==&gt; Person demo.entity.aaa.bbb.Car ==&gt; Car --&gt; &lt;package name="demo.entity" /&gt;&lt;/typeAliases&gt; 4.3. 内置别名MyBatis已经给Java很多的内置类设置了别名，你可以直接使用 别名 映射的类型 _byte byte _long long _short short _int int _integer int _double double _float float _boolean boolean string String byte Byte long Long short Short int Integer integer Integer double Double float Float boolean Boolean date Date decimal BigDecimal bigdecimal BigDecimal object Object map Map hashmap HashMap list List arraylist ArrayList collection Collection iterator Iterator 5. 类型处理器（typeHandlers）无论是 MyBatis 在预处理语句（PreparedStatement）中设置一个参数时，还是从结果集中取出一个值时， 都会用类型处理器将获取的SQL类型的值以合适的方式转换成 Java 类型。下表描述了一些默认的类型处理器。 提示：从 3.4.5 开始，MyBatis 默认支持 JSR-310（日期和时间 API） 类型处理器 Java 类型 JDBC 类型 BooleanTypeHandler java.lang.Boolean, boolean 数据库兼容的 BOOLEAN ByteTypeHandler java.lang.Byte, byte 数据库兼容的 NUMERIC 或 BYTE ShortTypeHandler java.lang.Short, short 数据库兼容的 NUMERIC 或 SMALLINT IntegerTypeHandler java.lang.Integer, int 数据库兼容的 NUMERIC 或 INTEGER LongTypeHandler java.lang.Long, long 数据库兼容的 NUMERIC 或 BIGINT FloatTypeHandler java.lang.Float, float 数据库兼容的 NUMERIC 或 FLOAT DoubleTypeHandler java.lang.Double, double 数据库兼容的 NUMERIC 或 DOUBLE BigDecimalTypeHandler java.math.BigDecimal 数据库兼容的 NUMERIC 或 DECIMAL StringTypeHandler java.lang.String CHAR, VARCHAR ClobReaderTypeHandler java.io.Reader - ClobTypeHandler java.lang.String CLOB, LONGVARCHAR NStringTypeHandler java.lang.String NVARCHAR, NCHAR NClobTypeHandler java.lang.String NCLOB BlobInputStreamTypeHandler java.io.InputStream - ByteArrayTypeHandler byte[] 数据库兼容的字节流类型 BlobTypeHandler byte[] BLOB, LONGVARBINARY DateTypeHandler java.util.Date TIMESTAMP DateOnlyTypeHandler java.util.Date DATE TimeOnlyTypeHandler java.util.Date TIME SqlTimestampTypeHandler java.sql.Timestamp TIMESTAMP SqlDateTypeHandler java.sql.Date DATE SqlTimeTypeHandler java.sql.Time TIME ObjectTypeHandler Any OTHER 或未指定类型 EnumTypeHandler Enumeration Type VARCHAR 或任何兼容的字符串类型，用以存储枚举的名称（而不是索引值） EnumOrdinalTypeHandler Enumeration Type 任何兼容的 NUMERIC 或 DOUBLE 类型，存储枚举的序数值（而不是名称）。 SqlxmlTypeHandler java.lang.String SQLXML InstantTypeHandler java.time.Instant TIMESTAMP LocalDateTimeTypeHandler java.time.LocalDateTime TIMESTAMP LocalDateTypeHandler java.time.LocalDate DATE LocalTimeTypeHandler java.time.LocalTime TIME OffsetDateTimeTypeHandler java.time.OffsetDateTime TIMESTAMP OffsetTimeTypeHandler java.time.OffsetTime TIME ZonedDateTimeTypeHandler java.time.ZonedDateTime TIMESTAMP YearTypeHandler java.time.Year INTEGER MonthTypeHandler java.time.Month INTEGER YearMonthTypeHandler java.time.YearMonth VARCHAR 或 LONGVARCHAR JapaneseDateTypeHandler java.time.chrono.JapaneseDate DATE 6. 对象工厂（objectFactory）MyBatis查询得到一条记录，将数据封装到对象中，这个对象就是通过objectFactory来创建的。 MyBatis底层有一个默认的对象工厂，来实例化对象。如果你有额外的需求，可以自己写一个对象工厂。但是实际开发中没有重写的必要 7. 插件（plugins）插件是MyBatis提供的一个非常强大的机制，我们可以通过插件来修改MyBatis的一些核心行为。插件通过动态代理机制，可以介入四大对象的任何一个方法的执行 Executor (update, query, flushStatements, commit, rollback, getTransaction, close, isClosed) ParameterHandler (getParameterObject, setParameters) ResultSetHandler (handleResultSets, handleOutputParameters) StatementHandler (prepare, parameterize, batch, update, query) 8. 环境配置（environments）环境配置就是指配置事务管理器和数据源，每一个环境environment就对应一个transactionManager和dataSource 你可以配置多个环境，如开发是连接数据库A，测试时连接数据库。通过&lt;environments&gt;的default属性，来决定当前SqlSessionFactory实例选择哪个环境 不过要记住：尽管可以配置多个环境，但每个 SqlSessionFactory 实例只能选择一种环境 1234567891011121314151617181920212223&lt;!-- default指定当前使用哪个环境配置 --&gt;&lt;environments default="development"&gt; &lt;!-- 开发环境设置 --&gt; &lt;environment id="development"&gt; &lt;transactionManager type="JDBC"/&gt; &lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="$&#123;jdbc.driver&#125;"/&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;"/&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;"/&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;!-- 测试环境设置，连向另一个数据库 --&gt; &lt;environment id="test"&gt; &lt;transactionManager type="JDBC"/&gt; &lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="$&#123;test.jdbc.driver&#125;"/&gt; &lt;property name="url" value="$&#123;test.jdbc.url&#125;"/&gt; &lt;property name="username" value="$&#123;test.jdbc.username&#125;"/&gt; &lt;property name="password" value="$&#123;test.jdbc.password&#125;"/&gt; &lt;/dataSource&gt; &lt;/environment&gt;&lt;/environments&gt; transactionManager是配置事务管理器的，不需要深研。因为Spring的事务管理机制更加强大，实际开发时是使用Spring来管理事务，MyBatis只要知道就这么个东西就可以了 dataSource配置数据源，就是配置连接数据库的一些参数。实际开发时，数据源也是交给Spring管理的，MyBatis只负责CRUD 9. 数据库厂商标识（databaseIdProvider）Hibernate的数据库移植性非常强，因为所有的SQL都是框架自动生成的。例如项目的数据库从MySQL移植到Oracle，Hibernate会自动检测，在执行时会转为生成Oracle的SQL语句。 但是MyBatis的移植性就没有那么好，因为SQL都是你亲自去写的。如果项目的数据库从MySQL移植到Oracle，你原来的SQL都是针对MySQL的，就会导致项目无法运行。MyBatis考虑到这一情况，就有了databaseIdProvider这一机制。 你可以在全局配置中添加databaseIdProvider123456789101112&lt;!-- type="DB_VENDOR"是固定的写法 --&gt;&lt;databaseIdProvider type="DB_VENDOR"&gt; &lt;!-- name: 数据库厂商的标识，这些标识的值都是固定的， 你可以通过JDBC的Connection的getDatabaseProductName()来获取 value: 给这个标识起一个好用的名字 --&gt; &lt;property name="SQL Server" value="sqlserver"/&gt; &lt;property name="DB2" value="db2"/&gt; &lt;property name="Oracle" value="oracle" /&gt; &lt;property name="MySQL" value="mysql" /&gt; &lt;property name="SQL Server" value="sqlserver" /&gt;&lt;/databaseIdProvider&gt; 在写SQL映射文件时，可以根据数据库环境来执行对应的SQL语句。如果匹配到databaseId，则执行对应的SQL，如果匹配不到，则执行没有databaseId的那个SQL123456789101112&lt;!-- 如果连接的是Oracle，则执行该语句 --&gt;&lt;select id="findById" resultType="demo.mybatis.entity.Person" databaseId="oracle"&gt; SELECT * FROM person WHERE person_id = #&#123;personId&#125;&lt;/select&gt;&lt;!-- 如果连接的是MySQL，则执行该语句 --&gt;&lt;select id="findById" resultType="demo.mybatis.entity.Person" databaseId="mysql"&gt; SELECT * FROM person WHERE person_id = #&#123;personId&#125;&lt;/select&gt;&lt;!-- 如果连接的是其它数据库，则执行该语句 --&gt;&lt;select id="findById" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person WHERE person_id = #&#123;personId&#125;&lt;/select&gt; 10. 映射器（mappers）你写了一些SQL映射文件，MyBatis默认是不知道这些配置文件在哪里的。&lt;mappers&gt;的作用就是告诉MyBatis到哪里去找这些SQL映射文件 10.1. resource使用类路径注册resources是在类路径下找对应的SQL映射文件 123456&lt;mappers&gt; &lt;!-- 给定相对于classpath的路径 --&gt; &lt;mapper resource="org/mybatis/builder/AuthorMapper.xml"/&gt; &lt;mapper resource="org/mybatis/builder/BlogMapper.xml"/&gt; &lt;mapper resource="org/mybatis/builder/PostMapper.xml"/&gt;&lt;/mappers&gt; 10.2. url指定网络地址注册url给出SQL映射文件的网络地址，包括本地路径。在实际开发时，该方式不可取1234567&lt;!-- 使用完全限定资源定位符（URL） --&gt;&lt;mappers&gt; &lt;!-- Linux路径 --&gt; &lt;mapper url="file:///var/mappers/AuthorMapper.xml"/&gt; &lt;!-- Windows路径 --&gt; &lt;mapper url="file:///c:/User.xml" /&gt;&lt;/mappers&gt; 10.3. class指定Mapper接口实现注册class给出指定Mapper接口的全类名。但是光指定Mapper接口，MyBatis怎么知道SQL映射文件在哪里呢？ 原因是，在使用class属性时，MyBatis默认认为SQL映射文件被要求放在与对应Mapper接口同一路径下，且名称必须要求。例如UserMapper接口，对应的SQL映射文件名就是UserMapper.xml，也两个文件必须放在同一路径下 12345&lt;mappers&gt; &lt;mapper class="org.mybatis.builder.AuthorMapper"/&gt; &lt;mapper class="org.mybatis.builder.BlogMapper"/&gt; &lt;mapper class="org.mybatis.builder.PostMapper"/&gt;&lt;/mappers&gt; 10.4. package包扫描注册package是class方式的扩展。&lt;package&gt;会扫描指定包及其子包下所有的Mapper接口，并找到同一路径下的SQL映射文件。且要求SQL映射文件的名称与Mapper接口的名称必须相同 123&lt;mappers&gt; &lt;package name="demo.mybatis.mapper"/&gt;&lt;/mappers&gt; 注意：实际项目就使用package方式引入映射文件，最方便。而且项目中很可能是基于MyBatis的注解驱动和XML混合开发，SQL映射文件可能就直接被省略了，resource/url的方式直接变得不可用了。此时只能选择class/package方式]]></content>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis HelloWorld]]></title>
    <url>%2F2019%2F04%2F08%2FMyBatis%2FMyBatis-HelloWorld%2F</url>
    <content type="text"><![CDATA[1. 环境搭建 1.1. pom.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.6&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- mybatis --&gt; &lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.5.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- log4j 用于输出mybatis日志 --&gt; &lt;dependency&gt; &lt;groupId&gt;log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j&lt;/artifactId&gt; &lt;version&gt;1.2.17&lt;/version&gt; &lt;/dependency&gt; &lt;!-- mysql --&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.32&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;&lt;build&gt; &lt;!-- resources标签 指定Maven构建项目时，要包含哪些资源文件 1、默认情况下，Maven构建时只包含src/main/resources下的资源文件，其它目录，如src/main/java下的资源文件会被Maven忽略 2、因为本项目Mapper配置文件放在src/main/java中，所以要指定包含这样资源文本 3、设定了&lt;resources&gt;，那么默认的src/main/resources就会失效，所以src/main/resources还要额外指定 --&gt; &lt;resources&gt; &lt;!-- 包含自定义位置的资源文件 --&gt; &lt;resource&gt; &lt;directory&gt;src/main/java&lt;/directory&gt; &lt;includes&gt; &lt;include&gt;**/*.xml&lt;/include&gt; &lt;/includes&gt; &lt;/resource&gt; &lt;!-- 包含默认位置的资源文件 --&gt; &lt;resource&gt; &lt;directory&gt;src/main/resources&lt;/directory&gt; &lt;/resource&gt; &lt;/resources&gt;&lt;/build&gt; 1.2. 实体类123456@Datapublic class Person &#123; private Long personId; private String name; private Date birthday;&#125; 1.3. Mapper123456public interface PersonMapper &#123; Person findById(Long personId); int deleteById(Long personId); int updateById(Person person); int insert(Person perosn);&#125; 1.4. SQL映射配置12345678910111213141516171819202122232425&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;!-- namespace设置为对应Mapper的全类名 --&gt;&lt;mapper namespace="demo.mybatis.mapper.PersonMapper"&gt; &lt;select id="findById" resultType="demo.mybatis.entity.Person"&gt; SELECT * FROM person WHERE person_id = #&#123;personId&#125; &lt;/select&gt; &lt;delete id="deleteById"&gt; DELETE FROM person WHERE person_id = #&#123;personId&#125; &lt;/delete&gt; &lt;insert id="updateById"&gt; UPDATE person SET name = #&#123;name&#125;, birthday = #&#123;birthday&#125; WHERE person_id = #&#123;personId&#125; &lt;/insert&gt; &lt;insert id="insert"&gt; INSERT INTO person (name, birthday) VALUES (#&#123;name&#125;, #&#123;birthday&#125;) &lt;/insert&gt;&lt;/mapper&gt; 1.5. log4j配置文件添加log4j.properties。Mybatis默认使用log4j输出日志信息，所以如果要将执行的SQL语句输出到控制台，需要在CLASSPATH（src目录）下添加log4j.properties，内容如下：123456# Global logging configurationlog4j.rootLogger=DEBUG, stdout# Console output...log4j.appender.stdout=org.apache.log4j.ConsoleAppenderlog4j.appender.stdout.layout=org.apache.log4j.PatternLayoutlog4j.appender.stdout.layout.ConversionPattern=%5p [%t] - %m%n 1.6. MyBatis全局配置文件12345678910111213141516171819202122232425262728293031&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;configuration&gt; &lt;settings&gt; &lt;!-- 支持属性使用驼峰的命名 --&gt; &lt;setting name="mapUnderscoreToCamelCase" value="true"/&gt; &lt;/settings&gt; &lt;!-- 和spring整合后 environments配置将废除 --&gt; &lt;environments default="development"&gt; &lt;environment id="development"&gt; &lt;!-- 使用jdbc事务管理 --&gt; &lt;transactionManager type="JDBC"/&gt; &lt;!-- 数据库连接池 --&gt; &lt;dataSource type="POOLED"&gt; &lt;property name="driver" value="com.mysql.jdbc.Driver"/&gt; &lt;property name="url" value="jdbc:mysql://localhost:3306/test?useSSL=true"/&gt; &lt;property name="username" value="root"/&gt; &lt;property name="password" value="123456"/&gt; &lt;/dataSource&gt; &lt;/environment&gt; &lt;/environments&gt; &lt;mappers&gt; &lt;!-- 声明SQL映射文件所在位置 --&gt; &lt;mapper resource="demo/mybatis/mapper/Person.xml"/&gt; &lt;/mappers&gt;&lt;/configuration&gt; 1.7. 测试方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class MabatisTest &#123; SqlSessionFactory sqlSessionFactory = null; @Before public void setup() throws IOException &#123; // 根据全局配置文件创建SqlSessionFactory，SqlSessionFactory负责创建SqlSession InputStream in = Resources.getResourceAsStream("sqlMapConfig.xml"); sqlSessionFactory = new SqlSessionFactoryBuilder().build(in); &#125; @Test public void findById() throws IOException &#123; // 创建SqlSession。SqlSession代表与数据库之间的一次会话，底层就是封装了JDBC的Connection SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; // 获取Mapper，执行相应的操作 PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); Person person = mapper.findById(1L); System.out.println(person); &#125; finally &#123; sqlSession.close(); &#125; &#125; @Test public void insert() throws IOException &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); for (int i = 0; i &lt; 10; i++) &#123; Person person = new Person(null, "name" + i, new Date()); int result = mapper.insert(person); System.out.println("result = " + result); &#125; sqlSession.commit(); &#125; finally &#123; sqlSession.close(); &#125; &#125; @Test public void deleteById() throws IOException &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); int result = mapper.deleteById(1L); System.out.println("result = " + result); // 增删改必须commit sqlSession.commit(); &#125; finally &#123; sqlSession.close(); &#125; &#125; @Test public void updateById() throws IOException &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); Person person = new Person(5L, "Jack", new Date()); mapper.updateById(person); sqlSession.commit(); &#125; finally &#123; sqlSession.close(); &#125; &#125;&#125; 2. 全局配置文件和SQL映射文件全局配置文件：MyBatis的全局配置SQL映射文件：Mapper接口的实现描述 3. 查看Mapper接口的实现类查看Mapper接口的实现类，可以看到是动态代理 12345678910@Testpublic void test() &#123; SqlSession sqlSession = sqlSessionFactory.openSession(); try &#123; PersonMapper mapper = sqlSession.getMapper(PersonMapper.class); System.out.println(mapper.getClass()); // com.sun.proxy.$Proxy &#125; finally &#123; sqlSession.close(); &#125;&#125;]]></content>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 生命周期]]></title>
    <url>%2F2019%2F04%2F08%2FMaven%2FMaven-%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[1. Maven生命周期Maven有三个生命周期：clean生命周期、default生命周期、site生命周期 生命周期可以理解为项目构建的步骤集合。 生命周期是由多个阶段（Phase）组成。每个阶段都是一个完整的功能，比如mvn clean中的clean就是一个阶段。 在maven中，只要在同一个生命周期，你执行后面的阶段，那么前面的阶段也会被执行，而且不需要额外去输入前面的阶段，这样大大减轻了程序员的工作。 1.1. Clean生命周期123pre-clean 执行一些需要在clean之前完成的工作 clean 移除所有上一次构建生成的文件 post-clean 执行一些需要在clean之后立刻完成的工作 1.2. Default生命周期12345678910111213141516171819202122validate generate-sources process-sources generate-resources process-resources 复制并处理资源文件，至目标目录，准备打包。 compile 编译项目的源代码。 process-classes generate-test-sources process-test-sources generate-test-resources process-test-resources 复制并处理资源文件，至目标测试目录。 test-compile 编译测试源代码。 process-test-classes test 使用合适的单元测试框架运行测试。这些测试代码不会被打包或部署。 prepare-package package 接受编译好的代码，打包成可发布的格式，如 JAR 。 pre-integration-test integration-test post-integration-test verify install 将包安装至本地仓库，以让其它项目依赖。 deploy 将最终的包复制到远程的仓库，以让其它开发人员与项目共享。 1.3. Site生命周期1234pre-site 执行一些需要在生成站点文档之前完成的工作 site 生成项目的站点文档 post-site 执行一些需要在生成站点文档之后完成的工作，并且为部署做准备 site-deploy 将生成的站点文档部署到特定的服务器上 2. 阶段、插件和目标生命周期的各个阶段仅仅定义了要执行的任务是什么 各个阶段和插件的目标是对应的 相似的目标由特定的插件来完成 Phase plugin:goal process resources:resources compile compiler:compile process resources:testResources test compiler:testCompile test surefire:test package jar:jar install install:install deploy deploy:deploy]]></content>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA 插件]]></title>
    <url>%2F2019%2F04%2F08%2FSoftware%2FIDEA%2FIDEA-%E6%8F%92%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[整理一下能提高生产力的IDEA插件，以便以后重装IDEA时，能快速配置好环境。插件下载可以直接到IDEA插件官网，输入对应名称，下载离线包，再打开IDEA进行离线安装即可 Lombok Alibaba Java Coding Guidelines（p3c） Maven Helper MyBatisCodeHelperPro 插件是收费的（可以找Crack版本的），Crack之后，第1次操作要求输入密钥，随便输一个就通过了，之后就不用再输入了]]></content>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 依赖范围]]></title>
    <url>%2F2019%2F04%2F08%2FMaven%2FMaven-%E4%BE%9D%E8%B5%96%E8%8C%83%E5%9B%B4%2F</url>
    <content type="text"><![CDATA[依赖范围（scope） 对于主代码的有效性 对于测试代码的有效性 是否打包（对于运行时classpath的有效性） 例子 compile Y Y Y log4j test N Y N junit provided Y Y N servlet-api runtime N N Y JDBC Driver Implementation Maven因为执行一系列编译、测试和部署运行等操作，在不同的操作下使用的classpath不同，依赖范围就是用来控制依赖与三种 classpath（编译classpath、测试classpath、运行classpath）的关系 Maven有以下几种依赖范围： compile：编译依赖范围（默认），使用此依赖范围对于编译、测试、运行三种 classpath 都有效，即在编译、测试和运行的时候都要使用该依赖jar包； test：测试依赖范围，从字面意思就可以知道此依赖范围只能用于测试classpath，而在编译和运行项目时无法使用此类依赖，典型的是JUnit，它只用于编译测试代码和运行测试代码的时候才需要； provided：此依赖范围，对于编译和测试classpath有效，而对运行时无效； runtime：运行时依赖范围，对于测试和运行classpath有效，但是在编译主代码时无效，典型的就是JDBC驱动实现； system：系统依赖范围，使用system范围的依赖时必须通过systemPath元素显示地指定依赖文件的路径，不依赖Maven仓库解析，所以可能会造成建构的不可移植，谨慎使用]]></content>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA 代码模板]]></title>
    <url>%2F2019%2F04%2F08%2FSoftware%2FIDEA%2FIDEA-%E4%BB%A3%E7%A0%81%E6%A8%A1%E6%9D%BF%2F</url>
    <content type="text"><![CDATA[psvm sout xxx.sout foriiterxxx.for]]></content>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA 快捷键]]></title>
    <url>%2F2019%2F04%2F08%2FSoftware%2FIDEA%2FIDEA-%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[1. 常用操作行内换行 SHIFT+回车 单行注释 CTRL+/多行注释 CTRL+SHIFT+/ 复制行 CTRL+D删除行 CTRL+Y 上下移动行 ALT+SHIFT+向上箭头/向下箭头 智能提示 ALT+ENTER 切换Tab ALT+向左箭头/向右箭头关闭当前Tab CTRL+F4 格式化代码 CTRL+ALT+L 方法参数提示 CTRL+P 查看类的所有成员 CTRL+F12]]></content>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA 常用设置]]></title>
    <url>%2F2019%2F04%2F08%2FSoftware%2FIDEA%2FIDEA-%E5%B8%B8%E7%94%A8%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1. Editor1.1. 设置CTRL+鼠标滚轮设置字体大小Editor-&gt;General 勾选 Change font size (Zoom) with Ctrl+Mouse Wheel 1.2. 设置鼠标悬浮提示Editor-&gt;General 勾选 Show quick documentation on mouse move 1.3. 设置自动导包Editor-&gt;General-&gt;Auto Import Insert imports on paste：修改为All（默认是Ask） 勾选Add unambiguous imports on the fly 勾选Optimize imports on the fly 1.4. 代码提示忽略大小写Editor-&gt;General 取消勾选Match case]]></content>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA Project与Module]]></title>
    <url>%2F2019%2F04%2F08%2FSoftware%2FIDEA%2FIDEA-Project%E4%B8%8EModule%2F</url>
    <content type="text"><![CDATA[Project类似Eclipse中的Workspace，Module类似Eclipse中的Project 1. 创建ProjectFile-&gt;New Project 2. 创建ModuleProject可以创建Module，Module也可以创建子Module，操作都是一样的 对应工程或者Module右键-&gt;New Module 3. 删除Module打开Module结构界面。有以下两种方式 方式1：File-&gt;Project Structures-&gt;选中侧边Modules-&gt; 方式2：对应Module右键-&gt;Open Module Settings 打开后，选中对应的Module，点击上方的减号，将当前Module从Project中去除 Module去除之后，会看到Module还在磁盘中。如果想继续从磁盘中删除，可以在对应Module右键-&gt;Delete。需要注意的是，在Module被去除之前，你是找不到Delete的，这是IDEA的保护机制，防止你误删Module。只有将Module从工程结构中去除之后，才有Delete操作]]></content>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA JVM运行参数设置]]></title>
    <url>%2F2019%2F04%2F08%2FSoftware%2FIDEA%2FIDEA-JVM%E8%BF%90%E8%A1%8C%E5%8F%82%E6%95%B0%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[JVM运行参数配置文件： Win32：bin/idea.exe.vmoptions Win64：bin/idea64.exe.vmoptions 参数说明 -Xms 初始堆大小。调大一些，可以加快IDEA的启动速度 -Xmx 最大堆大小。调大一些，可以降低JVM垃圾回收的频率 -XX:ReservedCodeCacheSize：用来存储已编译方法生成的本地代码 123456789101112-Xms1024m-Xmx2048m-XX:ReservedCodeCacheSize=500m-XX:+UseConcMarkSweepGC-XX:SoftRefLRUPolicyMSPerMB=50-ea-Dsun.io.useCanonCaches=false-Djava.net.preferIPv4Stack=true-Djdk.http.auth.tunneling.disabledSchemes=&quot;&quot;-XX:+HeapDumpOnOutOfMemoryError-XX:-OmitStackTraceInFastThrow-javaagent:C:\software\ide\IntelliJ IDEA 2018.2.4\bin\JetbrainsCrack.jar]]></content>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA 概述]]></title>
    <url>%2F2019%2F04%2F07%2FSoftware%2FIDEA%2FIDEA-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[参考： 官方IDEA使用教程 https://www.jetbrains.com/help/idea/installation-guide.html IntelliJ IDEA 从入门到上瘾教程，2019图文版！https://mp.weixin.qq.com/s/TEdY2kKatmWRhVmpgKasdA 1. IDEA介绍IDEA，全称IntelliJ IDEA，是java语言的集成开发环境，IDEA在业界被公认为是最好的java开发工具之一，尤其在智能代码助手、代码自动提示、重构、J2EE支持、Ant、JUnit、CVS整合、代码审查、创新的GUI设计等方面的功能可以说是超常的。 IntelliJ IDEA在2015年的官网上这样介绍自己： Excel at enterprise, mobile and web development with Java, Scala and Groovy, with all the latest modern technologies and frameworks available out of the box. 简明翻译：IntelliJ IDEA主要用于支持Java、Scala、Groovy等语言的开发工具，同时具备支持目前主流的技术和框架，擅长于企业应用、移动应用和Web应用的开发。 2. IDEA版本说明IDEA的版本命名原则：年份.第N个版本 ，前面的数字代表发布的年份，但是后面的数字不是代表发布的月份，而是该年发布的第N个版本。例如： 2019.1：2019年的第1个版本 2018.2：2018年的第2个版本 2018.3：2018年的第3个版本 3. IDEA软件环境要求IDEA自带JRE。如果你只是想运行Java程序，则可以直接在IDEA上运行。如果想进行Java开发，你应该要单独安装JDK 4. IDEA和Eclipse术语上的不同 IDEA Eclipse Project Workspace Module Project Facet Facet Library Library JRE JRE Classpath variable Path variable 4.1. IDAE为什么要取消工作空间？IDEA没有Workspace的概念，可以把IDEA的Project近似看作Eclipse的Workspace，实际上还是有区别的。 简单来说，IDEA不需要设置工作空间，因为每一个Project都具备一个工作空间！！对于每一个IDEA的项目工程（Project）而言，它的每一个子模块（Module）都可以使用独立的JDK和MAVEN配置。这对于传统项目迈向新项目的重构添加了极大的便利性，这种多元化的灵活性正是Eclipse所缺失的，因为开始Eclipse在初次使用时已经绑死了工作空间。 4.2. 为什么IDEA里面的子工程要称为Module？其实就是模块化的概念，作为聚合工程亦或普通的根目录，它称之为Project，而下面的子工程称为模块，每一个子模块之间可以相关联，也可以没有任何关联。 5. IDEA相较于Eclipse的优势 强大的整合能力。IDEA和Eclipse都集成了Git、Maven等功能。IDEA还集成了Spring，Eclipse必须安装STS插件才支持 提示功能快速便捷。IDEA只要输入单个字符，就有提示功能，而在Eclipse中必须 ALT+/ （其实通过偏好设置也可以实现）。IDEA提示比较流畅，Eclipse的提示给人一种卡顿的感觉]]></content>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow 新闻文本分类]]></title>
    <url>%2F2019%2F04%2F07%2FDeepLearning%2FTensorFlow%2FTensorflow-%E6%96%B0%E9%97%BB%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[1. 数据预处理12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273import jiebadef generate_seg_file(input_file, output_seg_file): """Segment the sentences in each line in input_file""" with open(input_file, 'r', encoding='utf-8') as f: lines = f.readlines() with open(output_seg_file, 'w', encoding='utf-8') as f: for line in lines: label, content = line.strip('\r\n').split('\t') word_content = '' for word in jieba.cut(content): word = word.strip(' ') if word != '': word_content += word + ' ' out_line = '%s\t%s\n' % (label, word_content.strip(' ')) f.write(out_line)def generate_vocab_file(input_seg_file, output_vocab_file): with open(input_seg_file, 'r', encoding="utf-8") as f: lines = f.readlines() word_dict = &#123;&#125; for line in lines: label, content = line.strip('\r\n').split('\t') for word in content.split(): word_dict.setdefault(word, 0) word_dict[word] += 1 # [(word, frequency), ..., ()] sorted_word_dict = sorted( word_dict.items(), key = lambda d:d[1], reverse=True) with open(output_vocab_file, 'w', encoding="utf-8") as f: f.write('&lt;UNK&gt;\t10000000\n') for item in sorted_word_dict: f.write('%s\t%d\n' % (item[0], item[1]))def generate_category_dict(input_file, category_file): with open(input_file, 'r', encoding='utf-8') as f: lines = f.readlines() category_dict = &#123;&#125; for line in lines: label, content = line.strip('\r\n').split('\t') category_dict.setdefault(label, 0) category_dict[label] += 1 category_number = len(category_dict) with open(category_file, 'w', encoding='utf-8') as f: for category in category_dict: line = '%s\n' % category print('%s\t%d' % ( category, category_dict[category])) f.write(line)if __name__ == '__main__': # input files train_file = 'data/cnews/text_classification_data/cnews.train.txt' val_file = 'data/cnews/text_classification_data/cnews.val.txt' test_file = 'data/cnews/text_classification_data/cnews.test.txt' # output files seg_train_file = 'data/cnews/text_classification_data/cnews.train.seg.txt' seg_val_file = 'data/cnews/text_classification_data/cnews.val.seg.txt' seg_test_file = 'data/cnews/text_classification_data/cnews.test.seg.txt' vocab_file = 'data/cnews/text_classification_data/cnews.vocab.txt' category_file = 'data/cnews/text_classification_data/cnews.category.txt' # generate target file generate_seg_file(train_file, seg_train_file) generate_seg_file(val_file, seg_val_file) generate_seg_file(test_file, seg_test_file) generate_vocab_file(seg_train_file, vocab_file) generate_category_dict(train_file, category_file) 2. RNN分类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333import osimport numpy as npimport tensorflow as tftf.logging.set_verbosity(tf.logging.INFO)def get_default_params(): return tf.contrib.training.HParams( num_embedding_size=16, # 词语对应的向量长度 num_timesteps=50, # LSTM步长 num_lstm_nodes=[32, 32], # 每一层的节点数 num_lstm_layers=2, # 层数 num_fc_nodes=32, # 全连接层节点数 batch_size=100, clip_lstm_grads=1.0, # 控制LSTM梯度大小。LSTM的梯度容易爆炸或消失 learning_rate=0.001, num_word_threshold=10 # 词频阈值，低于该值的词直接忽略，因为出现过少的词对模型训练是没有 )hps = get_default_params()train_file = 'data/cnews/cnews.train.seg.txt'val_file = 'data/cnews/cnews.val.seg.txt'test_file = 'data/cnews/cnews.test.seg.txt'vocab_file = 'data/cnews/cnews.vocab.txt'category_file = 'data/cnews/cnews.category.txt'output_folder = 'data/cnews/run_text_rnn'if not os.path.exists(output_folder): os.mkdir(output_folder)# ------------------------class Vocab: def __init__(self, filename, num_word_threshold): self._word_to_id = &#123;&#125; self._unk = -1 self._num_word_threshold = num_word_threshold self._read_dict(filename) def _read_dict(self, filename): """ 读取文件每一行，赋值一个id :param filename: :return: """ with open(filename, 'r', encoding='UTF-8') as f: lines = f.readlines() for line in lines: word, frequency = line.strip('\r\n').split('\t') frequency = int(frequency) if frequency &lt; self._num_word_threshold: continue idx = len(self._word_to_id) if word == '&lt;UNK&gt;': self._unk = idx self._word_to_id[word] = idx def word_to_id(self, word): return self._word_to_id.get(word, self._unk) @property def unk(self): return self._unk def size(self): return len(self._word_to_id) def sentence_to_id(self, sentence): word_ids = [self.word_to_id(cur_word) \ for cur_word in sentence.split()] return word_idsclass CategeoryDict: def __init__(self, filename): self._categeory_to_id = &#123;&#125; with open(filename, 'r', encoding='UTF-8') as f: lines = f.readlines() for line in lines: categeory = line.strip('\r\n') idx = len(self._categeory_to_id) self._categeory_to_id[categeory] = idx def categeory_to_id(self, categeory): if not categeory in self._categeory_to_id: raise Exception( "%s is not in our categeory list" % categeory) return self._categeory_to_id[categeory] def size(self): return len(self._categeory_to_id)class TextDataSet: """ 数据集封装模块 """ def __init__(self, filename, vocab, categeory_vocab, num_timesteps): self._vocab = vocab self._categeory_vocab = categeory_vocab self._num_timesteps = num_timesteps # matrix self._inputs = [] # vector self._outputs = [] self._indicator = 0 self._parse_file(filename) def _parse_file(self, filename): tf.logging.info('Loading data from %s', filename) with open(filename, 'r', encoding='UTF-8') as f: lines = f.readlines() category_dict = &#123;&#125; for line in lines: label, content = line.strip('\r\n').split('\t') id_label = self._categeory_vocab.categeory_to_id(label) id_words = self._vocab.sentence_to_id(content) id_words = id_words[0:self._num_timesteps] padding_num = self._num_timesteps - len(id_words) id_words = id_words + [self._vocab.unk for i in range(padding_num)] self._inputs.append(id_words) self._outputs.append(id_label) # 转换为numpy array self._inputs = np.asarray(self._inputs, dtype=np.int32) self._outputs = np.asarray(self._outputs, dtype=np.int32) self._random_shuffle() def _random_shuffle(self): p = np.random.permutation(len(self._inputs)) self._inputs = self._inputs[p] self._outputs = self._outputs[p] def next_batch(self, batch_size): end_indicator = self._indicator + batch_size if end_indicator &gt; len(self._inputs): self._random_shuffle() self._indicator = 0 end_indicator = batch_size if end_indicator &gt; len(self._inputs): raise Exception("batch_size: %d is too large" % batch_size) batch_inputs = self._inputs[self._indicator:end_indicator] batch_outputs = self._outputs[self._indicator:end_indicator] self._indicator = end_indicator return batch_inputs, batch_outputsdef create_model(hps, vocab_size, num_classes): num_timesteps = hps.num_timesteps batch_size = hps.batch_size inputs = tf.placeholder(tf.int32, (batch_size, num_timesteps)) outputs = tf.placeholder(tf.int32, (batch_size,)) # 随机失活剩下的神经单元 keep_prob = 1-dropout keep_prob = tf.placeholder(tf.float32, name='keep_prob') # save training_step global_step = tf.Variable(tf.zeros([], tf.int64), name='global_step', trainable=False) '''embedding层搭建''' # 定义初始化函数[-1,1] embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0) with tf.variable_scope('embedding', initializer=embedding_initializer): embeddings = tf.get_variable('embedding', [vocab_size, hps.num_embedding_size], tf.float32) embed_inputs = tf.nn.embedding_lookup(embeddings, inputs) # change inputs to embedding '''LSTM层搭建''' scale = (1.0 / np.sqrt(hps.num_embedding_size + hps.num_lstm_nodes[-1])) * 3.0 lstm_init = tf.random_uniform_initializer(-scale, scale) def _generate_params_for_lstm_cell(x_size, h_size, bias_size): x_w = tf.get_variable('x_weights', x_size) h_w = tf.get_variable('h_weights', h_size) b = tf.get_variable('biases', bias_size, initializer=tf.constant_initializer(0.0)) return x_w, h_w, b with tf.variable_scope('lstm_nn', initializer=lstm_init): with tf.variable_scope('inputs'): ix, ih, ib = _generate_params_for_lstm_cell( x_size=[hps.num_embedding_size, hps.num_lstm_nodes[0]], h_size=[hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]], bias_size=[1, hps.num_lstm_nodes[0]] ) with tf.variable_scope('outputs'): ox, oh, ob = _generate_params_for_lstm_cell( x_size=[hps.num_embedding_size, hps.num_lstm_nodes[0]], h_size=[hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]], bias_size=[1, hps.num_lstm_nodes[0]] ) with tf.variable_scope('forget'): fx, fh, fb = _generate_params_for_lstm_cell( x_size=[hps.num_embedding_size, hps.num_lstm_nodes[0]], h_size=[hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]], bias_size=[1, hps.num_lstm_nodes[0]] ) with tf.variable_scope('memory'): cx, ch, cb = _generate_params_for_lstm_cell( x_size=[hps.num_embedding_size, hps.num_lstm_nodes[0]], h_size=[hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]], bias_size=[1, hps.num_lstm_nodes[0]] ) state = tf.Variable(tf.zeros([batch_size, hps.num_lstm_nodes[0]]), trainable=False) h = tf.Variable(tf.zeros([batch_size, hps.num_lstm_nodes[0]]), trainable=False) for i in range(num_timesteps): embed_input = embed_inputs[:, i, :] embed_input = tf.reshape(embed_input, [batch_size, hps.num_embedding_size]) forget_gate = tf.sigmoid(tf.matmul(embed_input, fx) + tf.matmul(h, fh) + fb) input_gate = tf.sigmoid(tf.matmul(embed_input, ix) + tf.matmul(h, ih) + ib) output_gate = tf.sigmoid(tf.matmul(embed_input, ox) + tf.matmul(h, oh) + ob) mid_state = tf.tanh(tf.matmul(embed_input, cx) + tf.matmul(h, ch) + cb) state = mid_state * input_gate + state * forget_gate h = output_gate * tf.tanh(state) last = h ''' cells = [] for i in range(hps.num_lstm_layers): cell = tf.contrib.rnn.BasicLSTMCell(hps.num_lstm_nodes[i], state_is_tuple = True) cell = tf.contrib.rnn.DropoutWrapper(cell,output_keep_prob=keep_prob) cells.append(cell) cell = tf.contrib.rnn.MultiRNNCell(cells)#Cell is 多层LSTM initial_state = cell.zero_state(batch_size,tf.float32) #初始化隐藏状态为0 #RNNoutput: # 一维：batch_size # 二维：num_timesteps # 三维：lstm_outputs[-1] rnn_outputs, _ = tf.nn.dynamic_rnn(cell,embed_inputs,initial_state=initial_state) last = rnn_outputs[:, -1, : ] ''' '''FC层搭建''' fc_init = tf.uniform_unit_scaling_initializer(factor=1.0) with tf.variable_scope('fc', initializer=fc_init): fc1 = tf.layers.dense(last, hps.num_fc_nodes, activation=tf.nn.relu, name='fc1') fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob) logits = tf.layers.dense(fc1_dropout, num_classes, name='fc2') '''计算损失函数''' with tf.name_scope('metrics'): softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=outputs) loss = tf.reduce_mean(softmax_loss) y_pred = tf.argmax(tf.nn.softmax(logits), 1, output_type=tf.int32) correct_pred = tf.equal(outputs, y_pred) accuary = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) '''计算图构建''' with tf.name_scope('train_op'): tvars = tf.trainable_variables() # 获得所有训练变量 for var in tvars: tf.logging.info('variable name: %s' % (var.name)) grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), hps.clip_lstm_grads) # 梯度截断 optimizer = tf.train.AdamOptimizer(hps.learning_rate) train_op = optimizer.apply_gradients(zip(grads, tvars), global_step=global_step) return ((inputs, outputs, keep_prob), (loss, accuary), (train_op, global_step))def run_model(): # --------------------- vocab = Vocab(vocab_file, hps.num_word_threshold) vocab_size = vocab.size() categeory_vocab = CategeoryDict(category_file) num_classes = categeory_vocab.size() train_dataset = TextDataSet(train_file, vocab, categeory_vocab, hps.num_timesteps) val_dataset = TextDataSet(val_file, vocab, categeory_vocab, hps.num_timesteps) test_dataset = TextDataSet(test_file, vocab, categeory_vocab, hps.num_timesteps) # ---------------------- placeholders, metrics, others = create_model(hps, vocab_size, num_classes) inputs, outputs, keep_prob = placeholders loss, accuary = metrics train_op, global_step = others init_op = tf.global_variables_initializer() train_keep_prob_value = 0.8 test_keep_prob_value = 1.0 num_train_steps = 10000 with tf.Session() as sess: sess.run(init_op) for i in range(num_train_steps): batch_inputs, batch_labels = train_dataset.next_batch( hps.batch_size) outputs_val = sess.run([loss, accuary, train_op, global_step], feed_dict=&#123;inputs: batch_inputs, outputs: batch_labels, keep_prob: train_keep_prob_value&#125;) loss_val, accuary_val, _, global_step_val = outputs_val if global_step_val % 20 == 0: tf.logging.info("Step: %5d, loss: %3.3f, accuary: %3.3f" % (global_step_val, loss_val, accuary_val))def tmp(): # --------------------- vocab = Vocab(vocab_file, hps.num_word_threshold) vocab_size = vocab.size() print('vocab_size: %d' % vocab_size) test_str = '的 在 了 是' print(vocab.sentence_to_id(test_str)) # ---------------- print("-" * 100) categeory_vocab = CategeoryDict(category_file) test_str = '时尚' num_classes = categeory_vocab.size() print('label: %s,id: %d' % (test_str, categeory_vocab.categeory_to_id(test_str))) print('num_classes: %d' % num_classes) # ---------------- print("-" * 100) train_dataset = TextDataSet(train_file, vocab, categeory_vocab, hps.num_timesteps) val_dataset = TextDataSet(val_file, vocab, categeory_vocab, hps.num_timesteps) test_dataset = TextDataSet(test_file, vocab, categeory_vocab, hps.num_timesteps) print(train_dataset.next_batch(2)) print(val_dataset.next_batch(2)) print(test_dataset.next_batch(2))if __name__ == '__main__': run_model()]]></content>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow CIFAR10 图像分类]]></title>
    <url>%2F2019%2F04%2F07%2FDeepLearning%2FTensorFlow%2FTensorflow-CIFAR10-%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[1. CIFAR-10数据集1.1. 下载进入 http://www.cs.toronto.edu/~kriz/cifar.html ，下载 CIFAR-10 python 版本 解压后，得到以下文件。data_batch_xxx是图片数据，都是二进制文件。12345678batches.metadata_batch_1data_batch_2data_batch_3data_batch_4data_batch_5readme.htmltest_batch 1.2. 查看数据123456789101112131415161718import picklewith open("./data/cifar/data_batch_1", "rb") as f: data = pickle.load(f, encoding="bytes") print(type(data)) # &lt;class 'dict'&gt; print(data.keys()) # dict_keys([b'batch_label', b'labels', b'data', b'filenames']) print(type(data[b'batch_label'])) # &lt;class 'bytes'&gt; print(type(data[b'labels'])) # &lt;class 'list'&gt; print(type(data[b'data'])) # &lt;class 'numpy.ndarray'&gt; print(type(data[b'filenames'])) # &lt;class 'list'&gt; print(data[b'data'].shape) # (10000, 3072) 3072 = 32x32x3（长宽32x32，RGB三通道） print(data[b'data'].dtype) # uint8 print(data[b'data'][0]) # 值范围[0, 255] print(data[b'labels']) # 图像类别标签，范围0~9，总10类 print(data[b'filenames']) # 10000张图片对应的文件名 1.3. 显示图片123456789101112import matplotlib.pyplot as pltimport picklewith open("./data/cifar/data_batch_1", "rb") as f: data = pickle.load(f, encoding="bytes") images = data[b'data'][100] # 取其中1张图 images = images.reshape((3, 32, 32)) images = images.transpose((1, 2, 0)) # 轴(0,1,2) ==&gt; (1,2,0)，形状3x32x32转为32x32x3 plt.imshow(images) # 加载图片 plt.show() # 显示 2. 二分类logistic回归模型1个隐藏层，且只有1个神经元 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150import pickleimport numpy as npimport tensorflow as tfimport osdef load_data(filename): """ 读取CIFAR-10数据，返回特征和标签 """ with open(filename, 'rb') as f: data = pickle.load(f, encoding='bytes') return data[b'data'], data[b'labels']class CifarData: def __init__(self, filenames, need_shuffle): """ 读取数据 :param filenames: 数据文件，可以有多个 :param need_shuffle: 是否需要将数据打乱 """ data_list = [] label_list = [] for filename in filenames: data, labels = load_data(filename) for item, label in zip(data, labels): # 单个神经元logistic只能处理二分类问题，所以这里只取label=0和label=1的样本 if label in [0, 1]: data_list.append(item) label_list.append(label) # 将list转为numpy self._data = np.vstack(data_list) # 对数据进行归一化, 尺度固定在 [-1, 1] 之间 self._data = self._data / 127.5 - 1 # 将list转为numpy self._labels = np.hstack(label_list) print(self._data.shape) print(self._labels.shape) # 记录当前的样本 数量 self._num_examples = self._data.shape[0] # 记录是否需要将样本打乱 self._need_shuffle = need_shuffle # 样本的起始点 self._indicator = 0 if self._need_shuffle: self._shuffle_data() def _shuffle_data(self): # 生成随机全排列 np.random.permutation(6) =&gt; [3, 0, 4, 1, 5, 2] p = np.random.permutation(self._num_examples) # 打乱数据 self._data = self._data[p] self._labels = self._labels[p] def next_batch(self, batch_size): """return batch_size examples as a batch.""" end_indicator = self._indicator + batch_size # 如果结束点大于样本数量 if end_indicator &gt; self._num_examples: if self._need_shuffle: # 重新打乱数据 self._shuffle_data() # 从头开始 self._indicator = 0 end_indicator = batch_size else: raise Exception("have no more examples") # 再次查看是否 超出边界了 if end_indicator &gt; self._num_examples: raise Exception("batch size is larger than all examples") # 把 batch 区间 的data和label保存, 并最后return batch_data = self._data[self._indicator: end_indicator] batch_labels = self._labels[self._indicator: end_indicator] self._indicator = end_indicator return batch_data, batch_labelsdef logistic(): x = tf.placeholder(tf.float32, [None, 3072]) # [None] y = tf.placeholder(tf.int64, [None]) # (3072, 1) w = tf.get_variable('w', [x.get_shape()[-1], 1], initializer=tf.random_normal_initializer(0, 1)) # (1, ) b = tf.get_variable('b', [1], initializer=tf.constant_initializer(0.0)) # [None, 3072] * [3072, 1] = [None, 1] y_ = tf.matmul(x, w) + b # [None, 1] p_y_1 = tf.nn.sigmoid(y_) # [None, 1] y_reshaped = tf.reshape(y, (-1, 1)) y_reshaped_float = tf.cast(y_reshaped, tf.float32) loss = tf.reduce_mean(tf.square(y_reshaped_float - p_y_1)) # bool predict = p_y_1 &gt; 0.5 # [1,0,1,1,1,0,0,0] correct_prediction = tf.equal(tf.cast(predict, tf.int64), y_reshaped) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64)) with tf.name_scope('train_op'): train_op = tf.train.AdamOptimizer(1e-3).minimize(loss) init = tf.global_variables_initializer() batch_size = 20 train_steps = 30000 test_steps = 100 with tf.Session() as sess: sess.run(init) for i in range(train_steps): batch_data, batch_labels = train_data.next_batch(batch_size) loss_val, acc_val, _ = sess.run( [loss, accuracy, train_op], feed_dict=&#123; x: batch_data, y: batch_labels&#125;) if (i + 1) % 500 == 0: print('[Train] Step: %d, loss: %4.5f, acc: %4.5f' % (i + 1, loss_val, acc_val)) if (i + 1) % 5000 == 0: test_data = CifarData(test_filenames, False) all_test_acc_val = [] for j in range(test_steps): test_batch_data, test_batch_labels \ = test_data.next_batch(batch_size) test_acc_val = sess.run( [accuracy], feed_dict=&#123; x: test_batch_data, y: test_batch_labels &#125;) all_test_acc_val.append(test_acc_val) test_acc = np.mean(all_test_acc_val) print('[Test ] Step: %d, acc: %4.5f' % (i + 1, test_acc))if __name__ == '__main__': # 获取所有的训练文件名和测试文件名 CIFAR_DIR = "./data/cifar" train_filenames = [os.path.join(CIFAR_DIR, 'data_batch_%d' % i) for i in range(1, 6)] test_filenames = [os.path.join(CIFAR_DIR, 'test_batch')] # 训练数据和测试数据的CifarData对象 train_data = CifarData(train_filenames, True) test_data = CifarData(test_filenames, False) logistic() 3. 多分类logistic回归模型一个隐藏层，多个神经元 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153import pickleimport numpy as npimport tensorflow as tfimport osdef load_data(filename): """ 读取CIFAR-10数据，返回特征和标签 """ with open(filename, 'rb') as f: data = pickle.load(f, encoding='bytes') return data[b'data'], data[b'labels']class CifarData: def __init__(self, filenames, need_shuffle): """ 读取数据 :param filenames: 数据文件，可以有多个 :param need_shuffle: 是否需要将数据打乱 """ data_list = [] label_list = [] for filename in filenames: data, labels = load_data(filename) for item, label in zip(data, labels): # 读取所有数据 data_list.append(item) label_list.append(label) self._data = np.vstack(data_list) self._data = self._data / 127.5 - 1 self._labels = np.hstack(label_list) print(self._data.shape) print(self._labels.shape) self._num_examples = self._data.shape[0] self._need_shuffle = need_shuffle self._indicator = 0 if self._need_shuffle: self._shuffle_data() def _shuffle_data(self): p = np.random.permutation(self._num_examples) self._data = self._data[p] self._labels = self._labels[p] def next_batch(self, batch_size): """return batch_size examples as a batch.""" end_indicator = self._indicator + batch_size if end_indicator &gt; self._num_examples: if self._need_shuffle: self._shuffle_data() self._indicator = 0 end_indicator = batch_size else: raise Exception("have no more examples") if end_indicator &gt; self._num_examples: raise Exception("batch size is larger than all examples") batch_data = self._data[self._indicator: end_indicator] batch_labels = self._labels[self._indicator: end_indicator] self._indicator = end_indicator return batch_data, batch_labelsdef logistic(): x = tf.placeholder(tf.float32, [None, 3072]) y = tf.placeholder(tf.int64, [None]) # w形状修改为 [3072, 10] w = tf.get_variable('w', [x.get_shape()[-1], 10], initializer=tf.random_normal_initializer(0, 1)) # b形状修改为 [10] b = tf.get_variable('b', [10], initializer=tf.constant_initializer(0.0)) y_ = tf.matmul(x, w) + b ## p_y_1 = tf.nn.sigmoid(y_) p_y = tf.nn.softmax(y_) """ y_reshaped = tf.reshape(y, (-1, 1)) y_reshaped_float = tf.cast(y_reshaped, tf.float32) loss = tf.reduce_mean(tf.square(y_reshaped_float - p_y_1)) """ y_one_hot = tf.one_hot(y, 10, dtype=tf.float32) loss = tf.reduce_mean(tf.square(y_one_hot - p_y)) ## predict = p_y_1 &gt; 0.5 predict = tf.argmax(y_, 1) ## correct_prediction = tf.equal(tf.cast(predict, tf.int64), y_reshaped) correct_prediction = tf.equal(predict, y) accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float64)) with tf.name_scope('train_op'): train_op = tf.train.AdamOptimizer(1e-3).minimize(loss) init = tf.global_variables_initializer() batch_size = 20 train_steps = 30000 test_steps = 100 with tf.Session() as sess: sess.run(init) for i in range(train_steps): batch_data, batch_labels = train_data.next_batch(batch_size) loss_val, acc_val, _ = sess.run( [loss, accuracy, train_op], feed_dict=&#123; x: batch_data, y: batch_labels&#125;) if (i + 1) % 500 == 0: print('[Train] Step: %d, loss: %4.5f, acc: %4.5f' % (i + 1, loss_val, acc_val)) if (i + 1) % 5000 == 0: test_data = CifarData(test_filenames, False) all_test_acc_val = [] for j in range(test_steps): test_batch_data, test_batch_labels \ = test_data.next_batch(batch_size) test_acc_val = sess.run( [accuracy], feed_dict=&#123; x: test_batch_data, y: test_batch_labels &#125;) all_test_acc_val.append(test_acc_val) test_acc = np.mean(all_test_acc_val) print('[Test ] Step: %d, acc: %4.5f' % (i + 1, test_acc))if __name__ == '__main__': CIFAR_DIR = "./data/cifar" train_filenames = [os.path.join(CIFAR_DIR, 'data_batch_%d' % i) for i in range(1, 6)] test_filenames = [os.path.join(CIFAR_DIR, 'test_batch')] train_data = CifarData(train_filenames, True) test_data = CifarData(test_filenames, False) logistic() 3.1. 使用交叉熵损失12345678910111213""" 平方差损失 p_y = tf.nn.softmax(y_) y_one_hot = tf.one_hot(y, 10, dtype=tf.float32) loss = tf.reduce_mean(tf.square(y_one_hot - p_y))""""""sparse_softmax_cross_entropy: 1、预测值y_计算 softmax(y_) 2、真实值y计算one_hot 3、计算ylog(y_)"""loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=y_)]]></content>
      <tags>
        <tag>Tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepLearning seq2seq]]></title>
    <url>%2F2019%2F04%2F06%2FDeepLearning%2FDeepLearning-seq2seq%2F</url>
    <content type="text"><![CDATA[1. Seq2Seq的应用1.1. 机器翻译机器翻译最早期是逐字翻译。缺点很明显，翻译很不流畅，读起来很别扭 后来人们尝试根据统计学和概率来做机器翻译。每个词都有对应翻译的候选值，各个词之间的候选值组合成一句话，选择其中一个概率最大的组合 现在主流的做法是基于循环网络和编码。这里的编码是指将用户输入的话（序列）转化成一个向量，经过一些网络结构，最后再将向量解码得到另一种自然语言 输入是序列，输出是序列，中间要经过编码和解码，这就是Seq2Seq 1.2. 文本摘要 1.3. 情感对话生成 2. Seq2Seq网络架构 在解码过程中，可以看到 &lt;END&gt; ，有时也写为 &lt;EOS&gt;（End Of Sentence） ，代表终止字符。在解码时，不能让数据无限制地解码下去，所以要设置一个终止字符 3. Seq2Seq存在的问题前面压缩损失了信息，解码时忽略了这一问题，会导致问题 输入的序列不能过长。长度在10~20之间是最合适的]]></content>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepLearning 循环神经网络]]></title>
    <url>%2F2019%2F04%2F06%2FDeepLearning%2FDeepLearning-%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[1. 序列式问题普通神经网络（Vanilla Neural Networks）：1个输入，经过网络结构，得到1个输出。全连接层神经网络、卷积神经网络都是这样的。适合解决数据输入输出格式都是固定的问题，如图像分类 如果是变长的数据呢？例如文本，它的长度不是固定的。这时候就需要循环神经网络。RNN可以解决1对多问题（1个输出，多个输出），例如给你一张图上，生成图片描述，描述是文本，是不定长的。 RNN还适合处理多对1问题，如文本分类（如文本情感分类） RNN也可以解决多对多问题，如机器翻译 2. 循环神经网络2.1. RNN的结构RNN与普通神经网络类似，但是多了一个自我指向的路径 维护一个状态作为下一步的额外输入 每一步使用同样的激活函数和参数 3. LSTM 长短期记忆网络因为普通RNN的信息不能长久传播，所以引入LSTM LSTM引入了选择性机制 选择性输出 选择性输入 选择性遗忘 选择性机制是通过门来现实的]]></content>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 验证码识别]]></title>
    <url>%2F2019%2F04%2F06%2FDeepLearning%2FTensorFlow%2FTensorFlow-%E9%AA%8C%E8%AF%81%E7%A0%81%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepLearning 卷积神经网络]]></title>
    <url>%2F2019%2F04%2F06%2FDeepLearning%2FDeepLearning-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%2F</url>
    <content type="text"><![CDATA[1. 普通神经网络在处理图像时遇到的问题图像的每个像素，都可以看作是特征值。1000x1000的图片，就有100万个特征值。如果把100万个特征值输入到全连接神经网络，参数过多就会导致计算量过大、内存爆炸 输入层神经元 [None, 10^6] 全连接层 [10^6, 10^6]，大小10^12 参数过多还容易导致过拟合。神经网络每记住训练集的所有参数，导致神经网络的表达能力过强（泛化能力过弱） 如何解决这个问题呢？想象一张图片，某个像素点与其周围像素点的关联较强。例如眼睛中的像素点与周围其它像素点的关联很大，因为它们共同代表了眼睛，但是和距离它较远的头发上的像素点关联并不大，因为它们代表不同的特征。 对于一个图片序列数据，特征值具有区域性，基于这一特点，我们可以将全连接改为局部连接，从而降低它的参数量 2. 卷积神经网络的发展历史 3. 卷积神经网络的结构分析 神经网络(neural networks)的基本组成包括输入层、隐藏层、输出层。而卷积神经网络的特点在于隐藏层分为卷积层和池化层(pooling layer，又叫下采样层) 卷积层：通过在原始图像上平移来提取特征，每一个特征就是一个特征映射 池化层：通过特征后稀疏参数来减少学习的参数，降低网络的复杂度，（最大池化和平均池化） 3.1. 卷积层过滤器]]></content>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 手写数字图片识别]]></title>
    <url>%2F2019%2F04%2F05%2FDeepLearning%2FTensorFlow%2FTensorFlow-%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E5%9B%BE%E7%89%87%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[1. 手写数字数据集1.1. 下载数据集下载地址：http://yann.lecun.com/exdb/mnist/ 下载这4个数据集，放到工程的 ./data/mnist/input_data/ 目录下，后面程序中要读取这些数据1234train-images-idx3-ubyte.gz: 训练集图片train-labels-idx1-ubyte.gz: 训练集标签t10k-images-idx3-ubyte.gz: 测试集图片t10k-labels-idx1-ubyte.gz: 测试集标签 TensorFlow也提供了相应下载并读取MNIST数据集的API 12345678910111213141516from tensorflow.examples.tutorials.mnist import input_data"""input_data.read_data_sets 第1个参数train_dir: 数据集所在目录 参数one_hot: 是否进行one_hot编码 input_data.read_data_sets首先看会查看train_dir目录下是否存在这4个数据集: TRAIN_IMAGES = 'train-images-idx3-ubyte.gz' TRAIN_LABELS = 'train-labels-idx1-ubyte.gz' TEST_IMAGES = 't10k-images-idx3-ubyte.gz' TEST_LABELS = 't10k-labels-idx1-ubyte.gz' 如果存在，则直接从本地读取；如果不存在，则会联网去下载。 由于网络原因，最好手动先去下载，放到train_dir目录下"""mnist = input_data.read_data_sets("./data/mnist/input_data/", one_hot=True) 1.2. 查看数据集文件是二进制格式的，具体数据格式在下载的地方有说明。TensorFlow已经提供了读取数据的API，直接用即可12from tensorflow.examples.tutorials.mnist import input_datamnist = input_data.read_data_sets("./data/mnist/input_data/", one_hot=True) mnist.train.images就是训练集图片。有55000张图片，即55000个样本。每张图片都是灰度图，共28x28=784个像素，每个像素点的灰度值是浮点数，范围在[0,1]之间，0代表黑色，1代表白色，介于0和1之间就是灰色。你可以将[0,1]*255==&gt;[0,255]，放大到256进制，就可以理解了。数据中，784个像素的数据都放在一行。 123456789101112131415161718192021&gt;&gt;&gt; mnist.train.images.shape(55000, 784)&gt;&gt;&gt; mnist.train.imagesarray([[0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)&gt;&gt;&gt; mnist.train.images[0] # 查看第1张图片的数据array([0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , # ......... 0. , 0. , 0.3803922 , 0.37647063, 0.3019608 , 0.46274513, 0.2392157 , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , mnist.train.labels是训练集标签。经过one-hot编码，每一个样本是10个元素的数组，只有一个元素的值是1，其它值都为0。值为1对应的下标就是该图片代表的数字，数字范围是[0,9]1234567891011121314&gt;&gt;&gt; mnist.train.labels.shape(55000, 10)&gt;&gt;&gt; mnist.train.labelsarray([[0., 0., 0., ..., 1., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], ..., [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 0., 0.], [0., 0., 0., ..., 0., 1., 0.]])&gt;&gt;&gt; mnist.train.labels[0]array([0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]) # 代表数字7 测试集有10000个样本1234&gt;&gt;&gt; mnist.test.images.shape(10000, 784)&gt;&gt;&gt; mnist.test.labels.shape(10000, 10) 1.3. mnist.train.next_batch 批量迭代获取训练数据集1234567891011&gt;&gt;&gt; x, y = mnist.train.next_batch(1)&gt;&gt;&gt; x.shape(1, 784)&gt;&gt;&gt; y.shape(1, 10)&gt;&gt;&gt; x, y = mnist.train.next_batch(50)&gt;&gt;&gt; x.shape(50, 784)&gt;&gt;&gt; y.shape(50, 10) 2. 单一隐藏层神经网络实现手写数字图片识别单一隐藏层神经网络就是全连接神经网络。单一隐藏层，就是只有全连接层。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192import osos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datadef full_connected(): """ input_data.read_data_sets """ # # 获取真实的数据 mnist = input_data.read_data_sets("./data/mnist/input_data/", one_hot=True) with tf.variable_scope("data"): """ 第1步：建立数据的占位符 特征值 x [None, 784] 目标值 y_true [None, 10] 第1维代表样本数，因为不知道有多少样本，所以设置为None """ x = tf.placeholder(tf.float32, [None, 784]) y_true = tf.placeholder(tf.int32, [None, 10]) with tf.variable_scope("fc_model"): """ 第2步：建立一个全连接层的神经网络 y_predict = tf.matmul(x, weight) + bias 已知 x[None, 784], y_predict[None, 10] 可以确定 w的形状是[784, 10]，b的形状是[10] """ # 随机初始化 权重w[784, 10] 和 偏置b[10] weight = tf.Variable(tf.random_normal([784, 10], mean=0.0, stddev=1.0), name="w") bias = tf.Variable(tf.constant(0.0, shape=[10])) # 预测None个样本的输出结果矩阵 [None, 784]*[784, 10]+[10]=[None, 10] y_predict = tf.matmul(x, weight) + bias with tf.variable_scope("soft_cross"): """ 第3步：求出所有样本的损失，然后求平均值 tf.nn.softmax_cross_entropy_with_logits可以求交叉熵损失 labels: 标签值（真实值） logits: 预测值 返回值：损失值列表 """ loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict)) with tf.variable_scope("optimizer"): """ 第4步：梯度下降求出损失 """ train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss) with tf.variable_scope("acc"): """ 第5步：计算准确率 tf.argmax(input,axis=None) 假设input形状是[10, 20, 30] tf.argmax(input, 0) 表示求出10那一维度最大值下标 tf.argmax(input, 1) 表示求出20那一维度最大值下标 tf.argmax(input, 2) 表示求出30那一维度最大值下标 现在y的形状是[None, 10] tf.argmax(y, 1) 就可以求出10那一维度最大值下标，即对应数字0-9 """ equal_list = tf.equal(tf.argmax(y_true, 1), tf.argmax(y_predict, 1)) # equal_list 有None个样本，例如值为[1, 0, 1, 0, 1, 1,..........]，求平均值作为准确率 accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32)) # 定义一个初始化变量的op init_op = tf.global_variables_initializer() # 开启会话去训练 with tf.Session() as sess: # 初始化变量 sess.run(init_op) # 迭代步数去训练，更新参数预测 for i in range(2000): # 取出真实存在的特征值和目标值，每次取出50个 mnist_x, mnist_y = mnist.train.next_batch(50) # 运行train_op训练 sess.run(train_op, feed_dict=&#123;x: mnist_x, y_true: mnist_y&#125;) print("训练第%d步，准确率为:%f" % (i + 1, sess.run(accuracy, feed_dict=&#123;x: mnist_x, y_true: mnist_y&#125;)))if __name__ == "__main__": full_connected() 2.1. 添加可视化添加以下内容 1234567891011121314151617# 要显示的标题信息tf.summary.scalar("losses", loss)tf.summary.scalar("acc", accuracy)# 要显示的高纬度信息（用直方图显示）tf.summary.histogram("weightes", weight)tf.summary.histogram("biases", bias)# 定义一个合并的opmerged = tf.summary.merge_all()with tf.Session() as sess: # 建立events文件，然后写入 filewriter = tf.summary.FileWriter("./summary/test/", graph=sess.graph) for i in range(2000): # 写入每步训练的值 summary = sess.run(merged, feed_dict=&#123;x: mnist_x, y_true: mnist_y&#125;) filewriter.add_summary(summary, i) 启动tensorboard1tensorboard --logdir summary/test 2.2. 添加模型保存以及结果预测添加以下内容12345678910111213141516171819202122232425262728293031323334# 设置命令行参数is_train，1代表进行训练，0代表进行结果测试tf.app.flags.DEFINE_integer("is_train", 1, "指定程序是预测还是训练")# 获取命令行参数FLAGS = tf.app.flags.FLAGSdef full_connected(): # 创建一个saver saver = tf.train.Saver() with tf.Session() as sess: if FLAGS.is_train == 1: for i in range(2000): # .... # 保存模型 saver.save(sess, "./model/fc_model") else: # 加载模型 saver.restore(sess, "./model/fc_model") # 如果是0，做出预测 for i in range(100): # 每次测试一张图片，y_test数据格式为[0,0,0,0,0,1,0,0,0,0] x_test, y_test = mnist.test.next_batch(1) # 求出测试标签对应的数字 digit_true = tf.argmax(y_test, 1).eval() # 求出预测标签对应的数字 digit_test = tf.argmax(sess.run(y_predict, feed_dict=&#123;x: x_test, y_true: y_test&#125;), 1).eval() print("第%d张图片，手写数字图片目标是:%d, 预测结果是:%d %s" % ( i, digit_true, digit_test, "" if digit_true == digit_test else "错误" )) 先训练得到模型并保存，再加载模型进行测试12python mnist.py --is_train=1 # 训练python mnist.py --is_train=0 # 测试 2.3. 最终代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143import osos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_data# 设置命令行参数is_train，1代表进行训练，0代表进行结果测试tf.app.flags.DEFINE_integer("is_train", 1, "指定程序是预测还是训练")# 获取命令行参数FLAGS = tf.app.flags.FLAGSdef full_connected(): """ input_data.read_data_sets """ # # 获取真实的数据 mnist = input_data.read_data_sets("./data/mnist/input_data/", one_hot=True) with tf.variable_scope("data"): """ 第1步：建立数据的占位符 特征值 x [None, 784] 目标值 y_true [None, 10] 第1维代表样本数，因为不知道有多少样本，所以设置为None """ x = tf.placeholder(tf.float32, [None, 784]) y_true = tf.placeholder(tf.int32, [None, 10]) with tf.variable_scope("fc_model"): """ 第2步：建立一个全连接层的神经网络 y_predict = tf.matmul(x, weight) + bias 已知 x[None, 784], y_predict[None, 10] 可以确定 w的形状是[784, 10]，b的形状是[10] """ # 随机初始化 权重w[784, 10] 和 偏置b[10] weight = tf.Variable(tf.random_normal([784, 10], mean=0.0, stddev=1.0), name="w") bias = tf.Variable(tf.constant(0.0, shape=[10])) # 预测None个样本的输出结果矩阵 [None, 784]*[784, 10]+[10]=[None, 10] y_predict = tf.matmul(x, weight) + bias with tf.variable_scope("soft_cross"): """ 第3步：求出所有样本的损失，然后求平均值 tf.nn.softmax_cross_entropy_with_logits可以求交叉熵损失 labels: 标签值（真实值） logits: 预测值 返回值：损失值列表 """ loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict)) with tf.variable_scope("optimizer"): """ 第4步：梯度下降求出损失 """ train_op = tf.train.GradientDescentOptimizer(0.05).minimize(loss) with tf.variable_scope("acc"): """ 第5步：计算准确率 tf.argmax(input,axis=None) 假设input形状是[10, 20, 30] tf.argmax(input, 0) 表示求出10那一维度最大值下标 tf.argmax(input, 1) 表示求出20那一维度最大值下标 tf.argmax(input, 2) 表示求出30那一维度最大值下标 现在y的形状是[None, 10] tf.argmax(y, 1) 就可以求出10那一维度最大值下标，即对应数字0-9 """ equal_list = tf.equal(tf.argmax(y_true, 1), tf.argmax(y_predict, 1)) # equal_list 有None个样本，例如值为[1, 0, 1, 0, 1, 1,..........]，求平均值作为准确率 accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32)) # 要显示的标题信息 tf.summary.scalar("losses", loss) tf.summary.scalar("acc", accuracy) # 要显示的高纬度信息（用直方图显示） tf.summary.histogram("weightes", weight) tf.summary.histogram("biases", bias) # 定义一个合并的op merged = tf.summary.merge_all() # 定义一个初始化变量的op init_op = tf.global_variables_initializer() # 创建一个saver saver = tf.train.Saver() # 开启会话去训练 with tf.Session() as sess: # 初始化变量 sess.run(init_op) # 建立events文件，然后写入 filewriter = tf.summary.FileWriter("./summary/test/", graph=sess.graph) if FLAGS.is_train == 1: # 迭代步数去训练，更新参数预测 for i in range(2000): # 取出真实存在的特征值和目标值，每次取出50个 mnist_x, mnist_y = mnist.train.next_batch(50) # 运行train_op训练 sess.run(train_op, feed_dict=&#123;x: mnist_x, y_true: mnist_y&#125;) # 写入每步训练的值 summary = sess.run(merged, feed_dict=&#123;x: mnist_x, y_true: mnist_y&#125;) filewriter.add_summary(summary, i) print("训练第%d步，准确率为:%f" % (i + 1, sess.run(accuracy, feed_dict=&#123;x: mnist_x, y_true: mnist_y&#125;))) # 保存模型 saver.save(sess, "./model/fc_model") else: # 加载模型 saver.restore(sess, "./model/fc_model") # 如果是0，做出预测 for i in range(100): # 每次测试一张图片，y_test数据格式为[0,0,0,0,0,1,0,0,0,0] x_test, y_test = mnist.test.next_batch(1) # 求出测试标签对应的数字 digit_true = tf.argmax(y_test, 1).eval() # 求出预测标签对应的数字 digit_test = tf.argmax(sess.run(y_predict, feed_dict=&#123;x: x_test, y_true: y_test&#125;), 1).eval() print("第%d张图片，手写数字图片目标是:%d, 预测结果是:%d %s" % ( i, digit_true, digit_test, "" if digit_true == digit_test else "错误" ))if __name__ == "__main__": full_connected() 3. 卷积神经网络实现手写数字图片识别123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191import osos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'import tensorflow as tffrom tensorflow.examples.tutorials.mnist import input_datadef weight_variables(shape): """ 定义一个初始化权重的函数 :param shape: :return: """ w = tf.Variable(tf.random_normal(shape=shape, mean=0.0, stddev=1.0)) return wdef bias_variables(shape): """ 定义一个初始化偏置的函数 :param shape: :return: """ b = tf.Variable(tf.constant(0.0, shape=shape)) return bdef model(): """ 自定义的卷积模型 :return: """ with tf.variable_scope("data"): """ 第1步：建立数据的占位符 特征值 x [None, 784] 目标值 y_true [None, 10] 第1维代表样本数，因为不知道有多少样本，所以设置为None """ x = tf.placeholder(tf.float32, [None, 784]) y_true = tf.placeholder(tf.int32, [None, 10]) with tf.variable_scope("conv1"): """ 第1个卷积层 卷积: filter: 个数：取32个 大小：取5x5 步长：取strides=1 padding: "SAME" 卷积网络API: tf.nn.conv2d(input, filter, strides=, padding=, name=None) input：给定的输入4阶张量，具有[batch,height,width,channel]，类型为float32,64 batch: 样本数。因为是待定参数，所以这里为None height/width: 长宽=28x28 channel: 通道数。因为图片是单通道的，所以设置为1 所以输入数据 x的形状要改变 [None, 784] ==&gt; [None, 28, 28, 1] filter: 指定过滤器张量，具有[filter_height,filter_width,in_channels,out_channels] filter_height/filter_width: 过滤器长宽，这里取5x5 in_channels: 图片通道数，这里为1 out_channels: 过滤器数量，这里为32 所以weight形状为[5, 5, 1, 32] 激活: tf.nn.relu 激活函数不改变形状，只改变数值，所以还是[None, 28, 28, 32] 池化: filter: 大小: 取2x2 步长：取strides=2 padding: "SAME" 输入形状：[None, 28, 28, 32] 输出形状：[None, 14, 14, 32] """ # 初始化weight(filter) 和 bias w_conv1 = weight_variables([5, 5, 1, 32]) b_conv1 = bias_variables([32]) # tf.nn.conv2d要求输入的数据是4维张量，所以对x进行形状的改变[None, 784] ==&gt; [None, 28, 28, 1] x_reshape = tf.reshape(x, [-1, 28, 28, 1]) # 卷积+激活[None, 28, 28, 1]-----&gt; [None, 28, 28, 32] # strides=[1, stride, stride, 1]，代表4个方向的步长，为了统一才要求以4维的形式给出。填充stride即可 x_relu1 = tf.nn.relu(tf.nn.conv2d(x_reshape, w_conv1, strides=[1, 1, 1, 1], padding="SAME") + b_conv1) # 池化 2*2 ,strides2 [None, 28, 28, 32]----&gt;[None, 14, 14, 32] # ksize: 窗口大小 # strides: 步长 x_pool1 = tf.nn.max_pool(x_relu1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME") with tf.variable_scope("conv2"): """ 第2个卷积层 卷积: 5*5*32，64个filter，strides=1 激活: tf.nn.relu 池化： filter: 个数: 64 大小: 取2x2 步长：取strides=2 padding: "SAME" 输入形状：[None, 14, 14, 64] 输出形状：[None, 7, 7, 64] """ # 随机初始化权重, 权重：[5, 5, 32, 64] 偏置[64] w_conv2 = weight_variables([5, 5, 32, 64]) b_conv2 = bias_variables([64]) # 卷积，激活，池化计算 # [None, 14, 14, 32]-----&gt; [None, 14, 14, 64] x_relu2 = tf.nn.relu(tf.nn.conv2d(x_pool1, w_conv2, strides=[1, 1, 1, 1], padding="SAME") + b_conv2) # 池化 2*2, strides 2, [None, 14, 14, 64]----&gt;[None, 7, 7, 64] x_pool2 = tf.nn.max_pool(x_relu2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding="SAME") with tf.variable_scope("conv2"): """ 全连接层 输入x_pool2: [None, 7, 7, 64]---降维--&gt;[None, 7*7*64] [None, 7*7*64] * w_fc[7*7*64, 10] + b_fc[10] = [None, 10] """ # 随机初始化权重和偏置 w_fc = weight_variables([7 * 7 * 64, 10]) b_fc = bias_variables([10]) # 修改形状 [None, 7, 7, 64] ---&gt;None, 7*7*64] x_fc_reshape = tf.reshape(x_pool2, [-1, 7 * 7 * 64]) # 进行矩阵运算得出每个样本的10个结果 y_predict = tf.matmul(x_fc_reshape, w_fc) + b_fc return x, y_true, y_predictdef conv_fc(): # 获取真实的数据 mnist = input_data.read_data_sets("./data/mnist/input_data/", one_hot=True) # 定义模型，得出输出 x, y_true, y_predict = model() # 进行交叉熵损失计算 # 3、求出所有样本的损失，然后求平均值 with tf.variable_scope("soft_cross"): # 求平均交叉熵损失 loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict)) # 4、梯度下降求出损失 with tf.variable_scope("optimizer"): train_op = tf.train.GradientDescentOptimizer(0.0001).minimize(loss) # 5、计算准确率 with tf.variable_scope("acc"): equal_list = tf.equal(tf.argmax(y_true, 1), tf.argmax(y_predict, 1)) # equal_list None个样本 [1, 0, 1, 0, 1, 1,..........] accuracy = tf.reduce_mean(tf.cast(equal_list, tf.float32)) # 定义一个初始化变量的op init_op = tf.global_variables_initializer() # 开启回话运行 with tf.Session() as sess: sess.run(init_op) # 循环去训练 for i in range(1000): # 取出真实存在的特征值和目标值 mnist_x, mnist_y = mnist.train.next_batch(50) # 运行train_op训练 sess.run(train_op, feed_dict=&#123;x: mnist_x, y_true: mnist_y&#125;) print("训练第%d步,准确率为:%f" % (i, sess.run(accuracy, feed_dict=&#123;x: mnist_x, y_true: mnist_y&#125;)))if __name__ == '__main__': conv_fc() 3.1. 步长步长一般设置为1，这样观察得更仔细些 3.2. 激活层为什么要激活函数？如果没有激活函数，神经网络只能解决线性问题，反之有了激活函数，就能解决非线性问题 为什么不用sigmoid，而是relu？ 从计算公式可以看出，sigmoid比relu计算量大 sigmoid容易导致梯度爆炸]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepLearning 神经网络基础]]></title>
    <url>%2F2019%2F04%2F05%2FDeepLearning%2FDeepLearning-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[参考资料 神经网络浅讲：从神经元到深度学习 AI学习笔记：[1]神经元与神经网络 1. 神经元神经元是神经网络的基本结构，是最小的神经网络。神经元有多个输入和一个输出 输入x：代表样本特征值，如 [房屋面积，房屋价格，社区评分] 连接（Connection）是图中的边。每一个连接上都有一个权重（Weight） x = \begin{pmatrix} x_1 \\ x_2 \\ x_3 \end{pmatrix}， w = \begin{pmatrix} w_1 \\ w_2 \\ w_3 \end{pmatrix}信号处理函数分为两部分 权重加权。每个输入信号x1, x2, x3，对应的权重分别为w1, w2, w3，对应相乘相加。再加上内部强度（也叫偏置bias），用 b 表示 激活函数。用 a=σ(z) 表示 计算公式为： z = x_1w_1 + x_2w_2 + x_3w_3 + b \\ a = σ(z)用向量表示为： z = {w^T}x + bw可以解理为斜率，b可以解理为截距 z被代入到激活函数 a=σ(z) 得到神经元的输出，这里的 a 表示神经元的激活状态 注意到这里的σ(z)还没有具体展开。 σ(z)被称为激活函数，具体有很多种，在整个神经网络中不同层的神经元可以使用不同的激活函数 1.1. 多分类单个神经元的决策边界是一条直线，只能解决二分类问题 如果想要解决三分类问题，就要两个神经元，因为两个神经元决策边界是两条直线 扩展w，w从向量变成了矩阵 w = \begin{pmatrix} w_{11} & w_{21} \\ w_{12} & w_{22} \\ w_{13} & w_{23} \end{pmatrix}2. 激活函数σ(z)激活函数有好几种，常见的有以下几种： sigmoid函数（知名度最高） a = \frac{1}{1 + e^{-z}}tanh双曲正切函数 a = \frac{e^z - e^{-z}}{e^z + e^{-z}}ReLU（Rectified Linear Unit）线性整流函数 a = max(0, z)Leaky ReLu函数 a = max(0.01z, z)先记住以下几点： 几乎所有的情况下 Tanh 都要比 Sigmoid 要好 默认用 ReLu 就很好 至于为什么是这样，需要阅读好多论文才能懂，暂时先记住 3. 神经网络3.1. 什么是神经网络定义：在机器学习和认知科学领域，人工神经网络（artificial neural network，缩写ANN），简称神经网络（neural network，缩写NN）或类神经网络，是一种模仿生物神经网络的结构和功能的计算模型，用于对函数进行估计或近似。 多个神经元组成的网络就是神经网络 一个神经网络的训练算法就是让权重的值调整到最佳，以使得整个网络的预测效果最好 3.2. 神经网络的特点整个神经网络分为：输入层，隐藏层，输出层。一般说L层神经网络，指的是有L个隐层，输入层和输出层都不计算在内的 输入层的神经元的个数取决于输入向量的维度（样本数） 输出层的神经元的数量等于与每个输入相关联的输出的数量。例如手写数识别，每张图片只可能对应数字0~9，所以输出层神经元数量为10 每个连接都有个权值 同一层神经元之间没有连接 第N层与第N-1层的所有神经元连接，也叫全连接（full connection） 3.3. 神经网络的种类基础神经网络：单层感知器，线性神经网络，BP神经网络，Hopfield神经网络等 进阶神经网络：玻尔兹曼机，受限玻尔兹曼机，递归神经网络等 深度神经网络：深度置信网络，卷积神经网络，循环神经网络，LSTM网络等 3.4. 神经网络的组成结构（Architecture）例如，神经网络中的变量可以是神经元连接的权重 激励函数（Activity Rule）大部分神经网络模型具有一个短时间尺度的动力学规则，来定义神经元如何根据其他神经元的活动来改变自己的激励值。 学习规则（Learning Rule）学习规则指定了网络中的权重如何随着时间推进而调整。（反向传播算法）]]></content>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 队列]]></title>
    <url>%2F2019%2F04%2F05%2FDeepLearning%2FTensorFlow%2FTensorFlow-%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[1. 队列基本操作12345678910111213141516171819202122232425262728293031323334import tensorflow as tfimport osos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'# 创建队列，第1个参数capacity代表队列容量，第2个参数dtypes代表元素类型q = tf.FIFOQueue(3, tf.float32)# 添加数据enq_many = q.enqueue_many([[0.1, 0.2, 0.3]])# 取数据，加1，再放回队列out_q = q.dequeue()data = out_q + 1.0en_q = q.enqueue(data)with tf.Session() as sess: # 初始化队列 sess.run(enq_many) for i in range(100): """ 根据拓扑结构，sess.run(en_q)相当于执行以下3个过程 out_q = q.dequeue() data = out_q + 1.0 en_q = q.enqueue(data) """ sess.run(en_q) # q.size()是一个OP，调用eval()才能得到值 for i in range(q.size().eval()): # 出队列 print(sess.run(q.dequeue()))]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 线性回归]]></title>
    <url>%2F2019%2F04%2F05%2FDeepLearning%2FTensorFlow%2FTensorFlow-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[1. TensorFlow 实现线性回归1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import tensorflow as tfimport osos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'def linear_regression(): """ 自实现一个线性回归预测 :return: None """ with tf.variable_scope("data"): """ 第1步: 生成实验数据 """ # 生成 x，张量形状100行1列 [100, 1] x = tf.random_normal([100, 1], mean=1.75, stddev=0.5, name="x_data") # 生成 y=0.7x+0.8，矩阵相乘必须是二维的，所以0.7表示成 [[0.7]] y_true = tf.matmul(x, [[0.7]]) + 0.8 with tf.variable_scope("model"): """ 第2步：建立线性回归模型 y=w*x+b weight(w): 权重 bias(b): 偏置 随机给生成weight和bias初始值，计算y_predict=x*w+b，再去计算损失，再在当前状态下优化 注意weight/bias用tf.Variable定义为变量才能优化，不能定义为常量 """ # tf.random_normal生成一个随机的weight，形状[1, 1] weight = tf.Variable(tf.random_normal([1, 1], mean=0.0, stddev=1.0), name="w") # 生成bias，设置为0.0即可 bias = tf.Variable(0.0, name="b") # 计算y_predict = x * w + b 形状 y_predict[100,1] &lt;== x[100,1] x w[1,1] + b y_predict = tf.matmul(x, weight) + bias with tf.variable_scope("loss"): """ 第3步：建立损失函数，取均方误差MSE """ loss = tf.reduce_mean(tf.square(y_true - y_predict)) with tf.variable_scope("optimizer"): """ 第4步：梯度下降优化损失 leaning_rate: 一般取 0~1、2、3、5、7、10 """ # GradientDescentOptimizer(learning_rate) # minimize(loss)将损失最小化 train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss) # 定义一个初始化变量的op init_op = tf.global_variables_initializer() # 通过session运行程序 with tf.Session() as sess: # 初始化变量 sess.run(init_op) # 打印随机最先初始化的权重和偏置 print("随机初始化的参数权重为：%f, 偏置为：%f" % (weight.eval(), bias.eval())) # 循环训练 运行优化 max_step = 5000 # 定义训练次数 for i in range(max_step): sess.run(train_op) # 打印每一次训练之后的weight和bias print("第%d次优化的参数权重为：%f, 偏置为：%f" % (i + 1, weight.eval(), bias.eval()))if __name__ == "__main__": linear_regression() 1.1. tf.Variable的trainable参数trainable参数决定了变量在训练时，是否能被optimizer更新。默认值为True 这里我们使用的optimizer是GradientDescentOptimizer，也就是説，在递降下降优化时，如果变量的trainable设置为False，则不会更新该变量 修改程序，设置bias变量的trainable为False。weight不修改，trainable默认为True1234567891011weight = tf.Variable(tf.random_normal([1, 1], mean=0.0, stddev=1.0), name="w")bias = tf.Variable(0.0, name="b", trainable=False)"""训练优化时，可以看到weight在更新，但是bias就不会被更新 随机初始化的参数权重为：0.108960, 偏置为：0.000000 第1次优化的参数权重为：0.790025, 偏置为：0.000000 第2次优化的参数权重为：1.016438, 偏置为：0.000000 第3次优化的参数权重为：1.079658, 偏置为：0.000000 第4次优化的参数权重为：1.091300, 偏置为：0.000000""" 反一下，将weight的trainable设置为False1234567891011weight = tf.Variable(tf.random_normal([1, 1], mean=0.0, stddev=1.0), name="w", trainable=False)bias = tf.Variable(0.0, name="b")"""训练优化时，可以看到weight不更新，bias在更新 随机初始化的参数权重为：-0.799985, 偏置为：0.000000 第1次优化的参数权重为：-0.799985, 偏置为：0.668632 第2次优化的参数权重为：-0.799985, 偏置为：1.230450 第3次优化的参数权重为：-0.799985, 偏置为：1.676977 第4次优化的参数权重为：-0.799985, 偏置为：2.022964""" 1.2. 学习率的设置学习率一般都是设置为纯小数，0.1、0.01、0.05、0.0001都是比较常见的，不同场景取值不一定相同 尝试将学习率设置为1.012345678train_op = tf.train.GradientDescentOptimizer(1.0).minimize(loss)"""学习率过大，导致无法收敛 第44次优化的参数权重为：-inf, 偏置为：-258865874387307336192779716312766087168.000000 第45次优化的参数权重为：nan, 偏置为：inf 第46次优化的参数权重为：nan, 偏置为：nan 第47次优化的参数权重为：nan, 偏置为：nan""" 尝试将学习率设置为0.512345678train_op = tf.train.GradientDescentOptimizer(0.5).minimize(loss)"""学习率过大，无法收敛 第73次优化的参数权重为：25116022189550720086865452449868546048.000000, 偏置为：14163305502976779258935682315874992128.000000 第74次优化的参数权重为：-74728367966826988948298285007438348288.000000, 偏置为：-42178556168502280071211426706320523264.000000 第75次优化的参数权重为：inf, 偏置为：inf 第76次优化的参数权重为：nan, 偏置为：nan""" 尝试将学习率设置为0.312345678910train_op = tf.train.GradientDescentOptimizer(0.3).minimize(loss)"""学习率过大，无法收敛 第199次优化的参数权重为：27622535044875995800150868730826457088.000000, 偏置为：14452844505972508223613938308202102784.000000 第200次优化的参数权重为：-38350900392920344132144949338126155776.000000, 偏置为：-21995243882872851252496778696681586688.000000 第201次优化的参数权重为：59621299127281493356407110148567334912.000000, 偏置为：31287847878689103312685269306321141760.000000 第202次优化的参数权重为：-inf, 偏置为：-51636013054275812452146957581006405632.000000 第203次优化的参数权重为：nan, 偏置为：inf 第204次优化的参数权重为：nan, 偏置为：nan""" 尝试将学习率设置为0.212345678train_op = tf.train.GradientDescentOptimizer(0.2).minimize(loss)"""差不多训练500多次，就达到目标值 第542次优化的参数权重为：0.700000, 偏置为：0.799999 第543次优化的参数权重为：0.700000, 偏置为：0.799999 第544次优化的参数权重为：0.700000, 偏置为：0.800000 第545次优化的参数权重为：0.700000, 偏置为：0.800000""" 尝试将学习率设置为0.11234567891011121314train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss)"""差不多训练1000多次，近似达到目标值 第1005次优化的参数权重为：0.700001, 偏置为：0.799998 第1006次优化的参数权重为：0.700001, 偏置为：0.799998 第1007次优化的参数权重为：0.700001, 偏置为：0.799999 第1008次优化的参数权重为：0.700001, 偏置为：0.799999 ...... 第4996次优化的参数权重为：0.700000, 偏置为：0.799999 第4997次优化的参数权重为：0.700000, 偏置为：0.799999 第4998次优化的参数权重为：0.700000, 偏置为：0.799999 第4999次优化的参数权重为：0.700000, 偏置为：0.799999 第5000次优化的参数权重为：0.700000, 偏置为：0.799999""" 1.3. tf.variable_scope设置变量作用域tf.variable_scope() 设置变量作用域，本来的作用是结合tf.get_variable()实现变量的共享。但是这里使用tf.variable_scope，主要目的是为了使程序结构更加清晰。而且使用TensorBoard查看时，图的结构也更加清晰 2. 线性回归可视化2.1. 增加可视化功能在程序中添加tf.summary.FileWriter，将图序列化到事件文件123with tf.Session() as sess: # 序列化生成事件文件 filewriter = tf.summary.FileWriter("summary", graph=sess.graph) 启动TensorBoard1tensorboard --logdir summary 2.2. 添加数据的显示1234567891011121314151617181920# tf.summary.scalar添加标量统计结果tf.summary.scalar("losses", loss)# tf.summary.histogram添加任意shape的Tensor，统计这个Tensor的取值分布tf.summary.histogram("weights", weight)# 合并所有的summary操作，之后session只需要运行该op，就可以运行所有的summary opmerged = tf.summary.merge_all()with tf.Session() as sess: # ... # 序列化生成事件文件 filewriter = tf.summary.FileWriter("summary", graph=sess.graph) max_step = 5000 for i in range(max_step): sess.run(train_op) # 运行合并的tensor summary = sess.run(merged) # 序列化summary filewriter.add_summary(summary, i + 1) 3. 保存与加载模型假设训练要非常久，有时候训练到一半宕机了，下一次必须从头开始训练，这样前面的工作都白费了。 我们可以每训练一定次数，就把weight和bias序列化到文件中。下一次读取weight和bias的值，作为初始值进行训练即可 下面设置模型的保存123456789# 定义一个保存模型的实例saver = tf.train.Saver()with tf.Session() as sess: for i in range(max_step): # ...... # 训练结束后保存模型，将模型保存到model目录下 saver.save(sess, "model/linear_regression_model") 保存模型之后，在model目录可以看到12345model|--checkpoint|--linear_regression_model.data-00000-of-00001|--linear_regression_model.index|--linear_regression_model.meta 设置加载模型1234567891011121314# 定义一个保存模型的实例saver = tf.train.Saver()with tf.Session() as sess: # 开始训练之前，先加载模型，覆盖模型当中随机定义的参数，从上次训练的参数结果开始 # 如果存在model/checkpoint，说明之前已经保存过模型，才加载 if os.path.exists("model/checkpoint"): saver.restore(sess, "model/linear_regression_model") for i in range(max_step): # ...... # 训练结束后保存模型，将模型保存到model目录下 saver.save(sess, "model/linear_regression_model") 4. 自定义命令行参数123456789101112131415161718192021"""TF定义命令行参数: DEFINE_xxx(参数名, 默认值, 描述)"""tf.app.flags.DEFINE_integer("max_step", 100, "模型训练的步数")tf.app.flags.DEFINE_string("model_dir", "model/linear_regression_model", "模型文件的加载的路径")# 从命令行中取出参数，之后可以通过FLAGS.max_step和FLAGS.model_dir引用对应的参数FLAGS = tf.app.flags.FLAGSdef linear_regression(): # ...... with tf.Session() as sess: if os.path.exists("model/checkpoint"): saver.restore(sess, FLAGS.model_dir) # 引用FLAGS.model_dir for i in range(FLAGS.max_step): # 引用FLAGS.max_step # ...... saver.save(sess, FLAGS.model_dir) # 引用FLAGS.model_dir 用命令行执行该py文件1python tf_linear_regression.py --max_step=200 --model_dir=&quot;model/linear_regression_model&quot; 5. 最终代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112import tensorflow as tfimport osos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"""TF定义命令行参数: DEFINE_xxx(参数名, 默认值, 描述)"""tf.app.flags.DEFINE_integer("max_step", 100, "模型训练的步数")tf.app.flags.DEFINE_string("model_dir", "model/linear_regression_model", "模型文件的加载的路径")# 从命令行中取出参数，之后可以通过FLAGS.max_step和FLAGS.model_dir引用对应的参数FLAGS = tf.app.flags.FLAGSdef linear_regression(): """ 自实现一个线性回归预测 :return: None """ with tf.variable_scope("data"): """ 第1步: 生成实验数据 """ # 生成 x，张量形状100行1列 [100, 1] x = tf.random_normal([100, 1], mean=1.75, stddev=0.5, name="x_data") # 生成 y=0.7x+0.8，矩阵相乘必须是二维的，所以0.7表示成 [[0.7]] y_true = tf.matmul(x, [[0.7]]) + 0.8 with tf.variable_scope("model"): """ 第2步：建立线性回归模型 y=w*x+b weight(w): 权重 bias(b): 偏置 随机给生成weight和bias初始值，计算y_predict=x*w+b，再去计算损失，再在当前状态下优化 注意weight/bias用tf.Variable定义为变量才能优化，不能定义为常量 """ # tf.random_normal生成一个随机的weight，形状[1, 1] weight = tf.Variable(tf.random_normal([1, 1], mean=0.0, stddev=1.0), name="w") # 生成bias，设置为0.0即可 bias = tf.Variable(0.0, name="b") # 计算y_predict = x * w + b 形状 y_predict[100,1] &lt;== x[100,1] x w[1,1] + b y_predict = tf.matmul(x, weight) + bias with tf.variable_scope("loss"): """ 第3步：建立损失函数，取均方误差MSE """ loss = tf.reduce_mean(tf.square(y_true - y_predict)) with tf.variable_scope("optimizer"): """ 第4步：梯度下降优化损失 leaning_rate: 一般取 0~1、2、3、5、7、10 """ # GradientDescentOptimizer(learning_rate) # minimize(loss)将损失最小化 train_op = tf.train.GradientDescentOptimizer(0.1).minimize(loss) # 定义一个初始化变量的op init_op = tf.global_variables_initializer() # tf.summary.scalar添加标量统计结果 tf.summary.scalar("losses", loss) # tf.summary.histogram添加任意shape的Tensor，统计这个Tensor的取值分布 tf.summary.histogram("weights", weight) # 合并所有的summary操作，之后session只需要运行该op，就可以运行所有的summary op merged = tf.summary.merge_all() # 定义一个保存模型的实例 saver = tf.train.Saver() # 通过session运行程序 with tf.Session() as sess: # 初始化变量 sess.run(init_op) # 打印随机最先初始化的权重和偏置 print("随机初始化的参数权重为：%f, 偏置为：%f" % (weight.eval(), bias.eval())) # 序列化生成事件文件 filewriter = tf.summary.FileWriter("summary", graph=sess.graph) # 开始训练之前，先加载模型，覆盖模型当中随机定义的参数，从上次训练的参数结果开始 # 如果存在model/checkpoint，说明之前已经保存过模型，才加载 if os.path.exists("model/checkpoint"): saver.restore(sess, FLAGS.model_dir) # 循环训练 运行优化 for i in range(FLAGS.max_step): sess.run(train_op) # 运行合并的tensor summary = sess.run(merged) filewriter.add_summary(summary, i + 1) # 打印每一次训练之后的weight和bias print("第%d次优化的参数权重为：%f, 偏置为：%f" % (i + 1, weight.eval(), bias.eval())) saver.save(sess, FLAGS.model_dir) if __name__ == "__main__": linear_regression()]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow TensorBoard可视化]]></title>
    <url>%2F2019%2F04%2F05%2FDeepLearning%2FTensorFlow%2FTensorFlow-TensorBoard%E5%8F%AF%E8%A7%86%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1. 实现可视化TensorBoard通过读取TensorFlow的事件文件来运行，所以首先将图序列化到文件中，生成的文件就是事件文件12345678910a = tf.constant(4, name="input_a")b = tf.constant(2, name="input_b")c = tf.multiply(a, b, name="mul_c")d = tf.add(a, b, name="add_d")e = tf.add(c, d, name="add_e")with tf.Session() as sess: # 第1个参数: 要将文件序列化到哪个目录下 # graph参数: 将指定图序列化到文件中 file_writer = tf.summary.FileWriter("summary", graph=tf.get_default_graph()) 命令行启动tensorboard1tensorboard --logdir summary 启动tensorboard，在Windows下，会发现错误123 File &quot;c:\software\development environment\python37\lib\site-packages\tensorboard\manager.py&quot;, line 51, in &lt;lambda&gt; (dt - datetime.datetime.fromtimestamp(0)).total_seconds()),OSError: [Errno 22] Invalid argument 解决方法是，找到manager.py，做如下修改123# 将原来的注释掉 (dt - datetime.datetime.fromtimestamp(0)).total_seconds())# 将参数0修改为86400(dt - datetime.datetime.fromtimestamp(86400)).total_seconds()) 启动tensorboard之后，浏览器访问localhost:6006即可]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 变量]]></title>
    <url>%2F2019%2F04%2F05%2FDeepLearning%2FTensorFlow%2FTensorFlow-%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[1. 什么是变量变量也是一种OP，是一种特殊的张量，能够进行存储持久化，它的值就是张量 2. 创建变量1234var = tf.Variable(tf.random_normal([2, 3], mean=0.0, stddev=1.0)) # 创建变量with tf.Session() as sess: sess.run(tf.global_variables_initializer()) # 必须先运行global_variables_initializer print(sess.run(var))]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 张量元素类型转换]]></title>
    <url>%2F2019%2F04%2F05%2FDeepLearning%2FTensorFlow%2FTensorFlow-%E5%BC%A0%E9%87%8F%E5%85%83%E7%B4%A0%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[tf.string_to_number(string_tensor, out_type=_dtypes.float32, name=None) tf.to_double(x, name=’ToDouble’) tf.to_float(x, name=’ToFloat’) tf.to_int32(x, name=’ToInt32’) tf.to_int64(x, name=’ToInt64’) tf.cast(x, dtype, name=None) 万能]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 张量初始化]]></title>
    <url>%2F2019%2F04%2F05%2FDeepLearning%2FTensorFlow%2FTensorFlow-%E5%BC%A0%E9%87%8F%E5%88%9D%E5%A7%8B%E5%8C%96%2F</url>
    <content type="text"><![CDATA[tf.ones(shape, dtype=dtypes.float32, name=None) 初始化元素为1 tf.zeros(shape, dtype=dtypes.float32, name=None) 初始化元素为0 tf.ones_like(tensor, dtype=None, name=None, optimize=True) 初始化元素为1，形状与指定tensor相同的张量 tf.zeros_like(tensor, dtype=None, name=None, optimize=True) 初始化元素为0，形状与指定tensor相同的张量 tf.fill(dims, value, name=None) dims指定张量形状，填充值为value tf.constant(value, dtype=None, shape=None, name=&quot;Const&quot;, verify_shape=False) 创建常量tensor tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) 根据正态分布生成随机张量，mean是均值，stddev是标准差，seed是随机种子 tf.truncated_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32, seed=None, name=None) tf.random_uniform(shape, minval=0, maxval=None, dtype=tf.float32, seed=None, name=None) tf.lin_space(start, stop, num, name=None) tf.range(start, limit=None, delta=1, dtype=None, name=&quot;range&quot;)]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 张量]]></title>
    <url>%2F2019%2F04%2F05%2FDeepLearning%2FTensorFlow%2FTensorFlow-%E5%BC%A0%E9%87%8F%2F</url>
    <content type="text"><![CDATA[1. Tensor三要素Tensor包括三部分：名字、形状、数据类型，直接用 print就可以查看123456a = tf.constant(1)b = tf.constant([1, 2])c = tf.constant([[1, 2], [3, 4], [5, 6]])print(a) # Tensor("Const:0", shape=(), dtype=int32) name中冒号之前是op类型，冒号之后的0是无意义的print(b) # Tensor("Const_1:0", shape=(2,), dtype=int32)print(c) # Tensor("Const_2:0", shape=(3, 2), dtype=int32) 2. Tensor的阶Tensor底层是对Numpy的ndarray的封装。ndarray中叫维度，到了Tensor这里就叫阶。数学中更习惯用阶这个概念 阶 数学实例 Python 例子 0 纯量 (只有大小) s = 483 1 向量 (大小和方向) v = [1.1, 2.2, 3.3] 2 矩阵 (数据表) m = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] 3 3阶张量 (数据立体) t = [[[2], [4], [6]], [[8], [10], [12]], [[14], [16], [18]]] n n阶 (自己想想看) …. 3. Tensor数据类型张量的数据类型可以是以下数据类型中的任意一种 数据类型 Python 类型 描述 DT_FLOAT tf.float32 32 位浮点数 DT_DOUBLE tf.float64 64 位浮点数 DT_INT64 tf.int64 64 位有符号整型 DT_INT32 tf.int32 32 位有符号整型 DT_INT16 tf.int16 16 位有符号整型 DT_INT8 tf.int8 8 位有符号整型 DT_UINT8 tf.uint8 8 位无符号整型 DT_STRING tf.string 可变长度的字节数组.每一个张量元素都是一个字节数组 DT_BOOL tf.bool 布尔型 DT_COMPLEX64 tf.complex64 由两个32位浮点数组成的复数:实数和虚数 DT_QINT32 tf.qint32 用于量化Ops的32位有符号整型 DT_QINT8 tf.qint8 用于量化Ops的8位有符号整型 DT_QUINT8 tf.quint8 用于量化Ops的8位无符号整型 4. Tensor的属性12345678910tensor = tf.constant(1)print(tensor.graph) # 所属的图print(tensor.dtype) # 数据类型print(tensor.name) # 名称print(tensor.op) # 操作名print(tensor.shape) # 形状with tf.Session() as sess: print(tensor.eval()) # 值 5. Tensor动态形状和静态形状动态形状：生成新的张量静态形状：不会生成新的张量 5.1. 静态形状一旦张量形状固定了，就无法再修改静态形状 12345678p = tf.placeholder(tf.float32)print(p.shape) # &lt;unknown&gt;, placeholder一开始没有形状p.set_shape([3, 2])print(p.shape) # (3, 2)p.set_shape([4, 2]) # ValueError，静态形状一旦被设置，就无法再修改print(p.shape) 5.2. 动态形状tf.reshape() 返回修改某个tensor形状的副本，不会影响原tensor的形状。修改形状时，注意元素个数必须匹配 12345p = tf.placeholder(tf.float32, [6, 2])print(p.shape)p1 = tf.reshape(p, [4, 3])p2 = tf.reshape(p, [2, 6])]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 会话]]></title>
    <url>%2F2019%2F04%2F04%2FDeepLearning%2FTensorFlow%2FTensorFlow-%E4%BC%9A%E8%AF%9D%2F</url>
    <content type="text"><![CDATA[1. 创建销毁会话123sess = tf.Session() # 创建会话sess.run(...) # 计算sess.close() # 销毁会话，释放资源 可以使用with简化写法12with tf.Session() as sess: sess.run(...) # 计算 2. 运行指定图123456g = tf.Graph()with g.as_default(): a = tf.constant(1.0)# graph参数指定要运行哪个图with tf.Session(graph=g) as sess: print(sess.run(a)) 3. config=tf.ConfigProto(log_device_placement=True)1234a = tf.constant(1)# 打印各个操作在哪些设备上运行with tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess: print(sess.run(a)) 4. 交互式会话交互式会话一般用于命令行调试，比较方便 123a = tf.constant(1)tf.InteractiveSession() # 开启交互式会话print(a.eval()) # 此时a在交互式会话中运行 5. session.run()run的原型如下1234run(fetches feed_dict=None options=None run_metadata=None) 5.1. fetches参数fetches可以是图中的一个元素 123a = tf.constant(1)with tf.Session() as sess: print(sess.run(a)) fetche只能是图中的元素，不可以是其它类型 12with tf.Session() as sess: print(sess.run(1.0)) # Error 图元素可以是以下类型之一：1234567891011121314151617181920* An @&#123;tf.Operation&#125;. The corresponding fetched value will be `None`.# 相应的取值将会是None* A @&#123;tf.Tensor&#125;. The corresponding fetched value will be a numpy ndarray containing the value of that tensor.# 相应的获取值将是包含该张量值的numpy ndarray。* A @&#123;tf.SparseTensor&#125;. The corresponding fetched value will be a @&#123;tf.SparseTensorValue&#125; containing the value of that sparse tensor.# 相应的获取值将是包含该稀疏张量值的@&#123;tf.SparseTensorValue&#125;。* A `get_tensor_handle` op. The corresponding fetched value will be a numpy ndarray containing the handle of that tensor.# `get_tensor_handle`操作。 相应的获取值将是包含该张量的句柄的numpy ndarray。* A `string` which is the name of a tensor or operation in the graph.# `string`，它是图中张量或操作的名称。 fetches可以是包含图中元素的list/tuple 12345a = tf.constant(1)b = tf.constant(2)c = a + bwith tf.Session() as sess: print(sess.run([a, b, c])) 5.2. feed_dict参数feed_dict允许调用者覆盖图中张量的值（the value of tensors in the graph）， feed_dict中的每个键可以是以下类型之一： {tf.Tensor} - 则值value可能是Python标量scalar、字符串string、列表list、numpy ndarray，可以将其转化为‘dtype’相同的张量 {tf.placeholder} - 则将检查值的形状是否与占位符兼容。 {tf.SparseTensor} - 则该值应为{tf.SparseTensorValue}。 {tf.SparseTensorValue}. - 则该值应为{tf.SparseTensorValue}。nested tuple of Tensors or SparseTensors - 则该值应该是嵌套元组nested tuple，其结构与上面相应的值相同。 123456p = tf.placeholder(tf.float32)with tf.Session() as sess: print(sess.run(p, feed_dict=&#123;p: 1.0&#125;)) print(sess.run(p, feed_dict=&#123;p: 2.0&#125;)) print(sess.run(p, feed_dict=&#123;p: [1.0, 2.0]&#125;)) print(sess.run(p, feed_dict=&#123;p: [[1.0, 2.0], [3.0, 4.0]]&#125;))]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 图]]></title>
    <url>%2F2019%2F04%2F04%2FDeepLearning%2FTensorFlow%2FTensorFlow-%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[1. 默认图TF程序一开始，就已经创建了graph，可以通过 tf.get_default_graph() 来获取当前默认的计算图123import tensorflow as tfgraph = tf.get_default_graph() # &lt;tensorflow.python.framework.ops.Graph object at 0x000002D9EA92F908&gt;print(graph) 也可以通过tensor或者session的graph属性，来获取默认图，得到的都是同一个默认图123456789101112131415import tensorflow as tfa = tf.constant(4, name="input_a")b = tf.constant(2, name="input_b")c = tf.multiply(a, b, name="mul_c")d = tf.add(a, b, name="add_d")e = tf.add(c, d, name="add_e")graph = tf.get_default_graph() # 获取默认图print(a.graph is graph) # 通过tensor.graph获取默认图，可以看到就是graph对象print(c.graph is graph)print(e.graph is graph)with tf.Session() as sess: print(sess.graph is graph) # 通过session.graph获取默认图，可以看到就是graph对象 2. 创建图创建新的图，与默认图是两个不同的graph对象12345678910111213graph = tf.get_default_graph() # 获取当前默认的计算图g = tf.Graph() # 创建一张图print(graph)print(g)print(g is graph) # 可以看到新图与默认图是两个不同的实例a = tf.constant(1)print(a.graph is graph) # True，说明tensor.graph始终是默认图print(a.graph is g) # Falsewith tf.Session() as sess: print(sess.graph is graph) # True，说明session.graph始终是默认图 print(sess.graph is g) # False 将新图作为默认图 1234567891011121314151617graph = tf.get_default_graph()g = tf.Graph()a = tf.constant(1)print(a.graph is graph) # True, 因为此时默认图是graphprint(a.graph is g) # Falsewith g.as_default(): print(tf.get_default_graph() is g) # True，在执行g.as_default()之后，g成为默认图 print(tf.get_default_graph() is graph) # False，graph不再是默认图 print(a.graph is graph) # True，在执行g.as_default()之后，a的图不变，还是原来的图 print(a.graph is g) b = tf.constant(1) print(b.graph is g) # True，b是在执行g.as_default()之后定义的，此时默认图是g，所以b属于g print(b.graph is graph) # False]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 核心概念]]></title>
    <url>%2F2019%2F04%2F04%2FDeepLearning%2FTensorFlow%2FTensorFlow-%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[1. 张量（tensor）TensorFlow中的Tensor，底层实际上是对Numpy:ndarray的封装，只是改头换面换个名字，叫Tensor ndarray是多维度的，所以Tensor也是多维度的，只是Tensor这里维度换个叫法，称为阶 张量是基于向量和矩阵的推广 标量是0阶张量，只有大小 向量是1阶张量，有大小和方向 矩阵是2阶张量，是一张数据表 3阶张量，是立体的数据 任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据 可以用四阶张量表示一个包含多张图片的数据集，这四个维度分别是：图片在数据集中的编号，图片高度、宽度，以及色彩数据 2. 数据流图 / 计算图（graph）以一段TF程序为例1234567891011import tensorflow as tfa = tf.constant(4, name="input_a")b = tf.constant(2, name="input_b")c = tf.multiply(a, b, name="mul_c")d = tf.add(a, b, name="add_d")e = tf.add(c, d, name="add_e")sess = tf.Session()print(sess.run(e))sess.close() 整个程序可以表示为如下的数据流图 数据流图由节点和边构成，是一种有向图，用来表示计算任务 节点：代表对数据的运算或某种操作 边：代表节点之间数据的输入输出关系，或者説是数据的流向，是有方向的 数据流图中的数据就是张量，从输入端输入，经过各个节点完成计算。节点将被分配到各种计算设备异步并行地执行运算。张量流过的过程，就是TensorFlow框架名称的由来 3. 会话（session）以下代码只是定义了数据流图12345a = tf.constant(4, name="input_a")b = tf.constant(2, name="input_b")c = tf.multiply(a, b, name="mul_c")d = tf.add(a, b, name="add_d")e = tf.add(c, d, name="add_e") 数据流图只是定义/描述了tensor的计算流程，只是一个壳，还没有运行。session负责管理协调整个数据流图的计算过程123sess = tf.Session() # 创建会话print(sess.run(e)) # 计算数据流图sess.close() # 销毁会话 总的来讲，session有这些作用 运行计算图 分配计算资源 掌握变量、队列、线程等资源 4. 操作（op）5. 前端子系统和后端子系统TensorFlow的系统结构以API为界，将整个系统分为「前端」和「后端」两个子系统。前端系统扮演了Client的角色，完成计算图的构造，通过转发Protobuf格式的GraphDef给后端系统的Master，并启动计算图的执行过程]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 常量]]></title>
    <url>%2F2019%2F04%2F04%2FDeepLearning%2FTensorFlow%2FTensorFlow-%E5%B8%B8%E9%87%8F%2F</url>
    <content type="text"><![CDATA[1. tf.constanttf.constant 用来创建一个常量tensor，其原型如下。可以看到只有value参数是必须的1234567tf.constant( value, dtype=None, shape=None, name='Const', verify_shape=False) 1.1. value参数value可以是数值或者列表，创建各维度tensor123456789101112t0 = tf.constant(1) # 0维tensort1 = tf.constant([1, 2]) # 1维tensort2 = tf.constant([[1, 2], [3, 4]]) # 2维tensort3 = tf.constant([[[1, 2], # 3维tensor [3, 4]], [[1, 2], [3, 4]]])with tf.Session() as sess: # 通过eval()查看值 print(t0.eval(), t1.eval(), t2.eval(), t3.eval()) # 通过sess.run()查看值 print(sess.run(t0), sess.run(t1), sess.run(t2), sess.run(t3)) 1.2. dtype参数dtype指定元素的数据类型123456789t0 = tf.constant(1, dtype=tf.int16)t1 = tf.constant([1, 2], dtype=tf.int32)t2 = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)t3 = tf.constant([[[1, 2], [3, 4]], [[1, 2], [3, 4]]], dtype=tf.float64)with tf.Session() as sess: print(t0.eval(), t1.eval(), t2.eval(), t3.eval()) 1.3. shape参数shape指定张量的形状，即维数以及每一维的大小。如果指定了shape，当第一个参数value是数字时，张量的所有元素都会用该数字填充：12345678t = tf.constant(-1, shape=[2, 3])with tf.Session() as sess: """ 可以看到是一个二维张量，第一维大小为2， 第二维大小为3，全用数字-1填充 [[-1 -1 -1] [-1 -1 -1]] """ print(t.eval()) 当value是一个列表时，注意列表的长度必须&lt;=shape的大小（即各维大小的乘积），否则会报错1234tensor = tf.constant([1, 2, 3, 4, 5, 6, 7], shape=[2, 3])with tf.Session() as sess: # ValueError: Too many elements provided. Needed at most 6, but received 7 print(tensor.eval()) 这是因为函数会生成一个shape大小的张量，然后用value这个列表中的值一一填充shape中的元素。这里列表大小为7，而shape大小为2*3=6，无法正确填充，所以发生了错误 而如果列表大小小于shape大小，则会用列表的最后一项元素填充剩余的张量元素123456789tensor = tf.constant([1, 2, 3, 4, 5], shape=[1, 4, 3])with tf.Session() as sess: """ [[[1 2 3] [4 5 5] [5 5 5] [5 5 5]]] """ print(tensor.eval()) 1.4. name参数name给tensor取个名1234tensor = tf.constant(0)print(tensor) # Tensor("Const:0", shape=(), dtype=int32)，默认名是Consttensor = tf.constant(0, name="t0")print(tensor) # Tensor("t0:0", shape=(), dtype=int32) 1.5. verify_shape参数verify_shape默认为False，如果修改为True的话表示检查value的形状与shape是否相符，如果不符会报错1234# value与shape都是2x3，检查结果正确tensor = tf.constant([[1, 2, 3], [4, 5, 6]], shape=[2, 3], verify_shape=True)# 出错，TypeError: Expected Tensor's shape: (3, 2), got (2,).tensor = tf.constant([1, 2], shape=[3, 2], verify_shape=True)]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorFlow 安装]]></title>
    <url>%2F2019%2F04%2F04%2FDeepLearning%2FTensorFlow%2FTensorFlow-%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1. Windows CPU版本安装1.1. pip安装1pip install TensorFlow 2. HelloWorld1234import tensorflow as tfa = tf.constant("helloworld")with tf.Session() as sess: print(sess.run(a))]]></content>
      <tags>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepLearning 概述]]></title>
    <url>%2F2019%2F04%2F04%2FDeepLearning%2FDeepLearning-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1. 什么是深度学习 深度学习是机器学习算法中的一类，源于人工神经网络的研究 深度学习广泛应用于计算机视觉、音频处理、自然语言处理等领域 深度可以理解为数据计算转换的层数]]></content>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DeepLearning Python环境搭建]]></title>
    <url>%2F2019%2F04%2F04%2FDeepLearning%2FDeepLearning-Python%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[12pip install tensorflowpip install keras TensorFlow提供了更加底层的API，好比木材和各种工具，自己DIY Keras封装度更高，以Theano、TensorFlow等底层框架为backend，好比造好的轮子 用经典网络层搭模型时，Keras更方便；动手实现和修改棋型的细节时，TensorFlow更灵活 Keras的backend可以是Theano或TensorFlow，可以在代码中动态设置backend123import osos.environ['KERAS_BACKEND'] = 'tensorflow'import keras]]></content>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 配置]]></title>
    <url>%2F2019%2F04%2F04%2FMaven%2FMaven-%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[1. 设置本地仓库路径1&lt;localRepository&gt;C:\software\development environment\apache-maven-3.5.4\repo&lt;/localRepository&gt; 2. 设置镜像设置aliyun镜像123456&lt;mirror&gt; &lt;id&gt;alimaven&lt;/id&gt; &lt;name&gt;aliyun maven&lt;/name&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;&lt;/mirror&gt;]]></content>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 命令]]></title>
    <url>%2F2019%2F04%2F04%2FMaven%2FMaven-%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[命令 描述 mvn clean 删除target目录 mvn clean 删除target目录mvn compile 编译src]]></content>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven 概述]]></title>
    <url>%2F2019%2F04%2F04%2FMaven%2FMaven-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1. 开发时遇到的问题依赖管理上的问题 每次都要到官网下载jar包，手动加入到项目，真的很麻烦。有了Maven，你只要告诉Maven需要什么依赖，它会自动去中央仓库下载 项目加入A依赖，运行还是报错（ClassNotFoundException），因为A还依赖B，你还得再去找B。有了Maven，你只要告诉Maven需要A，它会自动加入B 有很多工程会用到相同的jar包，浪费存储空间。有了Maven，依赖统一在仓库中保存一份，有需要时去引用仓库中的jar即可，并不需要将jar复制到工程中，避免了空间浪费 项目使用A依赖，后来发现A中有BUG。将A的BUG修复之后，必须把A重新编译成jar包，导入到项目中，覆盖原来的A依赖。有了Maven，将A的BUG修复之后，只需要重新将A编译成jar包，发布到仓库中，项目会自动更新A依赖 项目构建上的问题 一个项目只有一个工程，模块通过package来划分（例如模块A的代码都放在com.example.module1，模块B的代码都放在com.example.module2）。小项目还好，大项目一个package下就会有非常多的代码，显得臃肿，难以管理。有了Maven，每一个模块对应一个工程，不同的项目组负责不同的工程，这样有利于分工，项目也变得轻巧 2. Maven是什么Maven是基于POM（Project Object Model工程对象模型），通过一小段描述来对项目的代码、报告、文件进管理的工具。 Maven是一个跨平台的项目管理工具，它是使用java开发的，依赖于JDK Maven主要有两大作用：管理依赖，构建项目 2.1. 什么是依赖依赖指的就是jar包 2.2. 什么是项目构建构建一个项目不是指创建一个项目。在一些IDE中你肯定会看到 Build Project 和 New Project，构建项目对应Build，创建项目对应New/Create 项目构建没有严格的定义，主要是指编译、测试、打包、部署的整个过程，整个过程也称为项目构建的生命周期。 对于小项目，项目构建不一定要有以上几个过程，但是编译肯定是有的。比如你写一个HelloWorld，你执行一个Build操作，此时项目构建主要就是指编译。 对于大项目，还可能包括其它的过程。如清理、生成测试报告等等 Maven项目构建包含以下几个过程 清理：删除以前的编译结果，为重新编译做准备 编译：将Java源程序编译为字节码文件 测试：针对项目中的关键点进行测试，确保在迭代开发过程正确性。这里的测试是指junit自动化测试。你提前把junit测试代码准备好，Maven在构建时会自动调用这些测试程序。 报告：在每一次测试后以标准的格式记录和展示结果 打包：将工程打包。普通Java工程对应jar包，Web工程对应war包 安装：将打包结果（jar包或war包）安装到本地仓库中。这是Maven中特有的概念 部署：将打包结果部署到远程仓库，或将war包部署到服务器上运行 2.3. 自动化构建Eclipse/IDEA等IDE都可以找到对应的操作，只是不太标准。我们明明可以通过IDE手动完成这些步骤，为什么还要用Maven呢？ 如果手动去操作，有时我们可能一整天会把时间都浪费在项目构建上。而这些操作都可以交给Maven，你只需要执行一个命令，Maven会从头到尾完成自动化构建 3. Maven POMPOM（Project Object Model）：项目对象模型。将Java工程相关信息封装为对象，作为便于操作和管理的模型。 pom.xml是Maven工程的核心配置文件。有没有pom.xml，决定了一个Java工程是不是Maven工程。只要给Java工程根目录加入pom.xml，就转变为Maven工程。学习Maven，就是学习pom.xml中的配置 4. Maven工程约定的目录结构1234567891011MavenTest: // 项目名├─pom.xml // 当前maven工程的描述文件├─src // 源码目录│ ├─main // 主程序目录│ │ ├─java // 存放java源文件│ │ ├─resources // 存放配置文件│ │ └─webapp // 存放web资源（web工程才有webapp目录）│ └─test // 测试程序目录│ ├─java // 存放测试的java源文件│ └─resources // 存放测试用到的配置文件└─target // class文件、报告等信息存储的地方 5. Maven坐标在数学中，建立好坐标系，给出一个坐标，就可以唯一确定一个点 Maven借鉴了数学中坐标的思想，也提出了坐标的概念，用来唯一标识一个Maven项目，或者Maven依赖，或者是Maven插件 Maven坐标由三部分组成： groupId：公司或组织名 + 模块或项目名。说明该jar包属于哪个公司的哪个项目 artifactId：groupId下的子项目名。 version：版本号 示例：123456&lt;!-- 组织名: org.apache，项目名: hadoop --&gt;&lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;&lt;!-- 子项目名 --&gt;&lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;&lt;!-- 版本号 --&gt;&lt;version&gt;2.6.5&lt;/version&gt; 根据坐标可以直接到仓库找到对应的依赖 6. Maven仓库6.1. 仓库分类本地仓库：自己维护 远程仓库 私服：架设在局域网内的Maven仓库服务，供局域网内的Maven用户使用。当Maven用户需要下载依赖时，会到私服寻找对应的依赖。如果私服上不存在该依赖，则由私服去下载依赖，再为Maven用户的下载请求提供服务。一般公司都会搭建自己的Maven私服，自己去维护。 中央仓库：Apache Maven团队维护，为全世界所有的Maven工程提供服务 中央仓库镜像：如Aliyun Maven 6.2. 仓库保存的内容仓库保存的内容就是Maven工程，具体包括以下几种工程 Maven插件 jar包 7. Maven插件Maven只是定义了抽象的生命周期，具体的工作由特定的插件来完成，而插件本身并不包含在Maven核心程序中。当我们执行某些构建命令，用到某些插件时，Maven会去仓库中查找，如果没有就到中央仓库下载。]]></content>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring注解驱动 @PostConstruct和@PreDestroy]]></title>
    <url>%2F2019%2F04%2F04%2FSpring%2FSpring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8-PostConstruct%E5%92%8C-PreDestroy%2F</url>
    <content type="text"><![CDATA[使用JSR注解给Bean设置生命周期方法 @PostConstruct: 在Bean创建并属性赋值完成之后，执行初始化方法 @PreDestroy: 在Bean销毁之前执行 1. 使用编写Bean12345678910111213public class Car &#123; public Car() &#123; System.out.println("car constructor"); &#125; @PostConstruct // JSR public void init() &#123; System.out.println("car init"); &#125; @PreDestroy // JSR public void destroy() &#123; System.out.println("car destory"); &#125;&#125; 注册Bean1234567@Configurationpublic class BeanConfig &#123; @Bean public Car car() &#123; return new Car(); &#125;&#125; 测试12345678@Testpublic void test() &#123; AnnotationConfigApplicationContext ioc = new AnnotationConfigApplicationContext(BeanConfig.class); Car car1 = ioc.getBean(Car.class); Car car2 = ioc.getBean(Car.class); System.out.println(car1 == car2); ioc.close();&#125;s]]></content>
      <tags>
        <tag>Spring注解驱动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring注解驱动 InitializingBean和DisposableBean]]></title>
    <url>%2F2019%2F04%2F04%2FSpring%2FSpring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8-InitializingBean%E5%92%8CDisposableBean%2F</url>
    <content type="text"><![CDATA[1. InitializingBean和DisposableBean接口使用流程Bean实现生命周期接口 12345678910111213141516/** 实现InitializingBean和DisposableBean */public class Car implements InitializingBean, DisposableBean &#123; public Car() &#123; System.out.println("car constructor"); &#125; /** 在Bean的属性被注入之后会被调用，就相当于init方法 */ @Override public void afterPropertiesSet() throws Exception &#123; System.out.println("car init"); &#125; /** 销毁方法 */ @Override public void destroy() throws Exception &#123; System.out.println("car destory"); &#125;&#125; 注册Bean1234567@Configurationpublic class BeanConfig &#123; @Bean public Car car() &#123; return new Car(); &#125;&#125; 测试12345678@Testpublic void test() &#123; AnnotationConfigApplicationContext ioc = new AnnotationConfigApplicationContext(BeanConfig.class); Car car1 = ioc.getBean(Car.class); Car car2 = ioc.getBean(Car.class); System.out.println(car1 == car2); ioc.close();&#125;]]></content>
      <tags>
        <tag>Spring注解驱动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring注解驱动 @Bean指定初始化和销毁方法]]></title>
    <url>%2F2019%2F04%2F04%2FSpring%2FSpring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8-Bean%E6%8C%87%E5%AE%9A%E5%88%9D%E5%A7%8B%E5%8C%96%E5%92%8C%E9%94%80%E6%AF%81%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Bean的生命周期，就是指Bean从创建、初始化，到销毁的过程 1. @Bean指定init和destory流程编写Bean12345678910111213public class Car &#123; public Car() &#123; System.out.println("car constructor"); &#125; /** 初始化方法 */ public void init() &#123; System.out.println("car init"); &#125; /** 销毁方法 */ public void destory() &#123; System.out.println("car destory"); &#125;&#125; 注册Bean12345678@Configurationpublic class BeanConfig &#123; // @Beam指定init方法和destory方法 @Bean(initMethod="init", destroyMethod="destory") public Car car() &#123; return new Car(); &#125;&#125; 测试方法123456@Testpublic void test() &#123; AnnotationConfigApplicationContext ioc = new AnnotationConfigApplicationContext(BeanConfig.class); // 销毁容器，才能看到Bean的destory被调用 ioc.close();&#125; 2. 容器不执行多实例Bean的销毁方法 单实例Bean，容器会执行Bean的init和destory 多实例Bean，容器只会执行Bean的init，但不管Bean的销毁 注册多实例Bean12345678@Configurationpublic class BeanConfig &#123; @Scope("prototype") // 多实例 @Bean(initMethod="init", destroyMethod="destory") public Car car() &#123; return new Car(); &#125;&#125; 执行测试方法，只看到构造方法和init方法被调用，destory没有被调用1234567@Testpublic void test() &#123; AnnotationConfigApplicationContext ioc = new AnnotationConfigApplicationContext(BeanConfig.class); Car car1 = ioc.getBean(Car.class); Car car2 = ioc.getBean(Car.class); ioc.close();&#125;]]></content>
      <tags>
        <tag>Spring注解驱动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring注解驱动 Factory注册组件]]></title>
    <url>%2F2019%2F04%2F04%2FSpring%2FSpring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8-Factory%E6%B3%A8%E5%86%8C%E7%BB%84%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[1. 使用FactoryBean注册Bean编写工厂Bean12345678910111213141516171819202122package demo.spring.bean;import org.springframework.beans.factory.FactoryBean;public class PersonFactoryBean implements FactoryBean&lt;Person&gt; &#123; /** 返回的对象会被添加到容器中 */ @Override public Person getObject() throws Exception &#123; System.out.println("getObject()"); return new Person(); &#125; @Override public Class&lt;?&gt; getObjectType() &#123; System.out.println("getObjectType()"); return Person.class; &#125; /** Bean是否单例 */ @Override public boolean isSingleton() &#123; return true; &#125;&#125; 注册工厂Bean1234567@Configurationpublic class BeanConfig &#123; @Bean // 注册工厂Bean public PersonFactoryBean personFactoryBean() &#123; return new PersonFactoryBean(); &#125;&#125; 测试，获取时使用工厂Bean的name，发现得到的不是工厂Bean，而是工厂Beanc调用getObject()得到的对象1234567@Testpublic void test() &#123; ApplicationContext ioc = new AnnotationConfigApplicationContext(BeanConfig.class); // 获取时使用用工厂Bean的name，得到的Bean是工厂创建的对象，而不是工厂Bean本身 Object bean = ioc.getBean("personFactoryBean"); System.out.println(bean.getClass());&#125; 2. 获取工厂Bean本身用工厂Bean的name，获取到的却不是工厂Bean。如果我们就想要获取工厂Bean，就要加一个&amp;前缀1234567@Testpublic void test() &#123; ApplicationContext ioc = new AnnotationConfigApplicationContext(BeanConfig.class); // 加上&amp;前缀，得到工厂Bean Object bean = ioc.getBean("&amp;personFactoryBean"); System.out.println(bean.getClass());&#125;]]></content>
      <tags>
        <tag>Spring注解驱动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring注解驱动 @Import注册组件]]></title>
    <url>%2F2019%2F04%2F04%2FSpring%2FSpring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8-Import%E6%B3%A8%E5%86%8C%E7%BB%84%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[1. Bean类对象作为参数1.1. 注册单个组件1234@Configuration@Import(Person.class) // 注册Person，id默认是Person的全类名public class BeanConfig &#123;&#125; 1.2. 注册多个组件1234@Configuration@Import(&#123;Person.class, Car.class&#125;)public class BeanConfig &#123;&#125; 2. ImportSelector作为参数编写ImportSelector类123456789101112/** 实现ImportSelector接口 */public class MyImportSelector implements ImportSelector &#123; /** * AnnotationMetadata: 标识@Import的类的所有注解信息 * 返回值: 要注册到容器的Bean的全类名数组 */ @Override public String[] selectImports(AnnotationMetadata importingClassMetadata) &#123; // 要注册的Bean的全类名，默认id就是全类名 return new String[] &#123;"demo.spring.bean.Teacher", "demo.spring.bean.Student"&#125;; &#125;&#125; 配置类12345@Configuration//@Import的参数可以是要注册的Bean的类对象，也可以是ImportSelector的类对象@Import(&#123;Person.class, Car.class, MyImportSelector.class&#125;) public class BeanConfig &#123;&#125; 3. ImportBeanDefinitionRegister作为参数编写ImportBeanDefinitionRegistrar1234567891011121314151617/** 实现ImportBeanDefinitionRegistrar */public class MyImportBeanDefinitionRegistrar implements ImportBeanDefinitionRegistrar &#123; /** * AnnotationMetadata: 当前类的注解信息 * BeanDefinitionRegistry: Bean的注册类 */ @Override public void registerBeanDefinitions(AnnotationMetadata importingClassMetadata, BeanDefinitionRegistry registry) &#123; // 如果容器中有name为person的Bean组件 if (registry.containsBeanDefinition("person")) &#123; // 注册Book, name为book BeanDefinition beanDefinition = new RootBeanDefinition(Book.class); registry.registerBeanDefinition("book", beanDefinition); &#125; &#125;&#125;]]></content>
      <tags>
        <tag>Spring注解驱动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring注解驱动 @Conditional按照条件注册Bean]]></title>
    <url>%2F2019%2F04%2F03%2FSpring%2FSpring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8-Conditional%E6%8C%89%E7%85%A7%E6%9D%A1%E4%BB%B6%E6%B3%A8%E5%86%8CBean%2F</url>
    <content type="text"><![CDATA[1. @Conditional使用流程编写条件类 1234567891011121314151617181920212223/** 实现Condition接口 */public class MyCondition implements Condition &#123; /** * 条件方法，满足则创建Bean * ConditionContext: 判断条件能使用的上下文（环境） * AnnotatedTypeMetadata: 注解信息 */ @Override public boolean matches(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; // 能获取ioc的Bean工厂 ConfigurableListableBeanFactory beanFactory = context.getBeanFactory(); // 能获取类加载器 ClassLoader classLoader = context.getClassLoader(); // 能获取环境信息 Environment environment = context.getEnvironment(); // 能获取Bean定义的注册类 BeanDefinitionRegistry registry = context.getRegistry(); // 判断当前系统是不是Windows，是则满足条件 String property = environment.getProperty("os.name"); return property.contains("Windows"); &#125;&#125; 配置类12345678@Configurationpublic class BeanConfig &#123; @Bean @Conditional(&#123;MyCondition.class&#125;) // 满足条件才注册 public Person person() &#123; return new Person("jack", 10); &#125;&#125; 测试方法 123456@Testpublic void test() &#123; ApplicationContext ioc = new AnnotationConfigApplicationContext(BeanConfig.class); String[] names = ioc.getBeanDefinitionNames(); Arrays.stream(names).forEach(System.out::println);&#125;]]></content>
      <tags>
        <tag>Spring注解驱动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring注解驱动 @Lazy组件懒加载]]></title>
    <url>%2F2019%2F04%2F03%2FSpring%2FSpring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8-Lazy%E7%BB%84%E4%BB%B6%E6%87%92%E5%8A%A0%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[懒加载是针对单实例Bean的。因为单实例Bean默认是在容器创建时就会被实例化，使用懒加载之后，只有在容器第1次获取Bean时才会实例化。多实例Bean本身就是”Lazy”的，所以@Lazy没有意义 1. @Lazy使用1234567@Lazy@Beanpublic Person person() &#123; // 容器第1次获取person，才会实例化 System.out.println("给容器添加person"); return new Person("jack", 10);&#125;]]></content>
      <tags>
        <tag>Spring注解驱动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring注解驱动 @Scope设置组件作用域]]></title>
    <url>%2F2019%2F04%2F03%2FSpring%2FSpring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8-Scope%E8%AE%BE%E7%BD%AE%E7%BB%84%E4%BB%B6%E4%BD%9C%E7%94%A8%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[1. @Scope使用流程1234567891011121314151617@Configurationpublic class BeanConfig &#123; @Scope("prototype") // 多实例 @Bean public Person person() &#123; // 多实例，容器每获取一次Person，就会调用该方法 System.out.println("给容器添加Person"); return new Person("jack", 10); &#125; @Scope("singleton") // 单实例 @Bean public Car car() &#123; // 单实例，容器创建时，就会调用该方法 System.out.println("给容器添加Car"); return new Car(); &#125;&#125; 测试方法1234567891011@Testpublic void test() &#123; ApplicationContext ioc = new AnnotationConfigApplicationContext(BeanConfig.class); // 每获取一次Person Person person1 = (Person) ioc.getBean("person"); Person person2 = (Person) ioc.getBean("person"); Car car1 = (Car) ioc.getBean("car"); Car car2 = (Car) ioc.getBean("car"); System.out.println(person1 == person2); System.out.println(car1 == car2);&#125;]]></content>
      <tags>
        <tag>Spring注解驱动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MongoDB 安装]]></title>
    <url>%2F2019%2F04%2F02%2FMongoDB%2FMongoDB-%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1. MongoDB Ubuntu 安装1.1. 下载版本说明 MongoDB的版本偶数版本为稳定版，奇数版本为开发版。例如3.2.x、3.4.x、3.6.x是稳定版。 MongoDB对于32位系统支持不佳，所以 3.2版本以后没有再对32位系统的支持。 MongoDB Download Center官网下载Community版本。 Version: 选择版本 OS: 选择Ubuntu16.04 Package: 选择tgz 解压12tar -zxvf mongodb-linux-x86_64-ubuntu1604-3.6.10.gz -C /usr/localmv /usr/local/mongodb-linux-x86_64-ubuntu1604-3.6.10 /usr/local/mongodb 创建data和log目录，用于存放mongodb数据和日志1mkdir /usr/local/mongodb/&#123;data,log&#125; 1.2. 运行服务指定参数运行 —dbpath指定数据存放路径 —logpath指定日志存放路径（必须是一个文件） —port 27017 指定端口 —bind_ip 0.0.0.0 指定IP —fork 后台运行 1/usr/local/mongodb/bin/mongod --dbpath=/usr/local/mongodb/data/ --logpath=/usr/local/mongodb/log/mongodb.log 也可以将运行参数先写到配置文件，再指定配置文件运行服务。首先创建配置文件mongodb.conf1234567891011121314# 数据存放路径 dbpath=/usr/local/mongodb/data# 日志输出文件logpath=/usr/local/mongodb/log/mongodb.log# 端口，默认27017port=27017# 设置在后台运行fork=true# IPbind_ip=0.0.0.0# 启用日志，默认truejournal=true# 启用用户认证，默认false# auth=true 指定配置文件，并运行服务1/usr/local/mongodb/bin/mongod --config /usr/local/mongodb/mongodb.conf 1.3. 关闭服务方式1：通过mongo shell1234&gt; use adminswitched to db admin&gt; db.shutdownServer()server should be down... 方式2：kill &lt;pid&gt; 发送SIGTERM信号，关闭守护进程。注意kill -9 &lt;pid&gt;发送SIGKILL信号，会强制关闭mongo，如果mongo没开启日志（—journal），可能会造成数据损失。 12345678$ netstat -ntlpActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 0.0.0.0:22 0.0.0.0:* LISTEN 1351/sshd tcp 0 0 0.0.0.0:27017 0.0.0.0:* LISTEN 1801/mongod tcp6 0 0 :::22 :::* LISTEN 1351/sshd $ kill 1801 方式3：通过客户端的shutdown指令（推荐）1bin/mongod -shutdown -config mongodb.conf]]></content>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring注解驱动 @ComponentScan组件扫描]]></title>
    <url>%2F2019%2F04%2F02%2FSpring%2FSpring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8-ComponentScan%E7%BB%84%E4%BB%B6%E6%89%AB%E6%8F%8F%2F</url>
    <content type="text"><![CDATA[1. @ComponentScan流程1.1. 编写Bean12345package demo.spring.bean;import org.springframework.stereotype.Component;@Componentpublic class Car &#123;&#125; 12345package demo.spring.bean;import org.springframework.stereotype.Component;@Componentpublic class Person &#123;&#125; 1.2. 组件扫描1234@Configuration@ComponentScan("demo.spring") // 要扫描的包及其子包public class BeanConfig &#123;&#125; 1.3. 测试方法1234567@Testpublic void test() &#123; ApplicationContext ioc = new AnnotationConfigApplicationContext(BeanConfig.class); // 输出容器中所有的组件名称 String[] names = ioc.getBeanDefinitionNames(); Arrays.stream(names).forEach(System.out::println);&#125; 2. 扫描多个包方式1：使用多个@ComponentScan注解重复注解是Java8的新特性12345@Configuration@ComponentScan("demo.spring.service")@ComponentScan("demo.spring.dao")public class BeanConfig &#123;&#125; 方式2：使用@ComponentScans123456@Configuration@ComponentScans(&#123; @ComponentScan("demo.spring.service"), @ComponentScan("demo.spring.dao") &#125;)public class BeanConfig &#123;&#125; 3. @ComponentScan的属性3.1. excludeFilters排除扫描123456@Configuration@ComponentScan(value = "demo.spring", excludeFilters = &#123; @Filter(type = FilterType.ANNOTATION, classes = &#123; Controller.class, Service.class &#125;), @Filter(type = FilterType.ASSIGNABLE_TYPE, classes = &#123; Person.class, Car.class &#125;) &#125;)public class BeanConfig &#123;&#125; 3.2. includeFilters包含扫描1234567@Configuration// useDefaultFilters=false禁用@ComponentScan的默认规则（默认规则就是扫描所有）@ComponentScan(value = "demo.spring", useDefaultFilters = false, includeFilters = &#123; @Filter(type = FilterType.ANNOTATION, classes = &#123; Controller.class, Service.class &#125;), @Filter(type = FilterType.ASSIGNABLE_TYPE, classes = &#123; Person.class, Car.class &#125;) &#125;)public class BeanConfig &#123;&#125;]]></content>
      <tags>
        <tag>Spring注解驱动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring注解驱动 @Configuration和@Bean注册组件]]></title>
    <url>%2F2019%2F04%2F01%2FSpring%2FSpring%E6%B3%A8%E8%A7%A3%E9%A9%B1%E5%8A%A8-Configuration%E5%92%8C-Bean%E6%B3%A8%E5%86%8C%E7%BB%84%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[1. @Configuration和@Bean注册组件流程1.1. 依赖12345678910111213141516171819202122232425262728293031&lt;properties&gt; &lt;spring.version&gt;5.1.5.RELEASE&lt;/spring.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!-- junit --&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;!-- lombok --&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.18.6&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- spring core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- spring test --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.2. 编写Bean12345678// 不需要设置@Component组件注解@Data@NoArgsConstructor@AllArgsConstructorpublic class Person &#123; private String name; private Integer age;&#125; 1.3. 编写Bean配置类以前写是Bean配置文件，现在改为Bean配置类1234567@Configuration // 标识当前类是一个配置类，配置类==配置文件public class BeanConfig &#123; @Bean // 给容器注册一个Bean，方法返回类型作为Bean的类型，方法名作为Bean的id public Person person() &#123; return new Person("张三", 20); &#125;&#125; 1.4. 编写测试方法以前创建ClassPathXmlApplicationContext，参数是指定配置文件。现在创建AnnotationConfigApplicationContext，参数是指定配置类123456789@Testpublic void test() &#123; // 创建AnnotationConfigApplicationContext，指定配置类class ApplicationContext ioc = new AnnotationConfigApplicationContext(BeanConfig.class); Person person = ioc.getBean(Person.class); String[] names = ioc.getBeanNamesForType(Person.class); System.out.println(person); System.out.println(Arrays.toString(names)); // 查看Person在容器中的name&#125; 2. 给Bean命名@Bean注解的value或name属性可以给Bean命名123@Bean("person1") @Bean(value = "person1")@Bean(name = "person1") 命名示例1234567@Configurationpublic class BeanConfig &#123; @Bean("person1") // 给Bean命名 public Person person() &#123; return new Person("张三", 20); &#125;&#125;]]></content>
      <tags>
        <tag>Spring注解驱动</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC 将数据传递到页面]]></title>
    <url>%2F2019%2F04%2F01%2FSpringMVC%2FSpringMVC-%E5%B0%86%E6%95%B0%E6%8D%AE%E4%BC%A0%E9%80%92%E5%88%B0%E9%A1%B5%E9%9D%A2%2F</url>
    <content type="text"><![CDATA[1. BindingAwareModelMap（Model/ModelMap等等）传递数据BindingAwareModelMap的继承关系如下 可以直接用BindingAwareModelMap作为请求参数，也可以用它的父类ModelMap / LinkedHashMap / Map等作为请求参数。SpringMVC会自动传入，实际类型就是BindingAwareModelMap。 从另一个继承树中也可得知，Model也可以作为请求参数。 123456789/** Map&lt;String, Object&gt; map作为请求参数 */@GetMapping("/test")public String test(Map&lt;String, Object&gt; map) throws Exception &#123; System.out.println(map.getClass()); // BindingAwareModelMap // map中的数据会放到请求域中 map.put("username", "jack"); map.put("intList", Arrays.asList(1, 2, 3)); return "index";&#125; 123456789@GetMapping("/test")public String test(ModelMap map) throws Exception &#123; System.out.println(map.getClass()); // BindingAwareModelMap // 可以用put()或者addAttribute() map.put("username", "jack"); map.put("intList", Arrays.asList(1, 2, 3)); map.addAttribute("msg", "你好"); return "index";&#125; 1234567@GetMapping("/test")public String test(Model model) throws Exception &#123; System.out.println(map.getClass()); // BindingAwareModelMap model.addAttribute("username", "jack"); model.addAttribute("intList", Arrays.asList(1, 2, 3)); return "index";&#125; 可以看到无论用哪个参数，实际对象都是BindingAwareModelMap，最后值都会放入request域中 2. ModelAndView传递数据ModelAndView 包含视图信息（视图名） 包含模型数据，传递给视图，数据也是放到request域中 12345678910@GetMapping("/test")public ModelAndView test() &#123; // 构造方法的参数就是视图名，拼接得到/WEB-INF/jsp/index.jsp ModelAndView mv = new ModelAndView("index"); // 设置数据 mv.addObject("username", "jack"); mv.addObject("intList", Arrays.asList(1, 2, 3)); // 返回mv return mv;&#125; 创建ModelAndView时，也可以用空参构造，但是还要用setViewName()设置视图12345678@GetMapping("/test")public ModelAndView test() &#123; ModelAndView mv = new ModelAndView(); mv.setViewName("index"); mv.addObject("username", "jack"); mv.addObject("intList", Arrays.asList(1, 2, 3)); return mv;&#125;]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC 乱码解决]]></title>
    <url>%2F2019%2F04%2F01%2FSpringMVC%2FSpringMVC-%E4%B9%B1%E7%A0%81%E8%A7%A3%E5%86%B3%2F</url>
    <content type="text"><![CDATA[1. GET请求乱码1.1. 终极解决方案：修改server.xml的URIEncoding如果使用tomcat插件，在&lt;configuration&gt;标签中设置&lt;uriEncoding&gt;属性即可123456789101112131415&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;port&gt;8080&lt;/port&gt; &lt;path&gt;/&lt;/path&gt; &lt;!-- 设置uriEncoding --&gt; &lt;uriEncoding&gt;UTF-8&lt;/uriEncoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 如果是tomcat服务器，则修改conf/server.xml 12345&lt;!-- 找到设置port="8080"的那个标签，添加URIEncoding="UTF-8"即可 --&gt;&lt;Connector port="8080" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" URIEncoding="UTF-8" /&gt; 2. POST请求乱码2.1. 方式1：原生API1234567@GetMapping("/test")public void haha(HttpServletRequest request) throws Exception &#123; // 在第1次获取请求参数之前设置 request.setCharacterEncoding("UTF-8"); // 再获取参数，就不会乱码了 String username = request.getParameter("username");&#125; 2.2. 方式2：CharacterEncodingFilter使用原生API，每一个POST方法都要写一遍request.setCharacterEncoding(&quot;UTF-8&quot;)，太麻烦。干脆统一写一个Filter，所有的请求先统一执行一遍该方法，保证编码能被解析。 这个Filter不用我们亲自写，SpringMVC内置了CharacterEncodingFilter，这个过滤器不仅能设置POST请求的编码，还能设置响应的编码 12345678910111213141516171819&lt;filter&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;!-- 设置请求数据的编码为UTF-8 --&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;!-- 设置响应数据的编码为UTF-8（该编码取决于上面设置的encoding属性） --&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;!-- 拦截所有请求 --&gt; &lt;filter-name&gt;characterEncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 3. CharacterEncodingFilter的设置顺序最好放在首位CharacterEncodingFilter会设置POST请求的编码1request.setCharacterEncoding(encoding) 设置POST请求的编码，这条语句应该要写在获取参数之前，这样才能保证读取的参数编码格式是正确的1234// 先设置编码request.setCharacterEncoding(encoding)// 再获取参数request.getParameter("username"); 有的过滤器，如HiddenHttpMethodFilter，查看源码可知有通过request.getParameter()获取参数。如果HiddenHttpMethodFilter配置顺序在CharacterEncodingFilter之前，就会出现先获取参数，再设置POST请求编码的情况。所以为了避免这种情况发生，在web.xml中，CharacterEncodingFilter应该配置在其它Filter之前 4. CharacterEncodingFilter源码分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081public class CharacterEncodingFilter extends OncePerRequestFilter &#123; /** 在web.xml中配置encoding，就会设置该值 */ @Nullable private String encoding; /** * 在web.xml中设置forceEncoding的值，会同时设置forceRequestEncoding/forceResponseEncoding */ /** request是否强制使用encoding编码 */ private boolean forceRequestEncoding = false; /** response是否强制使用encoding编码 */ private boolean forceResponseEncoding = false; public CharacterEncodingFilter() &#123; &#125; public CharacterEncodingFilter(String encoding) &#123; this(encoding, false); &#125; public CharacterEncodingFilter(String encoding, boolean forceEncoding) &#123; this(encoding, forceEncoding, forceEncoding); &#125; public CharacterEncodingFilter(String encoding, boolean forceRequestEncoding, boolean forceResponseEncoding) &#123; Assert.hasLength(encoding, "Encoding must not be empty"); this.encoding = encoding; this.forceRequestEncoding = forceRequestEncoding; this.forceResponseEncoding = forceResponseEncoding; &#125; public void setEncoding(@Nullable String encoding) &#123; this.encoding = encoding; &#125; @Nullable public String getEncoding() &#123; return this.encoding; &#125; public void setForceEncoding(boolean forceEncoding) &#123; this.forceRequestEncoding = forceEncoding; this.forceResponseEncoding = forceEncoding; &#125; public void setForceRequestEncoding(boolean forceRequestEncoding) &#123; this.forceRequestEncoding = forceRequestEncoding; &#125; public boolean isForceRequestEncoding() &#123; return this.forceRequestEncoding; &#125; public void setForceResponseEncoding(boolean forceResponseEncoding) &#123; this.forceResponseEncoding = forceResponseEncoding; &#125; public boolean isForceResponseEncoding() &#123; return this.forceResponseEncoding; &#125; @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; String encoding = getEncoding(); if (encoding != null) &#123; if (isForceRequestEncoding() || request.getCharacterEncoding() == null) &#123; // 设置POST请求的编码 request.setCharacterEncoding(encoding); &#125; if (isForceResponseEncoding()) &#123; // 设置响应编码 response.setCharacterEncoding(encoding); &#125; &#125; // 放行所有请求 filterChain.doFilter(request, response); &#125;&#125;]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC POJO数据绑定]]></title>
    <url>%2F2019%2F04%2F01%2FSpringMVC%2FSpringMVC-POJO%E6%95%B0%E6%8D%AE%E7%BB%91%E5%AE%9A%2F</url>
    <content type="text"><![CDATA[1. 简单POJO数据绑定流程编写POJO12345@Datapublic class Person &#123; private String name; private Integer age;&#125; 12345// 访问/test?name=jack&amp;age=10，参数名与POJO属性名只要对应，就能绑定@GetMapping("/test")public void test(Person person) &#123; System.out.println(person);&#125; 2. 级联POJO数据绑定流程级联POJO：POJO里有另一个POJO 123456789101112@Datapublic class Person &#123; private String name; private Integer age; private Car car;&#125;@Datapublic class Car &#123; private String name; private Double price;&#125; 12345// 请求URL /test?name=jack&amp;age=10&amp;car.name=benz&amp;car.price=500.0@GetMapping("/test")public void test(Person person) &#123; System.out.println(person);&#125;]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC @CoookieValue获取请求Cookie]]></title>
    <url>%2F2019%2F04%2F01%2FSpringMVC%2FSpringMVC-CoookieValue%E8%8E%B7%E5%8F%96%E8%AF%B7%E6%B1%82Cookie%2F</url>
    <content type="text"><![CDATA[1. @CookieValue@CookieValue与@RequestParam使用方式一致 12345678910@GetMapping("/test")public void test( // 获取key="JSESSIONID"的cookie @CookieValue("JSESSIONID") String jsessionId1, @CookieValue(value="JSESSIONID", required=false) String jsessionId2, @CookieValue(value="JSESSIONID", defaultValue="haha") String jsessionId3) &#123; System.out.println(jsessionId1); System.out.println(jsessionId2); System.out.println(jsessionId3);&#125;]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC @RequestHeader获取请求头参数]]></title>
    <url>%2F2019%2F04%2F01%2FSpringMVC%2FSpringMVC-RequestHeader%E8%8E%B7%E5%8F%96%E8%AF%B7%E6%B1%82%E5%A4%B4%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[1. @RequestHeader@RequestHeader和@RequestParam很像 @RequestHeader获取请求头参数，@RequestParam获取请求参数 两个注解的属性都一样，required/defaultValue含义相同 123456789@GetMapping("/test")public void test( @RequestHeader(value="accept", defaultValue="zh-CN,zh;q=0.9") String accept, @RequestHeader(value="Accept-Language",required=false) String acceptLanguage, @RequestHeader("Referer") String referer) &#123; System.out.println(accept); System.out.println(acceptLanguage); System.out.println(referer);&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC @RequestParam获取请求参数]]></title>
    <url>%2F2019%2F04%2F01%2FSpringMVC%2FSpringMVC-RequestParam%E8%8E%B7%E5%8F%96%E8%AF%B7%E6%B1%82%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[1. @RequestParam默认情况下，请求参数与形参名对应，就能实现数据绑定 1234// 请求URL: /user?username=123@PostMapping("/user")public void saveUser(String username) &#123;&#125; 如果请求参数与形参不对应，又想获取对应的值，需要使用@RequestParam 1234// 请求URL: /user?uname=123@PostMapping("/user")public void saveUser(@RequestParam("uname") String username) &#123;&#125; 1.1. required属性默认情况下，required=true，即请求必须传入指定参数，如果没有，则返回400状态码1234// 请求URL: /user?aa=123，返回400@PostMapping("/user")public void saveUser(@RequestParam("uname") String username) &#123;&#125; 此时可以设置required=false，如果获取不到参数，值就为null1234@PostMapping("/user")public void saveUser(@RequestParam(value="uname", required=false) String username) &#123; System.out.println(username == null);&#125; 1.2. defaultValue属性如果获取不到值，则设置为默认值123456@PostMapping("/user")public void saveUser(@RequestParam(defaultValue="1") Integer userId, @RequestParam(defaultValue="jack") String username) &#123; System.out.println(userId); System.out.println(username);&#125;]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC RESTful]]></title>
    <url>%2F2019%2F04%2F01%2FSpringMVC%2FSpringMVC-RESTful%2F</url>
    <content type="text"><![CDATA[1. RESTful CRUD实现流程 参数传递形式 RESTful风格 /getUser?id=1 GET /user/1 /updateUser?id=1 PUT /user/1 /deleteUser?id=1 DELETE /user/1 /saveUser POST /user/1 1.1. 编写HTML页面只能发送GET/POST请求。为了实现RESTful，SpringMVC提供了一种机制：页面可以发送POST请求，通过”_method”参数设置实际要发送的请求类型，SpringMVC会根据”_method”的值转变为对应的请求进行处理 123456789101112&lt;a href="/user/1"&gt;查询&lt;/a&gt;&lt;form action="/user" method="post"&gt; &lt;button type="submit"&gt;添加&lt;/button&gt;&lt;/form&gt;&lt;form action="/user/1" method="post"&gt; &lt;input type="hidden" name="_method" value="delete"&gt; &lt;button type="submit"&gt;删除&lt;/button&gt;&lt;/form&gt;&lt;form action="/user/1" method="post"&gt; &lt;input type="hidden" name="_method" value="put"&gt; &lt;button type="submit"&gt;更新&lt;/button&gt;&lt;/form&gt; 1.2. 设置HiddenHttpMethodFilter（实现请求类型转变的原理）浏览器 form 表单只支持 GET 与 POST 请求，而DELETE、PUT 等 method 并不支持，Spring3.0 添加了一个过滤器，可以将这些请求转换为标准的 http 方法，使得支持 GET、POST、PUT 与 DELETE 请求。 SpringMVC有一个内置的HiddenHttpMethodFilter，可以根据请求参数中的”_method”的值，转化为对应的请求方式，并进行处理 编辑web.xml，添加Filter123456789&lt;filter&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.HiddenHttpMethodFilter&lt;/filter-class&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;HiddenHttpMethodFilter&lt;/filter-name&gt; &lt;!-- 拦截所有请求 --&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 1.3. 编写Controller12345678910111213141516171819202122@Controllerpublic class UserController &#123; @GetMapping("/user/&#123;id&#125;") public void getUser(@PathVariable Integer id) &#123; System.out.println("查询id=" + id); &#125; @PutMapping("/user/&#123;id&#125;") public void updateUser(@PathVariable Integer id) &#123; System.out.println("更新id=" + id); &#125; @DeleteMapping("/user/&#123;id&#125;") public void deleteUser(@PathVariable Integer id) &#123; System.out.println("删除id=" + id); &#125; @PostMapping("/user") public void saveUser() &#123; System.out.println("添加"); &#125;&#125; 2. HiddenHttpMethodFilter源码分析（5.1.5.RELEASE）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class HiddenHttpMethodFilter extends OncePerRequestFilter &#123; /** 允许的请求方法：PUT/DELETE/PATCH */ private static final List&lt;String&gt; ALLOWED_METHODS = Collections .unmodifiableList(Arrays.asList(HttpMethod.PUT.name(), HttpMethod.DELETE.name(), HttpMethod.PATCH.name())); /** 默认method参数的名称 Default method parameter: &#123;@code _method&#125;. */ public static final String DEFAULT_METHOD_PARAM = "_method"; private String methodParam = DEFAULT_METHOD_PARAM; /** * Set the parameter name to look for HTTP methods. * @see #DEFAULT_METHOD_PARAM */ public void setMethodParam(String methodParam) &#123; Assert.hasText(methodParam, "'methodParam' must not be empty"); this.methodParam = methodParam; &#125; @Override protected void doFilterInternal(HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException &#123; HttpServletRequest requestToUse = request; // 只处理POST请求，即只允许POST请求转化为其它类型的请求 if ("POST".equals(request.getMethod()) &amp;&amp; request.getAttribute(WebUtils.ERROR_EXCEPTION_ATTRIBUTE) == null) &#123; // 获取"_method"值 String paramValue = request.getParameter(this.methodParam); // 判断值是否为空，为空就不处理 if (StringUtils.hasLength(paramValue)) &#123; // 统一将值转换为大写，如put==&gt;PUT, delete==&gt;DELETE String method = paramValue.toUpperCase(Locale.ENGLISH); // 如果值是PUT/DELETE/PATCH，就将当前的HttpServletRequest转为HttpMethodRequestWrapper // HttpMethodRequestWrapper就是写在下面的静态内部类 if (ALLOWED_METHODS.contains(method)) &#123; requestToUse = new HttpMethodRequestWrapper(request, method); &#125; &#125; &#125; // 最后统一放行所有请求 filterChain.doFilter(requestToUse, response); &#125; /** * HttpMethodRequestWrapper 继承了HttpServletRequest * 1、传入一个method参数，该值就是之前处理过的PUT/DELETE/PATH * 2、重载了getMethod方法 */ /** * Simple &#123;@link HttpServletRequest&#125; wrapper that returns the supplied method for * &#123;@link HttpServletRequest#getMethod()&#125;. */ private static class HttpMethodRequestWrapper extends HttpServletRequestWrapper &#123; private final String method; public HttpMethodRequestWrapper(HttpServletRequest request, String method) &#123; super(request); this.method = method; &#125; @Override public String getMethod() &#123; return this.method; &#125; &#125;&#125;]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC @PathVariable]]></title>
    <url>%2F2019%2F04%2F01%2FSpringMVC%2FSpringMVC-PathVariable%2F</url>
    <content type="text"><![CDATA[1. @PathVariable概述带占位符的 URL 是 Spring3.0 新增的功能，该功能在 SpringMVC 向 REST 目标挺进发展过程中具有里程碑的意义 通过 @PathVariable 可以将 URL 中占位符参数绑定到控制器处理方法的入参中：URL 中的 {xxx} 占位符可以通过 @PathVariable(“xxx”) 绑定到操作方法的入参中。 2. @PathVariable的使用基本使用 1234567// 访问/user，无法映射// 访问/user/123，得到username=123// 访问/user/haha/123，无法映射@RequestMapping("/user/&#123;username&#125;")public void hello(@PathVariable String username) &#123; System.out.println("hello: " + username);&#125; 如果占位符参数与接收参数的形参名不一致，则需要指定占位参数名 1234@RequestMapping("/user/&#123;aaa&#125;")public void hello(@PathVariable("aaa") String username) &#123; System.out.println("hello: " + username);&#125; 可以有多个占位符参数 12345@GetMapping("/hello/&#123;id&#125;/&#123;age&#125;")public void haha(@PathVariable Integer id, @PathVariable Integer age) &#123; System.out.println("id = " + id); System.out.println("age = " + age);&#125;]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC @RequestMapping]]></title>
    <url>%2F2019%2F04%2F01%2FSpringMVC%2FSpringMVC-RequestMapping%2F</url>
    <content type="text"><![CDATA[1. 路径开头的”/“可有可无无论@RequestMapping是加在类上，还是方法上，开头的”/“可有可无，习惯加上”/“123// 以下两个mapping是等价的@RequestMapping("hello")@RequestMapping("/hello") 2. 两个方法不能处理同一请求12345678@RequestMapping("/hello")public void fun1() &#123; System.out.println("helloworld");&#125;@RequestMapping("/hello")public void fun2() &#123; System.out.println("helloworld");&#125; 3. @RequestMapping 来处理多个 URI你可以将多个请求映射到一个方法上去，只需要添加一个带有请求路径值列表的 @RequestMapping 注解就行了 1@RequestMapping(value = &#123;"/user", "/user*", "**/hello"&#125;) 4. @RequestMapping标注在类上与方法上@RequestMapping标注在类上，可以为当前类所有方法的请求地址指定一个基准路径 12345678910111213@RequestMapping("/user") // 注意路径开头的"/"省略，即 "/user" 可简写为"user"@Controllerpublic class HelloController &#123; @RequestMapping("/fun1") // 请求的完整路径：/user/fun1 public void fun1() &#123; &#125; @RequestMapping("fun2") // 请求的完整路径：/user/fun2，注意路径开头的"/"省略 public void fun2() &#123; &#125; @RequestMapping("/") // 请求的完整路径：/user public void fun3() &#123; &#125;&#125; 5. Ant风格路径通配符映射请求 Wildcard Description ? 匹配任意单字符 * 匹配0或者任意数量的字符，不包含/ ** 匹配0或者更多数量的目录，不包含/ 123456// 匹配/user, /user11, 但无法匹配/user11/22, /user/11@RequestMapping("/user*")// 匹配/user1, /user2，但无法匹配/user@RequestMapping("/user?")// 匹配/user, /aa/user, /aa/bb/user@RequestMapping("/**/user") 需要注意的是，路径匹配遵循最长匹配原则(has more characters)123// 例如同时存在以下映射，"/user/aa"优先匹配"/user/*"@RequestMapping("/**/user/*")@RequestMapping("/user/*") 6. @RequestMapping的各种参数6.1. method限制请求类型请求可取的值123public enum RequestMethod &#123; GET, HEAD, POST, PUT, PATCH, DELETE, OPTIONS, TRACE&#125; 123456// 默认处理所有类型的请求@RequestMapping("/fun")// 只处理GET请求@RequestMapping(value="/fun", method=RequestMethod.GET)// 只处理GET和POST请求@RequestMapping(value="/fun", method= &#123;RequestMethod.GET, RequestMethod.POST&#125;) 6.2. params限制请求参数注意各种URL中是否带有某个参数的区别1234http://localhost:8080/user?username username值为空串http://localhost:8080/user?username= username值为空串http://localhost:8080/user?username=123 username值为123http://localhost:8080/user username值为null params参数限定12345678910// 参数必须带有username@RequestMapping(value = "/user", params = &#123;"username"&#125;)// 参数必须不带有username@RequestMapping(value = "/user", params = &#123;"!username"&#125;)// 参数必须带有username，且值=123@RequestMapping(value = "/user", params = &#123;"username=123"&#125;)// 参数可以不带有username，或者有username但是值不是123@RequestMapping(value = "/user", params = &#123;"username!=123"&#125;)// 多参数限定@RequestMapping(value = "/user", params = &#123;"username","!aaa", "bbb=123", "ccc!=456"&#125;) 6.3. headers限定请求头12345678// 限定content-type@RequestMapping(value = "/user", headers = &#123; "content-type=text/plain" &#125;)// 限定content-type可以是text/plain或者text/html@RequestMapping(value = "/user", headers = &#123; "content-type=text/plain", "content-type=text/html" &#125;) 6.4. consumes限定请求头中的Content-Type6.5. produces给响应头设置Content-Type7. 组合注解Spring 4.3 引入了方法级注解的变体，也被叫做 @RequestMapping 的组合注解。组合注解可以更好的表达被注解方法的语义。它们所扮演的角色就是针对 @RequestMapping 的封装，而且成了定义端点的标准方法 例如，@GetMapping 是一个组合注解，它所扮演的是 @RequestMapping(method =RequestMethod.GET) 的一个快捷方式 方法级别的注解变体有如下几个： @GetMapping @PostMapping @PutMapping @DeleteMapping @PatchMapping 123456789101112131415161718192021222324252627282930313233@RestController@RequestMapping("/home")public class IndexController &#123; @GetMapping("/person") public @ResponseBody ResponseEntity&lt;String&gt; getPerson() &#123; return new ResponseEntity&lt;String&gt;("Response from GET", HttpStatus.OK); &#125; @GetMapping("/person/&#123;id&#125;") public @ResponseBody ResponseEntity&lt;String&gt; getPersonById(@PathVariable String id) &#123; return new ResponseEntity&lt;String&gt;("Response from GET with id " + id, HttpStatus.OK); &#125; @PostMapping("/person") public @ResponseBody ResponseEntity&lt;String&gt; postPerson() &#123; return new ResponseEntity&lt;String&gt;("Response from POST method", HttpStatus.OK); &#125; @PutMapping("/person") public @ResponseBody ResponseEntity&lt;String&gt; putPerson() &#123; return new ResponseEntity&lt;String&gt;("Response from PUT method", HttpStatus.OK); &#125; @DeleteMapping("/person") public @ResponseBody ResponseEntity&lt;String&gt; deletePerson() &#123; return new ResponseEntity&lt;String&gt;("Response from DELETE method", HttpStatus.OK); &#125; @PatchMapping("/person") public @ResponseBody ResponseEntity&lt;String&gt; patchPerson() &#123; return new ResponseEntity&lt;String&gt;("Response from PATCH method", HttpStatus.OK); &#125;&#125;]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC 视图解析器]]></title>
    <url>%2F2019%2F04%2F01%2FSpringMVC%2FSpringMVC-%E8%A7%86%E5%9B%BE%E8%A7%A3%E6%9E%90%E5%99%A8%2F</url>
    <content type="text"><![CDATA[12]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC HelloWord]]></title>
    <url>%2F2019%2F03%2F31%2FSpringMVC%2FSpringMVC-HelloWord%2F</url>
    <content type="text"><![CDATA[1. HelloWord 环境搭建1.1. Tomcat服务环境这里介绍一下IDAE Maven工程怎么关联本地Tomcat服务。 首先编辑pom.xml，将打包方式修改为war1&lt;packaging&gt;war&lt;/packaging&gt; 创建webapp/WEB-INF目录，再添加web.xml1234567&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;web-app xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://java.sun.com/xml/ns/javaee" xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd" id="WebApp_ID" version="2.5"&gt;&lt;/web-app&gt; Run-&gt;Edit Run Configurations-&gt;添加Tomcat Server Local 点击右下角的Fix-&gt;选择带有exploded的选项 1.2. 依赖Spring依赖 4个核心容器模块 AOP（Spring注解依赖于AOP） web和webmvc（webmvc就是SpringMVC） 根据依赖传递，只需要导入webmvc 123456789101112&lt;properties&gt; &lt;spring.version&gt;5.1.5.RELEASE&lt;/spring.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!-- springmvc --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-webmvc&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.3. 编辑web.xml，配置前端控制器 DispatcherServlet配置前端控制器DispatcherServlet，从名称就可以看出是一个Servlet 不仅是针对DispatcherServlet，所有的Servlet在创建时都分以下3种情况 loadOnStartup &lt; 0：即负数的情况下，web容器启动的时候不做实例化处理，servlet首次被调用时做实例化。这种情况和没有设置loadOnStartup是一样的 loadOnStartup &gt; 0：web容器启动的时候做实例化处理，顺序是由小到大，正整数小的先被实例化 loadOnStartup = 0：web容器启动的时候做实例化处理，相当于是最大整数，因此web容器启动时，最后被实例化 1234567891011121314151617181920212223&lt;!-- The front controller of this Spring Web application, responsible for handling all application requests （配置前端控制器，本质是一个Servlet，拦截所有请求并智能派发） --&gt;&lt;servlet&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;!-- contextConfigLocation指定springmvc配置文件的路径 默认值是/WEB-INF/&lt;servlet-name&gt;-servlet.xml --&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:springmvc.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;!-- load-on-startup是可选配置，值越小，优先级越高 若值为1，则web应用在启动时就会加载该Servlet 如果&lt;load-on-startup&gt;未配置，则首次访问该Servlet时，才被加载 --&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;!-- Map all requests to the DispatcherServlet for handling --&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; DispatcherServlet的URL映射拦截规则如下，/*的拦截范围比/更大，还会拦截到*.jsp请求，这样jsp页面就无法显示了 url-pattern 拦截请求 使用说明 /* 一切资源。包括js、css等 不建议使用。全部拦截后，JSP页面就无法显示了 / 除了jsp之外的一切资源 强烈建议使用。多用于前台请求 *.action或*.do 指定后缀的请求 可以使用。多用于后台请求 1.4. 创建springmvc.xml在类路径下创建springmvc.xml，就是Spring Bean Configuration File Eclipse：用STS插件快速创建即可 IDEA：New-&gt;XML Configuration File-&gt;Spring Config 配置组件扫描1&lt;context:component-scan base-package="demo.springmvc"/&gt; 1.5. 创建JSP页面创建/WEB-INF/jsp/index.jsp 1.6. 创建Controller注意在SpringMVC中，@Controller不能与其它注解组件混用。如果改为@Component，则无法识别@RequestMapping等注解12345678@Controllerpublic class HelloController &#123; @RequestMapping("/hello") public String hello() &#123; System.out.println("HelloWorld"); return "/WEB-INF/jsp/index.jsp"; &#125;&#125; 1.7. 运行Tomcat测试在Tomcat上运行，访问 /hello 2. 添加视图解析器返回的jsp页面（视图）的前后缀总是/WEB-INF/jsp/和.jsp，每次都要写，很麻烦，而且页面路径一改，所有地方都要修改。这时候我们可以使用视图解析器，它会自动给视图名拼接前后缀，得到完整的路径，简化开发 编辑springmvc.xml1234&lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;property name="prefix" value="/WEB-INF/jsp/"&gt;&lt;/property&gt; &lt;property name="suffix" value=".jsp"&gt;&lt;/property&gt;&lt;/bean&gt; 修改Controller12345678910@Controllerpublic class HelloController &#123; @RequestMapping("/hello") public String hello() &#123; System.out.println("hellowolrd"); // return "/WEB-INF/jsp/index.jsp"; // 去掉前后缀 return "index"; &#125;&#125; 3. HelloWorld运行流程 客户端请求/hello 请求来到tomcat服务器； SpringMVC的前端控制器收到所有请求 前端控制器看一下请求地址和@RequestMapping标注的哪个匹配，来找到到底使用那个类的哪个方法来处理 前端控制器找到了目标处理器类和目标方法，直接利用返回执行目标方法； 方法执行完成以后会有一个返回值；SpringMVC认为这个返回值就是要去的页面地址 拿到方法返回值以后；用视图解析器进行拼串得到完整的页面地址； 拿到页面地址，前端控制器帮我们转发到页面； 4. DispatcherServlet尝试不指定bean配置文件的位置尝试不指定bean配置文件的位置12345&lt;servlet&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt; 运行tomcat，发现报错，说找不到Bean配置文件，默认会到/WEB-INF/下去找springDispatcherServlet-servlet.xml，这个配置文件的名称由你配置的&lt;servlet-name&gt;决定，默认文件名就是&lt;servlet-name&gt;-servlet.xml12Caused by: java.io.FileNotFoundException: Could not open ServletContext resource [/WEB-INF/springDispatcherServlet-servlet.xml] 5. DispatcherServlet的url-pattern分析5.1. url-pattern为/的分析打开tomcat服务器的全局web.xml，所有tomcat项目中的web.xml都继承该配置文件。可以看到有一个DefaultServlet，它是专用用来处理静态资源的请求的。仔细看，你会发现它的url-pattern就是/ 12345678910111213141516171819202122232425&lt;!-- DefaultServlet会应用于所有的tomcat项目，用于处理静态资源请求。 哪些资源是静态资源呢？简单地讲，除了jsp/servlet（jsp本质还是servlet），其它的资源都是静态资源 DefaultServlet是如何处理静态资源的请求呢？以用户访问index.html为例 DefaultServlet会在服务器下找到index.html这个资源，并返回给用户--&gt;&lt;servlet&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.catalina.servlets.DefaultServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;debug&lt;/param-name&gt; &lt;param-value&gt;0&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;listings&lt;/param-name&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;default&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 如果我们给DispatcherServlet的url-pattern也配置/，相当于禁用了DefaultServlet，静态资源也交给DispatcherServlet去处理。用户访问/index.html，DispatcherServlet会去找哪个Controller的RequestMapping能够匹配/index.html，如果没有匹配，用户就无法访问/index.html了 那为什么用户可以直接访问jsp页面呢？再查看tomcat服务器的全局web.xml，可以看到有一个名为jsp的servlet，它会拦截所有的jsp请求，渲染jsp页面返回给用户12345678910111213141516171819&lt;servlet&gt; &lt;servlet-name&gt;jsp&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.jasper.servlet.JspServlet&lt;/servlet-class&gt; &lt;init-param&gt; &lt;param-name&gt;fork&lt;/param-name&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;xpoweredBy&lt;/param-name&gt; &lt;param-value&gt;false&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;3&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;jsp&lt;/servlet-name&gt; &lt;url-pattern&gt;*.jsp&lt;/url-pattern&gt; &lt;url-pattern&gt;*.jspx&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 5.2. 修改修改为/*1234&lt;servlet-mapping&gt; &lt;servlet-name&gt;springDispatcherServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 访问Controller对应的/hello，输出以下警告，说12警告 [http-nio-8080-exec-6] org.springframework.web.servlet.DispatcherServlet.noHandlerFound No mapping for GET /WEB-INF/jsp/index.jsp]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SpringMVC 概述]]></title>
    <url>%2F2019%2F03%2F31%2FSpringMVC%2FSpringMVC-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1. SpringMVC 简介 Spring 为展现层提供的基于 MVC 设计理念的优秀的 Web 框架，是目前最主流的 MVC 框架之一 Spring3.0 后全面超越 Struts2，成为最优秀的 MVC 框架 Spring MVC 通过一套 MVC 注解，让 POJO 成为处理请求的控制器，而无须实现任何接口。 支持 REST 风格的 URL 请求 采用了松散耦合可插拔组件结构，比其他 MVC 框架更具扩展性和灵活性]]></content>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 整合单元测试]]></title>
    <url>%2F2019%2F03%2F31%2FSpring%2FSpring-%E6%95%B4%E5%90%88%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[1. 单独使用JUnit测试Spring应用Spring整合JUnit之前，我都是直接用JUnit测试Spring应用，每次都要手动创建容器，再从容器中取出Bean123456@Testpublic void test() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Person person = (Person) ioc.getBean("person"); System.out.println(person);&#125; 后来才看到，原来Spring整合了JUnit，叫做Spring Test，专门用来测试Spring应用，给测试带来很大的方便 2. Spring 整合JUnit流程添加依赖12345678910111213&lt;!-- junit --&gt;&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 编写单元测试123456789101112@ContextConfiguration("classpath:applicationContext.xml") // 指定Bean配置文件的路径@RunWith(SpringJUnit4ClassRunner.class) // 指定使用Spring的单元测试驱动类public class SpringTest &#123; @Autowired // 可以直接注入 private Person person; /** 测试方法 */ @Test public void test() &#123; System.out.println(person); &#125;&#125; 3. @ContextConfiguration注解3.1. 指定Spring Bean配置文件的路径指定单个Bean配置文件1234// 以下3种方式等价@ContextConfiguration("classpath:applicationContext.xml")@ContextConfiguration(locations="classpath:applicationContext.xml")@ContextConfiguration(locations = &#123; "classpath:applicationContext.xml" &#125;) 指定多个Bean配置文件12@ContextConfiguration(locations = &#123; "classpath:applicationContext1.xml", "classpath:applicationContext2.xml" &#125;)@ContextConfiguration(&#123; "classpath:applicationContext1.xml", "classpath:applicationContext2.xml" &#125;) 3. @RunWith注解@RunWith是JUnit注解，指定用哪种驱动进行单元测试]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 基于注解自动装配]]></title>
    <url>%2F2019%2F03%2F31%2FSpring%2FSpring-%E5%9F%BA%E4%BA%8E%E6%B3%A8%E8%A7%A3%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%2F</url>
    <content type="text"><![CDATA[1. 基于@Autowired自动装配流程编写Bean123456@Data@Component // 组件注解public class Car &#123; private String name; private Double price;&#125; 123456@Data@Component // 组件注解public class Person &#123; @Autowired // 自动装配注解 private Car car;&#125; 组件扫描1&lt;context:component-scan base-package="demo.spring.bean" /&gt; 编写测试方法123456@Testpublic void test() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Person person = (Person) ioc.getBean("person"); System.out.println(person.getCar() != null); // true，自动装配成功&#125; 1.1. 只有Bean组件才能自动装配如果去掉Person的组件注解，Spring根本就不会去管Person，@Autowired就没有意义了123456@Data// @Component 去掉组件注解public class Person &#123; @Autowired // 无意义 private Car car;&#125; 如果去掉Car的组件注解，也是无法装配成功的123456@Data@Componentpublic class Person &#123; @Autowired // NoSuchBeanDefinitionException，找不到类型为Car的组件 private Car car;&#125; 2. @Autowired原理的简单探索参考：https://blog.csdn.net/horacehe16/article/details/79811763 @Autowired可以应用于 属性、构造方法、setter等等。但是开发过程中应用于属性是最常见的。时间有限，就先只探索一下@Autowired应用于属性的情况 2.1. 探索：容器中该类型的组件只有一个修改Person类，修改car属性名，发现自动装配成功，说明容器中同类型组件只有1个时，是根据类型注入，变量名与id不一致也没有关系123456@Data@Componentpublic class Person &#123; @Autowired private Car haha; // 修改属性名，使之与Car组件的id不一致&#125; 2.2. 探索：容器中该类型的组件有多个基于注解注册多个Car组件123456@Data@Componentpublic class Car &#123; private String name; private Double price;&#125; 123@Componentpublic class CarExtend extends Car &#123; // 继承Car&#125; 基于XML注册多个Car组件12&lt;bean id="car1" class="demo.spring.bean.Car"/&gt;&lt;bean id="car2" class="demo.spring.bean.Car"/&gt; 修改Person，使得Car属性名与所有Car组件id不一致123456@Data@Componentpublic class Person &#123; @Autowired private Car haha; // 与所有Car组件的id都不一致&#125; 运行，抛出异常12345org.springframework.beans.factory.UnsatisfiedDependencyException: Error creating bean with name 'person': Unsatisfied dependency expressed through field 'haha'; nested exception is org.springframework.beans.factory.NoUniqueBeanDefinitionException: No qualifying bean of type 'demo.spring.bean.Car' available: expected single matching bean but found 2: car,carExtend 修改Person，使得Car属性名与其中一个Car组件id一致，发现注入成功。可知，容器中同一类型组件有多个，是根据 类型+id 同时匹配才注入。只有类型匹配是不够的123456789101112@Data@Componentpublic class Person &#123; @Autowired private Car car; // 与Car的id一致，成功注入Car @Autowired private Car carExtend; // 与CarExtend的id一致，成功注入CarExtend @Autowired private Car car1; @Autowired private Car car2;&#125; 如果你不想修改变量名呢？此时可以使用@Autowired+@Qualifier的组合。注意必须两个注解结合使用12345678910111213141516@Data@Componentpublic class Person &#123; @Autowired @Qualifier("car") // 指定组件id private Car carAaa; @Autowired @Qualifier("carExtend") private Car carBbb; @Autowired @Qualifier("car1") private Car carCcc; @Autowired @Qualifier("car2") private Car carDdd;&#125; @Autowired+@Qualifier写起来太麻烦，所以用@Resource进行简化 123456789101112@Data@Componentpublic class Person &#123; @Resource(name="car") private Car carAaa; @Resource(name="carExtend") private Car carBbb; @Resource(name="car1") private Car carCcc; @Resource(name="car2") private Car carDdd;&#125; 3. @Autowired的required属性将Car的组件注解去掉123456@Data// @Component 去掉public class Car &#123; private String name; private Double price;&#125; 因为没有类型为Car的组件，所以Person在注入Car时会出错。这是因为@Autowired有一个required属性，默认为true，表示找不到要注入的组件就会出错123456@Data@Componentpublic class Person &#123; @Autowired // NoSuchBeanDefinitionException，找不到类型为Car的组件 private Car car;&#125; 设置required=false123456@Data@Componentpublic class Person &#123; @Autowired(required=false) // 找不到类型为Car的组件，但不会出错，只是car的值为null private Car car;&#125; 4. @Autowired与@Resource的区别除了@Autowired，也可以直接用@Resource，二者效果上是等同的 123456@Data@Componentpublic class Person &#123; @Resource // 也可以直接用@Resource实现自动装配 private Car car;&#125; 在指定id时，@Autowired要结合@Qualifier，@Resource直接指定name属性即可123456@Resource(name="car")private Car car;@Autowired@Qualifier("car")private Car car; 有3个注解都是实现自动装配的 @Resource属于包javax.annotation，是JavaEE注解，所以最通用，扩展性最好，Spring/EJB都可以使用 @Autowired属于包org.springframework.beans.factory.annotation，是Spring注解，功能最强大 @Inject是EJB注解]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 基于注解标识和扫描Bean]]></title>
    <url>%2F2019%2F03%2F31%2FSpring%2FSpring-%E5%9F%BA%E4%BA%8E%E6%B3%A8%E8%A7%A3%E6%A0%87%E8%AF%86%E5%92%8C%E6%89%AB%E6%8F%8FBean%2F</url>
    <content type="text"><![CDATA[1. 使用注解标识Bean组件标识Bean组件的注解有四个： @Respository：标识持久化层（Dao层）组件。常用来标识XxxDao/XxxRepository @Service：标识业务逻辑层组件。常用来标识XxxService @Controller：标识控制器组件。常用来标识XxxServlet/XxxController @Component：标识普通组件。 1.1. 注解和XML使用场景用注解简洁又优雅。如果是自己写的类，尽量直接用注解 如果引用了其它依赖，但是依赖包中的类没有加注解，你还想要把这些类注册到IOC容器中，就只能用xml配置的方式。毕竟你不能直接修改源码，加上组件注册。 注解和xml配置可以结合使用 1.2. 标识流程编写Bean，添加组件注解123456@Data@Component // 添加组件注解public class Person &#123; private String name; private Integer age;&#125; 配置组件扫描12&lt;!-- 扫描指定包及其子包下 所有带有组件注解的Bean --&gt;&lt;context:component-scan base-package="demo.spring.bean"/&gt; 编写测试方法，从容器中取出Bean12345678910@Testpublic void test() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); // 通过类对象获取 Person person1 = ioc.getBean(Person.class); // 获取名称获取，Bean的默认名称是 类名首字母小写，即Person =&gt; person Person person2 = (Person) ioc.getBean("person"); // 默认是单实例的 System.out.println(person1 == person2); // true&#125; 1.3. 组件命名组件命名规则： 默认情况：组件简单类名首字母小写作为Bean的id 使用组件注解的value属性指定Bean的id 123456@Data@Component("hello") // 命名public class Person &#123; private String name; private Integer age;&#125; 1234567@Testpublic void test() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Person person1 = ioc.getBean(Person.class); Person person2 = (Person) ioc.getBean("aaa"); System.out.println(person1 == person2);&#125; 1.4. 设置组件作用域 @Scope(value=”singleton”) @Scope(value=”prototype”) 1234567@Data@Component@Scope("prototype") // 多实例public class Person &#123; private String name; private Integer age;&#125; 2. 组件扫描扫描单个包下的组件12&lt;!-- 扫描指定包及其子包下 所有带有组件注解的class --&gt;&lt;context:component-scan base-package="demo.spring.bean"/&gt; 扫描多个包下的组件12&lt;!-- 扫描多个包用逗号分隔 --&gt;&lt;context:component-scan base-package="demo.spring.bean,demo.spring.service"/&gt; 多个context:component-scan不建议共存，可能会出现奇怪的问题，暂时没有研究123&lt;!-- 不建议写多个context:component-scan --&gt;&lt;context:component-scan base-package="demo.spring.bean"/&gt;&lt;context:component-scan base-package="demo.spring.service"/&gt; 2.1. context:exclude-filter排除扫描排除某个class1234&lt;context:component-scan base-package="demo.spring.bean"&gt; &lt;!-- 排除Car --&gt; &lt;context:exclude-filter type="assignable" expression="demo.spring.bean.Car"/&gt;&lt;/context:component-scan&gt; 排除某个注解1234&lt;context:component-scan base-package="demo.spring.bean"&gt; &lt;!-- 排除@Service --&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.stereotype.Service" /&gt;&lt;/context:component-scan&gt; 多个context:exclude-filter可以共存123456&lt;context:component-scan base-package="demo.spring.bean"&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.stereotype.Service" /&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.stereotype.Controller" /&gt; &lt;context:exclude-filter type="assignable" expression="demo.spring.bean.Car"/&gt; &lt;context:exclude-filter type="assignable" expression="demo.spring.bean.Person"/&gt;&lt;/context:component-scan&gt; 特别注解，如果排除@Component，相当于还排除了@Repository、@Service、@Controller 2.2. context:include-filter包含扫描包含某个注解1234&lt;!-- use-default-filters设置为false，context:component-scan默认不会扫描到任何class --&gt;&lt;context:component-scan base-package="demo.spring.bean" use-default-filters="false"&gt; &lt;context:include-filter type="annotation" expression="org.springframework.stereotype.Service"/&gt;&lt;/context:component-scan&gt; 包含某个class123&lt;context:component-scan base-package="demo.spring.bean" use-default-filters="false"&gt; &lt;context:include-filter type="assignable" expression="demo.spring.bean.Person"/&gt;&lt;/context:component-scan&gt; 多个context:include-filter可以共存1234&lt;context:component-scan base-package="demo.spring.bean" use-default-filters="false"&gt; &lt;context:include-filter type="assignable" expression="demo.spring.bean.Person"/&gt; &lt;context:include-filter type="annotation" expression="org.springframework.stereotype.Service"/&gt;&lt;/context:component-scan&gt;]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring SpEL]]></title>
    <url>%2F2019%2F03%2F30%2FSpring%2FSpring-SpEL%2F</url>
    <content type="text"><![CDATA[参考：https://docs.spring.io/spring/docs/4.2.x/spring-framework-reference/html/expressions.html 1. 什么是SpELThe Spring Expression Language (SpEL for short) is a powerful expression language that supports querying and manipulating an object graph at runtime SpEL是一种表达式语言，支持在运行时查询、操作对象图。这里不太理解对象图是什么意思，简单来讲说SpEL可以操作各种对象的数据和方法吧 和JSP页面上的EL表达式、Struts2中用到的OGNL表达式一样，SpEL根据JavaBean风格的getXxx()、setXxx()方法定义的属性访问对象图，完全符合我们熟悉的操作习惯 基本语法：SpEL使用#{…}作为定界符，所有在大框号中的字符都将被认为是SpEL表达式 2. 功能概览 英文 中文 Literal expressions 字面值表达式 Boolean and relational operators 布尔和关系操作符 Regular expressions 正则表达式 Class expressions 类表达式 Accessing properties, arrays, lists, maps 访问properties、arrays、lists、maps Method invocation 方法调用 Relational operators 关系操作符 Assignment 赋值 Calling constructors 调用构造器 Bean references bean引用 Array construction 构建数组 Inline lists 内联lists Inline maps 内联maps Ternary operator 三元操作符 Variables 变量 User defined functions 用户定义的功能 Collection projection 集合投影 Collection selection 集合选择 Templated expressions 模板化表达式 运算符 算术运算符：+、-、*、/、%、^ 字符串连接：+ 比较运算符：&lt;、&gt;、==、&lt;=、&gt;=、lt、gt、eq、le、ge 逻辑运算符：and, or, not, | 三目运算符：判断条件?判断结果为true时的取值:判断结果为false时的取值 正则表达式：matches 3. 字面值表达式整数 1&lt;property name="count" value="#&#123;5&#125;"/&gt; 浮点数 1&lt;property name="point" value="#&#123;89.7&#125;"/&gt; 科学计数法 1&lt;property name="capacity" value="#&#123;1e4&#125;"/&gt; 字符串，可以用单引号或者双引号作为定界符号 12&lt;property name=“name” value="#&#123;'Chuck'&#125;"/&gt;&lt;property name='name' value='#&#123;"Chuck"&#125;'/&gt; 布尔 12&lt;property name="enabled" value="#&#123;false&#125;"/&gt;&lt;property name="enabled" value="#&#123;true&#125;"/&gt; 4. 引用Bean组件4.1. 引用其它Bean组件123456789&lt;bean id="car" class="bean.Car"&gt; &lt;property name="name" value="haha"/&gt;&lt;/bean&gt;&lt;bean id="person" class="bean.Person"&gt; &lt;!-- SpEL 引用Bean --&gt; &lt;property name="car" value="#&#123;car&#125;"/&gt; &lt;!-- ref 引用Bean，两种方式效果一样 --&gt; &lt;property name="car" ref="car"/&gt; &lt;/bean&gt; 4.2. 引用其它Bean组件的属性123456&lt;bean id="car" class="bean.Car"&gt; &lt;property name="name" value="haha"/&gt;&lt;/bean&gt;&lt;bean id="person" class="bean.Person"&gt; &lt;property name="name" value="#&#123;car.name&#125;"/&gt;&lt;/bean 4.3. 调用其它Bean组件的方法123456&lt;bean id="car" class="bean.Car"&gt; &lt;property name="name" value="haha"/&gt;&lt;/bean&gt;&lt;bean id="person" class="bean.Person"&gt; &lt;property name="name" value="#&#123;car.getName()&#125;"/&gt;&lt;/bean&gt; 5. 调用静态方法语法：#{T(类全名).静态方法()} 1234&lt;bean id="person" class="bean.Person"&gt; &lt;property name="name" value="#&#123;T(java.util.UUID).randomUUID().toString().substring(0, 10)&#125;"/&gt; &lt;property name="age" value="#&#123;T(java.lang.Math).PI*20&#125;"/&gt;&lt;/bean&gt;]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 基于XML的自动装配]]></title>
    <url>%2F2019%2F03%2F30%2FSpring%2FSpring-%E5%9F%BA%E4%BA%8EXML%E7%9A%84%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%2F</url>
    <content type="text"><![CDATA[1. 什么是自动装配自动装配是相对于手动装配而言的 手动装配：以value或ref的方式明确指定属性值都是手动装配 自动装配：根据指定的装配规则，不需要明确指定，Spring自动将匹配的属性值注入bean中 自动装配有注解和XML配置两种实现方式。在XML文档中进行的自动装配略显笨拙，在项目中更多的使用注解的方式实现 2. autowire属性值2.1. autowire=no/defaultautowire=no 就是不启用自动装配 autowire=default 是默认值。由上级beans标签的default-autowire属性确定。默认效果和autowire=no一样，不启用自动装配 2.2. autowire=byName12345@Datapublic class Car &#123; private String name; private Double price;&#125; 1234@Datapublic class Person &#123; private Car car;&#125; 12345&lt;bean id="car" class="bean.Car"&gt; &lt;property name="name" value="benz" /&gt; &lt;property name="price" value="500.0" /&gt;&lt;/bean&gt;&lt;bean id="person" class="bean.Person" autowire="byName"/&gt; 运行测试，看到car自动装配了123456@Testpublic void test() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Person person = ioc.getBean(Person.class); System.out.println(person);&#125; 现在探索一下原理。修改一下Person类，只有一个setCar方法。运行测试方法，发现setCar()被调用了12345public class Person &#123; public void setCar(Car haha) &#123; System.out.println("自动装配调用" + haha); &#125;&#125; 再修改，setCar方法名不变，参数类型改为其它类型，如String。再次运行，发现setCar()没有被调用12345public class Person &#123; public void setCar(String haha) &#123; System.out.println("自动装配调用" + haha); &#125;&#125; 综上可知，自动装配的过程是： 找到setCar()，就认为Person有一个car属性 Spring会去容器中查找id/name为car的组件，如果组件类型和setCar()参数的类型对应，则调用setCar(car)，实现自动装配 2.3. autowire=byTypeBean配置1&lt;bean id="person" class="bean.Person" autowire="byType" /&gt; 编写Person，运行之前的测试方法，发现自动装配成功1234@Datapublic class Person &#123; private Car car;&#125; 修改Person，修改一下属性名，只要与组件中的Car的id不相同即可。运行之前的测试方法，发现自动装配还是能成功1234@Datapublic class Person &#123; private Car aaa;&#125; 探索一下原理。修改Person类，随便写一个setter方法，但是参数类型是Car。再运行测试方法，发现还是能自动装配 12345public class Person &#123; public void setAbc(Car haha) &#123; System.out.println("自动装配调用" + haha); &#125;&#125; 再修改，将参数修改为其它类型。运行测试方法，发现无法自动装配12345public class Person &#123; public void setAbc(String haha) &#123; System.out.println("自动装配调用" + haha); &#125;&#125; 综上可知，byType自动装配取决于setter的参数类型，如果该参数类型与容器中某个组件的类型对应，就会调用setter，并将组件作为参数传入，实现自动装配 再想一下，如果有多个setter的参数类型都是Car，会发生什么事情呢？修改一下Person，两个setter的参数类型都是Car。运行之前的测试方法，发现两个setter都被调用了。也就是説，只要setter的参数类型与组件对应，所有的setter都会被调用12345678public class Person &#123; public void setAaa(Car aaa) &#123; System.out.println("自动装配调用aaa " + aaa); &#125; public void setBbb(Car bbb) &#123; System.out.println("自动装配调用bbb " + bbb); &#125;&#125; 再想一下，如果容器中有多个Car组件，会自动装配哪个组件呢？修改Bean配置，注册2个Car组件123456789&lt;bean id="car1" class="bean.Car"&gt; &lt;property name="name" value="car1" /&gt; &lt;property name="price" value="100.0" /&gt;&lt;/bean&gt;&lt;bean id="car2" class="bean.Car"&gt; &lt;property name="name" value="car2" /&gt; &lt;property name="price" value="200.0" /&gt;&lt;/bean&gt;&lt;bean id="person" class="bean.Person" autowire="byType"/&gt; 再运行，抛出异常。这说明，如果容器中有多个Car组件，此时不应该通过byType将Car自动装配到Person中1org.springframework.beans.factory.NoUniqueBeanDefinitionException: No qualifying bean of type 'bean.Car' available: expected single matching bean but found 2: car1,car2 2.4. autowire=constructor12345678public class Person &#123; public Person() &#123; System.out.println("无参构造"); &#125; public Person(Car haha) &#123; System.out.println("自动装配car: " + haha); &#125;&#125; 12345&lt;bean id="car1" class="bean.Car"&gt; &lt;property name="name" value="car1" /&gt; &lt;property name="price" value="100.0" /&gt;&lt;/bean&gt;&lt;bean id="person" class="bean.Person" autowire="constructor" /&gt; 运行之前的测试方法，发现自动装配成功。说明只要Person有一个有参构造方法，其参数类型是Car，该类型与容器中组件的类型对应，就会调用该构造方法 如果容器中没有对应的组件，会怎样呢？修改Bean配置，只留一下Person组件1&lt;bean id="person" class="bean.Person" autowire="constructor" /&gt; 再运行，发现无参构造方法被调用。说明没有组件匹配时，Spring会调用无参构造对Bean进行实例化，不会抛什么异常 如果容器中有多个Car组件，会出现什么情况呢？尝试修改Bean配置文件，定义多个Car组件123456789&lt;bean id="car1" class="bean.Car"&gt; &lt;property name="name" value="car1" /&gt; &lt;property name="price" value="100.0" /&gt;&lt;/bean&gt;&lt;bean id="car2" class="bean.Car"&gt; &lt;property name="name" value="car2" /&gt; &lt;property name="price" value="200.0" /&gt;&lt;/bean&gt;&lt;bean id="person" class="bean.Person" autowire="constructor" /&gt; 再运行测试方法，发现无参构造被调用了。也就是説，两个Car都没有匹配，自动装配没有成功。 修改一个有参构造方法的参数名，再运行，发现car1成功自动装配123456789public class Person &#123; public Person() &#123; System.out.println("无参构造"); &#125; /** 修改参数名，使参数名与容器中某个Car组件的id/name相对应 */ public Person(Car car1) &#123; System.out.println("自动装配car: " + car1); &#125;&#125; 经过以上实验，可总结如下： 容器中只有1个Car组件，只要Person的有参构造参数类型是Car，就能自动装配 容器中有多个Car组件，Person的有参构造参数类型不仅得是Car类型，参数名还要与其中一个组件的id/name相对应 如果无法自动装配，则调用无参构造对Person进行实例化。即能装就装，不装还能实例化，反正就是不报错。但是如果你没有给Person定义无参构造，在无法自动装配情况下，还是会报错的]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 引用外部属性文件]]></title>
    <url>%2F2019%2F03%2F30%2FSpring%2FSpring-%E5%BC%95%E7%94%A8%E5%A4%96%E9%83%A8%E5%B1%9E%E6%80%A7%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[1. property-placeholder加载外部文件的几种方式12345678&lt;!-- 从classpath加载 --&gt;&lt;context:property-placeholder location="classpath:db.properties" /&gt;&lt;!-- 从本地加载 --&gt;&lt;context:property-placeholder location="file:///c:/file3.properties" /&gt;&lt;!-- 相对路径加载（相对于当前Bean配置文件的路径） --&gt;&lt;context:property-placeholder location="config/file1.properties" /&gt;&lt;!-- 加载多个文件，路径之间用逗号分隔 --&gt;&lt;context:property-placeholder location="classpath:file1.properties,config/file2.properties,file:///c:/file3.properties" /&gt; 2. property-placeholder属性location：表示属性文件位置，多个之间通过如(逗号/分号)等分隔； file-encoding：文件编码； ignore-resource-not-found：如果属性文件找不到，是否忽略，默认false，即不忽略，找不到将抛出异常 ignore-unresolvable：是否忽略解析不到的属性，如果不忽略，找不到将抛出异常 properties-ref：本地java.util.Properties配置 local-override：是否本地覆盖模式，即如果true，那么properties-ref的属性将覆盖location加载的属性 system-properties-mode：系统属性模式，ENVIRONMENT（默认），NEVER，OVERRIDE 3. property-placeholder读取中文3.1. 方式1：值以Unicode编码表示properties文件12# 值以Unicode编码表示name=\u5F20\u4E09 Bean配置12&lt;!-- 直接加载文件即可 --&gt;&lt;context:property-placeholder location="classpath:file1.properties"/&gt; 3.2. 方式2：加载UTF-8properties文件，编码UTF-81name=张三 Bean配置12&lt;!-- 指定file-encoding --&gt;&lt;context:property-placeholder location="classpath:file1.properties" file-encoding="UTF-8" /&gt; 4. property-placeholder只有首个配置有效Spring容器是采用反射扫描的发现机制，通过标签的命名空间实例化实例，当Spring探测到容器中有一个org.springframework.beans.factory.config.PropertyPlaceholderCVonfigurer的Bean就会停止对剩余PropertyPlaceholderConfigurer的扫描，即只能存在一个实例123456&lt;!-- 有效 --&gt;&lt;context:property-placeholder location="classpath:file1.properties"/&gt;&lt;!-- 无效 --&gt;&lt;context:property-placeholder location="classpath:file2.properties"/&gt;&lt;!-- 无效 --&gt;&lt;context:property-placeholder location="classpath:file3.properties"/&gt; 5. 注意空白字符有时在设置value时，一不小心前后可能多打了几个空白字符，这会导致出错。因为Spring不会把这些空白字符给去掉12&lt;!-- ERROR --&gt;&lt;property name="username" value=" $&#123;jdbc.username&#125; "/&gt;]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 整合数据库连接池]]></title>
    <url>%2F2019%2F03%2F30%2FSpring%2FSpring-%E6%95%B4%E5%90%88%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[1. 整合Druid1.1. 依赖123456789101112&lt;!-- druid --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.15&lt;/version&gt;&lt;/dependency&gt;&lt;!-- mysql --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt;&lt;/dependency&gt; 1.2. Bean配置12345678910&lt;bean id="dataSource" class="com.alibaba.druid.pool.DruidDataSource" init-method="init" destroy-method="close"&gt; &lt;property name="url" value="jdbc:mysql://localhost:3306/test" /&gt; &lt;property name="username" value="root" /&gt; &lt;property name="password" value="123456" /&gt; &lt;property name="driverClassName" value="com.mysql.jdbc.Driver" /&gt; &lt;property name="maxActive" value="10" /&gt; &lt;property name="minIdle" value="5" /&gt;&lt;/bean&gt; 1.3. 测试1234567@Testpublic void test() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); DataSource dataSource = ioc.getBean(DataSource.class); Connection conn = dataSource.getConnection(); System.out.println(conn);&#125; 2. 整合C3P02.1. 依赖123456789101112&lt;!-- c3p0 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.mchange&lt;/groupId&gt; &lt;artifactId&gt;c3p0&lt;/artifactId&gt; &lt;version&gt;0.9.5.2&lt;/version&gt;&lt;/dependency&gt;&lt;!-- mysql --&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.38&lt;/version&gt;&lt;/dependency&gt; 2.2. Bean配置123456789&lt;bean id="dataSource" class="com.mchange.v2.c3p0.ComboPooledDataSource"&gt; &lt;property name="jdbcUrl" value="jdbc:mysql://localhost:3306/test" /&gt; &lt;property name="user" value="root" /&gt; &lt;property name="password" value="123456" /&gt; &lt;property name="driverClass" value="com.mysql.jdbc.Driver" /&gt; &lt;property name="minPoolSize" value="3"/&gt; &lt;property name="maxPoolSize" value="15" /&gt;&lt;/bean&gt;]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 生命周期方法]]></title>
    <url>%2F2019%2F03%2F30%2FSpring%2FSpring-%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1. 给Bean设置生命周期方法1.1. 单实例Bean1234567891011@Datapublic class Car &#123; private String name; private Double price; public void init() &#123; System.out.println("调用init()"); &#125; public void destroy() &#123; System.out.println("调用destroy()"); &#125;&#125; 1&lt;bean id="car" class="bean.Car" init-method="init" destroy-method="destroy" /&gt; 123456@Testpublic void test() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Car car1 = (Car) ioc.getBean("car"); Car car2 = (Car) ioc.getBean("car");&#125; 运行测试方法，发现只能看到init()被调用，没有看到destroy()。原因是程序运行完后，spring容器没来得及销毁bean，进程立即就结束了。1调用init() IOC容器其实有一个close方法，只是ApplicationContext接口没有123456789@Testpublic void test() throws Exception &#123; // 将ApplicationContext修改为子接口ConfigurableApplicationContext ConfigurableApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Car car1 = (Car) ioc.getBean("car"); Car car2 = (Car) ioc.getBean("car"); // 手动销毁IOC容器 ioc.close();&#125; 运行，可以看到init和destroy都被调用了。可以得知，单实例Bean的生命周期为： 容器创建之后，实例化Bean，为Bean设置属性，调用init-method 容器销毁之前，调用Bean的destroy-method 12调用init()调用destroy() 1.2. 多实例Bean12&lt;bean id="car" class="bean.Car" scope="prototype" init-method="init" destroy-method="destroy" /&gt; 123456789@Testpublic void test() throws Exception &#123; // 将ApplicationContext修改为子接口ConfigurableApplicationContext ConfigurableApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Car car1 = (Car) ioc.getBean("car"); Car car2 = (Car) ioc.getBean("car"); // 手动销毁IOC容器 ioc.close();&#125; 运行，可以看到每一次获取Bean，都会创建新的对象，并调用init()。但是容器关闭时，destroy-method没有被调用。可以得知，多实例Bean的生命周期为： 容器创建之后。每一次获取Bean时，实例化新的Bean，为Bean设置属性，调用init-method 容器销毁时，不会调用Bean的destroy-method 12调用init()调用init()]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 静态工厂与实例工厂]]></title>
    <url>%2F2019%2F03%2F30%2FSpring%2FSpring-%E9%9D%99%E6%80%81%E5%B7%A5%E5%8E%82%E4%B8%8E%E5%AE%9E%E4%BE%8B%E5%B7%A5%E5%8E%82%2F</url>
    <content type="text"><![CDATA[1. 静态工厂创建Bean12345@Datapublic class Car &#123; private String name; private Double price;&#125; 12345678910111213141516171819/** 静态工厂类 */public class CarStaticFactory &#123; /** 无参 */ public static Car getCar1() &#123; System.out.println("static Car getCar1()"); Car car = new Car(); car.setName("car1"); car.setPrice(3.14); return car; &#125; /** 有参 */ public static Car getCar2(String name, Double price) &#123; System.out.println("static Car getCar2()"); Car car = new Car(); car.setName(name); car.setPrice(price); return car; &#125;&#125; 123456789&lt;bean id="car1" class="bean.CarStaticFactory" factory-method="getCar1" /&gt;&lt;bean id="car2" class="bean.CarStaticFactory" factory-method="getCar2"&gt; &lt;!-- 传入参数 --&gt; &lt;constructor-arg name="name" value="benz"/&gt; &lt;constructor-arg name="price" value="500.0"/&gt;&lt;/bean&gt; 12345678@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Car car1 = (Car) ioc.getBean("car1"); Car car2 = (Car) ioc.getBean("car2"); System.out.println(car1); System.out.println(car2);&#125; 运行。可知通过静态工厂类创建Bean的原理是：直接调用静态方法 调用CarStaticFactory.getCart1() 调用CarStaticFactory.getCart2(name, price) 2. 实例工厂123456789101112131415161718public class CarInstanceFactory &#123; /** 无参 */ public Car getCar1() &#123; System.out.println("Car getCar1()"); Car car = new Car(); car.setName("car1"); car.setPrice(3.14); return car; &#125; /** 有参 */ public Car getCar2(String name, Double price) &#123; System.out.println("Car getCar2()"); Car car = new Car(); car.setName(name); car.setPrice(price); return car; &#125;&#125; 123456789101112&lt;!-- 先注册实例工厂类 --&gt;&lt;bean id="carInstanceFactory" class="bean.CarInstanceFactory" /&gt;&lt;!-- factory-bean指定工厂类对应的Bean组件 --&gt;&lt;bean id="car1" factory-bean="carInstanceFactory" factory-method="getCar1" /&gt;&lt;bean id="car2" factory-bean="carInstanceFactory" factory-method="getCar2"&gt; &lt;constructor-arg name="name" value="benz" /&gt; &lt;constructor-arg name="price" value="500.0" /&gt;&lt;/bean&gt; 12345678@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Car car1 = (Car) ioc.getBean("car1"); Car car2 = (Car) ioc.getBean("car2"); System.out.println(car1); System.out.println(car2);&#125; 运行。可知通过实例工厂类创建Bean的原理是： 先注册实例工厂类到IOC容器，即IOC容器创建时，会先创建一个工厂类实例 有了工厂类实例之后，会调用工厂方法，创建对应的Bean 3. FactoryBean工厂接口12345678910111213141516171819202122232425package bean;import org.springframework.beans.factory.FactoryBean;public class CarFactoryBeanImpl implements FactoryBean&lt;Car&gt;&#123; /** 工厂方法，返回要创建的对象 */ @Override public Car getObject() throws Exception &#123; System.out.println("getObject()"); Car car = new Car(); car.setName("benz"); car.setPrice(300.0); return car; &#125; /** 返回创建的对象的类型 */ @Override public Class&lt;?&gt; getObjectType() &#123; return Car.class; &#125; /** 创建的对象是单例还是多例 */ @Override public boolean isSingleton() &#123; return true; &#125;&#125; 1&lt;bean id="carFactoryBeanImpl" class="bean.CarFactoryBeanImpl"/&gt; 1234567@Testpublic void test() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); // 取出的不是工厂类，而是工厂创建的Bean Car car = (Car) ioc.getBean("carFactoryBeanImpl"); System.out.println(car);&#125; 3.1. 单例与多例实现FactoryBean接口时，isSingleton() 决定了对象是单例还是多例12345/** 创建的对象是单例还是多例 */@Overridepublic boolean isSingleton() &#123; return true;&#125; 运行测试方法，可知单例模式下，从工厂中取出的是同一对象。但是与scope=&quot;singleton&quot; 不同之处在于，只有你去取对象时，才会被实例化12345678910@Testpublic void test() throws Exception &#123; // 在ioc创建时，car没有被实例化 ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); // 在第1次获取Bean时，car才被实例化 Car car1 = (Car) ioc.getBean("carFactoryBeanImpl"); Car car2 = (Car) ioc.getBean("carFactoryBeanImpl"); // true System.out.println(car1 == car2);&#125; 修改为多例1234@Overridepublic boolean isSingleton() &#123; return false;&#125; 12345678910@Testpublic void test() throws Exception &#123; // 在ioc创建时，car没有被实例化 ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); // 每一次获取时，都会实例化新的car Car car1 = (Car) ioc.getBean("carFactoryBeanImpl"); Car car2 = (Car) ioc.getBean("carFactoryBeanImpl"); // false System.out.println(car1 == car2);&#125;]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Bean作用域]]></title>
    <url>%2F2019%2F03%2F30%2FSpring%2FSpring-Bean%E4%BD%9C%E7%94%A8%E5%9F%9F%2F</url>
    <content type="text"><![CDATA[1. Bean的四种作用域1.1. singleton 单实例singleton特点： 在容器创建时，Bean就会被实例化 从容器中取出的总是同一个对象，就是一开始被实例化的那个对象 12345678@Datapublic class Car &#123; private String name; private Double price; public Car() &#123; System.out.println("Car()构造方法"); &#125;&#125; 12&lt;!-- scope默认值就是singleton，所以可省略不写 --&gt;&lt;bean id="car" class="bean.Car" scope="singleton" /&gt; 12345678@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Car car1 = (Car) ioc.getBean("car"); Car car2 = (Car) ioc.getBean("car"); // true System.out.println(car1 == car2);&#125; 1.2. prototype 多实例prototype特点： 在容器创建时，不会实例化Bean 从容器中取出时，才会实例化Bean，而且每取一次，就会创建一个新的Bean 1&lt;bean id="car" class="bean.Car" scope="prototype" /&gt; 12345678@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Car car1 = (Car) ioc.getBean("car"); Car car2 = (Car) ioc.getBean("car"); // false System.out.println(car1 == car2);&#125; 1.3. request / session / global-sessionrequest：在Web环境下，每一个Http Request会创建一个Bean实例session：在Web环境下，每一个Http Session会创建一个Bean实例global-session：作用于集群环境下的session（全局session），如果不是集群环境，相当于session]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Bean依赖]]></title>
    <url>%2F2019%2F03%2F30%2FSpring%2FSpring-Bean%E4%BE%9D%E8%B5%96%2F</url>
    <content type="text"><![CDATA[1. Bean的默认创建顺序Bean的创建顺序默认是由配置顺序决定的。 123&lt;!-- 在IOC容器被创建时，先创建car再创建person --&gt;&lt;bean id="car" class="bean.Car"/&gt;&lt;bean id="person" class="bean.Person"/&gt; 2. Bean依赖2.1. 单依赖A依赖B并不説没有B，A就无法存在。这里的依赖只是Bean创建的先后顺序。只有B先被创建，A才会被创建 123&lt;!-- 先创建person，再创建car --&gt;&lt;bean id="car" class="bean.Car" depends-on="person"/&gt;&lt;bean id="person" class="bean.Person"/&gt; 2.2. 多依赖依赖可以有多个，用逗号分隔1234&lt;!-- 创建先后顺序：person1, person2, car --&gt;&lt;bean id="car" class="bean.Car" depends-on="person1,person2"/&gt;&lt;bean id="person1" class="bean.Person"/&gt;&lt;bean id="person2" class="bean.Person"/&gt; 多个依赖之间也是有顺序的。将person1和person2反一下1234&lt;!-- 创建先后顺序：person2, person1, car --&gt;&lt;bean id="car" class="bean.Car" depends-on="person2,person1"/&gt;&lt;bean id="person1" class="bean.Person"/&gt;&lt;bean id="person2" class="bean.Person"/&gt;]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 继承配置信息]]></title>
    <url>%2F2019%2F03%2F30%2FSpring%2FSpring-%E7%BB%A7%E6%89%BF%E9%85%8D%E7%BD%AE%E4%BF%A1%E6%81%AF%2F</url>
    <content type="text"><![CDATA[1. bean配置信息重用parent只是重用了另一个Bean组件的配置信息，要覆盖的就重新设置，不变的就复用配置信息。但并不是説当前Bean继承另一个Bean，这不是类的继承关系12345678910&lt;bean id="car1" class="bean.Car"&gt; &lt;property name="name" value="car1" /&gt; &lt;property name="price" value="5000.0" /&gt;&lt;/bean&gt;&lt;!-- parent: 指定要重用哪个Bean组件的配置信息 --&gt;&lt;bean id="car2" class="bean.Car" parent="car1"&gt; &lt;!-- 覆盖配置信息 --&gt; &lt;property name="name" value="car2" /&gt;&lt;/bean&gt; 编写测试方法，可以看到属性的配置继承下来了123456@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Car car = (Car) ioc.getBean("car"); System.out.println(car);&#125; 2. abstract声明模板Bean123456789&lt;!-- abstract声明当前Bean不会被Spring实例化，只是作为一个配置模板，供其它Bean组件去继承 --&gt;&lt;bean id="carTemplate" class="bean.Car" abstract="true"&gt; &lt;property name="name" value="car1" /&gt; &lt;property name="price" value="5000.0" /&gt;&lt;/bean&gt;&lt;bean id="car" class="bean.Car" parent="carTemplate"&gt; &lt;property name="name" value="car2" /&gt;&lt;/bean&gt; 尝试获取模板Bean12345@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Car carTemplate = (Car) ioc.getBean("carTemplate");&#125; 运行，抛出异常，abstract Bean不能被实例化，所以无法从IOC容器中获取1org.springframework.beans.factory.BeanIsAbstractException: Error creating bean with name 'carTemplate': Bean definition is abstract]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 内部Bean]]></title>
    <url>%2F2019%2F03%2F30%2FSpring%2FSpring-%E5%86%85%E9%83%A8Bean%2F</url>
    <content type="text"><![CDATA[1. 案例环境12345@Datapublic class Car &#123; private String name; private Double price;&#125; 12345@Datapublic class Person &#123; private Car car; private Object[] arr;&#125; 2. 内部Bean不可获取内部Bean不可获取，只供内部使用，外部不能获取。因此内部Bean设置id是没有意义的，相当于没有设置 2.1. 测试从容器中取出内部Bean12345678&lt;bean id="person" class="bean.Person"&gt; &lt;property name="car"&gt; &lt;bean id="carInner" class="bean.Car"&gt; &lt;property name="name" value="carInner" /&gt; &lt;property name="price" value="5000.0" /&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt; 123456@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Car car = (Car) ioc.getBean("carInner"); System.out.println(car);&#125; 运行，抛出异常。虽然定义了内部Bean，并设置了ID，但是根据ID获取内部Bean时，还是提示Bean没有定义（注册）过，因此可知内部Bean是无法获取的1org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'carInner' available 2.2. 测试使用ref引用内部Bean1234567891011121314&lt;bean id="person" class="bean.Person"&gt; &lt;property name="car"&gt; &lt;bean id="carInner" class="bean.Car"&gt; &lt;property name="name" value="carInner" /&gt; &lt;property name="price" value="5000.0" /&gt; &lt;/bean&gt; &lt;/property&gt; &lt;property name="arr"&gt; &lt;array&gt; &lt;!-- ref引用内部Bean --&gt; &lt;ref bean="carInner"/&gt; &lt;/array&gt; &lt;/property&gt;&lt;/bean&gt; 123456@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Person person = ioc.getBean(Person.class); System.out.println(person);&#125; 运行，也是提示Bean没有定义1org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'person' defined in class path resource [applicationContext.xml]: Cannot resolve reference to bean 'carInner' while setting bean property 'arr' with key [0]; nested exception is org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'carInner' available]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 属性注入]]></title>
    <url>%2F2019%2F03%2F29%2FSpring%2FSpring-%E5%B1%9E%E6%80%A7%E6%B3%A8%E5%85%A5%2F</url>
    <content type="text"><![CDATA[1. 案例环境1.1. 依赖12345&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.1.5.RELEASE&lt;/version&gt;&lt;/dependency&gt; 1.2. 编写Bean123456789package bean;import lombok.Data;@Datapublic class Car &#123; private String name; private Double price;&#125; 123456789101112131415161718192021222324252627282930313233343536373839package bean;import java.util.List;import java.util.Map;import java.util.Properties;import java.util.Set;import lombok.Data;@Datapublic class Person &#123; private String name; private Integer age; private Object[] arr; private int[] intArr; private List&lt;Object&gt; list; private Set&lt;Object&gt; set; private Map&lt;Object, Object&gt; map; private Properties prop; private Car car1; private Car car2; public Person() &#123; System.out.println("person"); &#125; public Person(String name) &#123; System.out.println(name); &#125; public Person(String name, Integer age) &#123; this.name = name; this.age = age; System.out.println("Person(String name, Integer age)"); &#125; public Person(Integer age, String name) &#123; this.name = name; this.age = age; System.out.println("Person(Integer age, String name)"); &#125;&#125; 2. 通过property标签注入1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889&lt;bean id="car" class="bean.Car"&gt; &lt;property name="name" value="car" /&gt; &lt;property name="price" value="5000.0" /&gt;&lt;/bean&gt;&lt;bean id="person" class="bean.Person"&gt; &lt;property name="name" value="foo" /&gt; &lt;property name="age" value="10" /&gt; &lt;property name="arr"&gt; &lt;array&gt; &lt;!-- 元素类型是Object，不指定type，设置类型是字符串，所以这里的10是字符串，而不 是数字 --&gt; &lt;value&gt;10&lt;/value&gt; &lt;value type="int"&gt;100&lt;/value&gt; &lt;value type="double"&gt;3.14&lt;/value&gt; &lt;value&gt;默认是字符串类型&lt;/value&gt; &lt;value type="java.lang.Integer"&gt;200&lt;/value&gt; &lt;value type="java.lang.String"&gt;字符串&lt;/value&gt; &lt;!-- 引用外部Bean --&gt; &lt;ref bean="car" /&gt; &lt;!-- 引用内部Bean --&gt; &lt;bean class="bean.Car"&gt; &lt;property name="name" value="Santana"/&gt; &lt;/bean&gt; &lt;/array&gt; &lt;/property&gt; &lt;property name="intArr"&gt; &lt;!-- 容器元素类型是int，所以不必设置type属性 --&gt; &lt;array&gt; &lt;value&gt;1&lt;/value&gt; &lt;value&gt;2&lt;/value&gt; &lt;/array&gt; &lt;/property&gt; &lt;property name="list"&gt; &lt;list&gt; &lt;value type="int"&gt;100&lt;/value&gt; &lt;value type="double"&gt;3.14&lt;/value&gt; &lt;value&gt;默认是字符串类型&lt;/value&gt; &lt;value type="java.lang.Integer"&gt;200&lt;/value&gt; &lt;value type="java.lang.String"&gt;字符串&lt;/value&gt; &lt;ref bean="car" /&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="set"&gt; &lt;set&gt; &lt;value type="int"&gt;100&lt;/value&gt; &lt;value type="double"&gt;3.14&lt;/value&gt; &lt;value&gt;默认是字符串类型&lt;/value&gt; &lt;value type="java.lang.Integer"&gt;200&lt;/value&gt; &lt;value type="java.lang.String"&gt;字符串&lt;/value&gt; &lt;ref bean="car" /&gt; &lt;/set&gt; &lt;/property&gt; &lt;property name="map"&gt; &lt;!-- map = new LinkedHashMap&lt;&gt;() --&gt; &lt;map&gt; &lt;entry key="key1默认是字符串" value="value默认是字符串" /&gt; &lt;entry key="key2" value="100" value-type="int" /&gt; &lt;entry key="key3" value="100" value-type="java.lang.Integer" /&gt; &lt;entry key="key4"&gt; &lt;array&gt; &lt;value type="int"&gt;10&lt;/value&gt; &lt;value type="java.lang.String"&gt;10&lt;/value&gt; &lt;/array&gt; &lt;/entry&gt; &lt;entry key="key5"&gt; &lt;bean class="bean.Car"&gt; &lt;property name="name" value="hehe" /&gt; &lt;property name="price" value="100.0" /&gt; &lt;/bean&gt; &lt;/entry&gt; &lt;entry key-ref="car" value="111" /&gt; &lt;entry key="car" value-ref="car" /&gt; &lt;entry key-ref="car" value-ref="car" /&gt; &lt;/map&gt; &lt;/property&gt; &lt;property name="prop"&gt; &lt;props&gt; &lt;prop key="aaa"&gt;111&lt;/prop&gt; &lt;prop key="bbb"&gt;222&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;property name="car1" ref="car" /&gt; &lt;property name="car2"&gt; &lt;bean class="bean.Car"&gt; &lt;property name="name" value="haha" /&gt; &lt;property name="price" value="100.0" /&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt; 3. 属性类型声明3.1. value标签的type属性关于数组、集合等容器元素的声明： 如果容器元素类型是Object，则使用value标签为元素赋值时，有必要指定type属性。若不指定，默认是String 如果容器元素类型不是Object，则不必使用type属性。Spring会自动判断类型 12345678910111213141516171819&lt;property name="arr"&gt; &lt;array&gt; &lt;!-- 元素类型是Object，不指定type，设置类型是字符串，所以这里的10是字符串，而不 是数字 --&gt; &lt;value&gt;10&lt;/value&gt; &lt;value type="int"&gt;100&lt;/value&gt; &lt;value type="double"&gt;3.14&lt;/value&gt; &lt;value&gt;默认是字符串类型&lt;/value&gt; &lt;value type="java.lang.Integer"&gt;200&lt;/value&gt; &lt;value type="java.lang.String"&gt;字符串&lt;/value&gt; &lt;ref bean="car" /&gt; &lt;/array&gt;&lt;/property&gt;&lt;property name="intArr"&gt; &lt;!-- 容器元素类型是int，所以不必设置type属性 --&gt; &lt;array&gt; &lt;value&gt;1&lt;/value&gt; &lt;value&gt;2&lt;/value&gt; &lt;/array&gt;&lt;/property&gt; 3.2. 容器标签的key-type / value-type属性array、list、set、props标签都有value-type属性，用来声明元素的类型123456&lt;!-- value-type: 统一声明数组元素的类型 --&gt;&lt;array value-type="int"&gt; &lt;value&gt;100&lt;/value&gt; &lt;value&gt;200&lt;/value&gt; &lt;value&gt;#&#123;200 + 300&#125;&lt;/value&gt;&lt;/array&gt; map可以声明key-type和value-type1234&lt;map key-type="int" value-type="java.lang.String"&gt; &lt;entry key="111" value="aaa" /&gt; &lt;entry key="222" value="bbb" /&gt;&lt;/map&gt; 4. null空引用注入错误示例：value设置为”null”，这是字符串，而不是空引用12345&lt;bean id="car" class="bean.Car"&gt; &lt;!-- 字符串"null" --&gt; &lt;property name="name" value="null"/&gt; &lt;property name="price" value="5000.0" /&gt;&lt;/bean&gt; 正确示例：123456&lt;bean id="car" class="bean.Car"&gt; &lt;property name="name"&gt; &lt;null /&gt; &lt;/property&gt; &lt;property name="price" value="5000.0" /&gt;&lt;/bean&gt; 5. 有参构造方法注入5.1. 方式1：constructor-arg指定name/value（最常用）12345&lt;bean id="person" class="bean.Person"&gt; &lt;!-- 匹配Person(String name, Integer age), name=10, age=20 --&gt; &lt;constructor-arg name="name" value="10" /&gt; &lt;constructor-arg name="age" value="20" /&gt;&lt;/bean&gt; 5.2. 方式2：constructor-arg只指定value12345&lt;bean id="person" class="bean.Person"&gt; &lt;!-- 匹配Person(String name, Integer age), name=10, age=20 --&gt; &lt;constructor-arg value="10" /&gt; &lt;constructor-arg value="20" /&gt;&lt;/bean&gt; 5.3. 方式3: constructor-arg指定value和type12345&lt;bean id="person" class="bean.Person"&gt; &lt;!-- 匹配Person(String name, Integer age), name=20, age=10 --&gt; &lt;constructor-arg value="10" type="java.lang.Integer" /&gt; &lt;constructor-arg value="20" type="java.lang.String" /&gt;&lt;/bean&gt; 5.4. 方式4: constructor-arg指定index12345&lt;bean id="person" class="bean.Person"&gt; &lt;!-- 匹配Person(Integer age, String name), name=20, age=10 --&gt; &lt;constructor-arg value="10" type="java.lang.Integer" index="0" /&gt; &lt;constructor-arg value="20" type="java.lang.String" index="1" /&gt;&lt;/bean&gt; 6. ref引用外部Bean组件123456789&lt;bean id="car" class="bean.Car"&gt; &lt;property name="name" value="benz" /&gt; &lt;property name="price" value="5000.0" /&gt;&lt;/bean&gt;&lt;bean id="person" class="bean.Person"&gt; &lt;!-- 引用car组件 --&gt; &lt;property name="car1" ref="car" /&gt;&lt;/bean&gt; 编写测试方法，可以看到person的car1其实就是car组件，car1并没有创建新的对象12345678@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Person person = ioc.getBean("person", Person.class); Car car = ioc.getBean(Car.class); // 输出true。说明两个引用指向同一对象 System.out.println(person.getCar1() == car);&#125; 7. 引用内部Bean组件123456789&lt;bean id="person" class="bean.Person"&gt; &lt;property name="car1"&gt; &lt;!-- 内部再创建一个Bean组件，与外部的car是不同的对象 --&gt; &lt;bean id="car1" class="bean.Car"&gt; &lt;property name="name" value="benz" /&gt; &lt;property name="price" value="5000.0" /&gt; &lt;/bean&gt; &lt;/property&gt;&lt;/bean&gt;]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 两种Bean工厂]]></title>
    <url>%2F2019%2F03%2F29%2FSpring%2FSpring-%E4%B8%A4%E7%A7%8DBean%E5%B7%A5%E5%8E%82%2F</url>
    <content type="text"><![CDATA[1. BeanFactory与ApplicationContext1.1. Bean工厂继承图 1.2. 两种工厂的区别BeanFactory与ApplicationContext都是接口。它们都是bean工厂，或者说bean容器（IOC容器）。它按照我们的要求，生产我们需要的各种各样的bean，提供给我们使用。只是在生产bean的过程中，需要解决bean之间的依赖问题，才引入了依赖注入(DI)这种技术。也就是说依赖注入是beanFactory生产bean时为了解决bean之间的依赖的一种技术而已 BeanFactory是Spring原始的接口。具有的功能非常单一。实现BeanFactory的类，特点是在获取bean时才创建bean。这是因为BeanFactory出现的年代较早，当时内存资源匮乏，用到bean再创建bean ApplicationContext是Spring的新型接口。具有较多的功能。ApplicationContext容器在启动时就会创建所有配置的对象，而不是等到用户获取bean时再创建]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 加载Bean配置文件]]></title>
    <url>%2F2019%2F03%2F29%2FSpring%2FSpring-%E5%8A%A0%E8%BD%BDBean%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[1. 加载Bean配置文件的两种方式ApplicationContext是IOC容器的接口，主要有两个实现类 ClassPathXmlApplication: 从类路径加载Bean配置文件 FileSystemXmlApplicationContext: 从文件系统加载Bean配置文件 1234// 从类路径加载ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml");// 从文件系统加载ApplicationContext ioc = new FileSystemXmlApplicationContext("C:\\applicationContext.xml");]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java classpath]]></title>
    <url>%2F2019%2F03%2F29%2FJava%2FJava-classpath%2F</url>
    <content type="text"><![CDATA[1. 源码目录（Source folders）和生成路径（build path）Source folders所有的文件，编译后都会统一放到对应的build path中 build path就是类路径的开始。所以在写代码时，将Source folders视为类路径的开始]]></content>
      <tags>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 从容器中获取Bean]]></title>
    <url>%2F2019%2F03%2F29%2FSpring%2FSpring-%E4%BB%8E%E5%AE%B9%E5%99%A8%E4%B8%AD%E8%8E%B7%E5%8F%96Bean%2F</url>
    <content type="text"><![CDATA[1. 案例：通过ID从容器中获取Bean1.1. 添加依赖 根据依赖传递，Spring核心容器只需要添加spring-context即可。在Spring5之前，核心容器还依赖commons-logging日志包，Spring5现在将commons-logging的内容集成到spring-jcl这个包下了。根据依赖传递，spring-jcl这个依赖也已经添加了，所以最后也只用添加spring-context这个依赖123456&lt;!-- Spring 核心依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.1.5.RELEASE&lt;/version&gt;&lt;/dependency&gt; 1.2. 编写Bean类123456789package bean;import lombok.Data;@Datapublic class Person &#123; private String name; private Integer age;&#125; 1.3. 创建Spring Bean配置文件在classpath下创建Bean配置文件。普通Java工程就放到src目录下，Maven工程就放到src/main/resources目录下 Eclipse如果安装了STS插件，可以直接New-&gt;Spring Bean Configuration File123456&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;/beans&gt; 1.4. 在Bean配置文件中注册Bean123456789&lt;!-- &lt;bean&gt;: 注册一个bean，Spring会自动创建该对象，放入容器中 id: bean的标识 class: 全类名 --&gt;&lt;bean id="person" class="bean.Person"&gt; &lt;property name="age" value="10" /&gt; &lt;property name="name" value="foo" /&gt;&lt;/bean&gt; 1.5. 代码测试从容器中取出Bean12345678@Testpublic void testIoc() throws Exception &#123; // 创建容器，指定Bean配置文件的的路径 ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); // 在创建容器时，Bean就被创建了，此时可以直接根据id取出bean Person person = (Person) ioc.getBean("person"); System.out.println(person);&#125; 2. 案例：通过类对象从容器中获取Bean除了ID，还可以通过类对象从容器中获取Bean。这种方式可以发现一个好处，即返回值不用类型转换1234567@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); // 通过类对象获取Bean Person person = ioc.getBean(Person.class); System.out.println(person);&#125; 3. 案例：同时指定ID和类对象，从容器中获取Bean12345678&lt;bean id="person1" class="bean.Person"&gt; &lt;property name="age" value="10" /&gt; &lt;property name="name" value="foo" /&gt;&lt;/bean&gt;&lt;bean id="person2" class="bean.Person"&gt; &lt;property name="age" value="20" /&gt; &lt;property name="name" value="bar" /&gt;&lt;/bean&gt; 123456789@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); // 第1个参数指定ID，第2个参数指定类型，返回值不必类型转换 Person person1 = ioc.getBean("person1", Person.class); Person person2 = ioc.getBean("person2", Person.class); System.out.println(person1); System.out.println(person2);&#125; 4. 案例总结4.1. 谁创建了Bean？我没有主动创建Person对象，是谁创建的呢？是Spring IOC容器 4.2. Bean是什么时候被创建的？是如何被创建的？在IOC容器被创建时，即执行new ClassPathXmlApplicationContext时，所有在配置文件中注册过的Bean都会被创建 这里要注意，Spring有两种工厂。BeanFactory是在获取bean时才创建bean，而ApplicationContext容器在创建时就会创建所有配置的Bean Bean又是如何被创建的呢？我们可以在Person中编写空参构造方法，输出一些内容12345678@Datapublic class Person &#123; public Person() &#123; System.out.println("Person被创建了"); &#125; private String name; private Integer age;&#125; 再写一个测试方法。根据运行结果可知，IOC容器在创建时就创建了Bean，而且是通过反射，调用Bean的空参构造方法，创建一个实例1234@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml");&#125; 4.3. 同一个Bean组件在IOC容器中是单实例的？两次使用同一ID，从容器中取出Bean组件，得到的两个对象是相同的，说明Bean组件在IOC容器是单实例的1234567@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Person person1 = (Person) ioc.getBean("person"); Person person2 = (Person) ioc.getBean("person"); System.out.println(person1 == person2);&#125; 4.4. Bean的属性是如何被设置的？在配置文件中，我们是通过property标签为Bean设置属性的。但这只是配置，我们并不知道IOC容器在程序中具体是如何为Bean设置属性的1234&lt;bean id="person" class="bean.Person"&gt; &lt;property name="age" value="10" /&gt; &lt;property name="name" value="foo" /&gt;&lt;/bean&gt; 给Person添加一个setter，输出一些内容123456789@Datapublic class Person &#123; private String name; private Integer age; public void setName(String name) &#123; System.out.println("setName"); this.name = name; &#125;&#125; 运行测试方法，setter被调用了，可知IOC容器是通过反射，调用Bean的setter，将属性注入1234@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml");&#125; 4.5. 如果容器中没有你要的组件，会出现什么现象？先猜一下，如果没有你要的组件，可能会返回null 编写测试方法，取一个没有注册过的组件12345@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Object bean = ioc.getBean("abc");&#125; 运行。结果并不是返回null，而是直接抛出异常1org.springframework.beans.factory.NoSuchBeanDefinitionException: No bean named 'abc' available 4.6. 如果定义了两个同类型的Bean组件，通过类对象获取Bean会出现什么现象注册两个同类型的Bean，但是id不同12345678&lt;bean id="person1" class="bean.Person"&gt; &lt;property name="age" value="10" /&gt; &lt;property name="name" value="foo" /&gt;&lt;/bean&gt;&lt;bean id="person2" class="bean.Person"&gt; &lt;property name="age" value="20" /&gt; &lt;property name="name" value="bar" /&gt;&lt;/bean&gt; 编写测试方法，通过类对象获取Bean1234567@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); // 通过类对象获取Bean Person person = ioc.getBean(Person.class); System.out.println(person);&#125; 运行，抛出异常。即通过类对象获取Bean时，要求容器中该类型的Bean组件只能有1个，如果有多个，就会报错。1org.springframework.beans.factory.NoUniqueBeanDefinitionException: No qualifying bean of type 'bean.Person' available: expected single matching bean but found 2: person1,person2 在这种情况下，只能通过ID获取12345678@Testpublic void testIoc() throws Exception &#123; ApplicationContext ioc = new ClassPathXmlApplicationContext("applicationContext.xml"); Person person1 = (Person) ioc.getBean("person1"); Person person2 = (Person) ioc.getBean("person2"); System.out.println(person1); System.out.println(person2);&#125;]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 概述]]></title>
    <url>%2F2019%2F03%2F29%2FSpring%2FSpring-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1. 什么是Spring（Spring Framework）Spring是用于构建企业级应用的轻量级一站式框架（或者说解决方案）。核心是控制反转IoC和面向切片AOP 2. 为什么要用Spring方便解耦，简化开发。Spring就是一个大工厂，可以将所有对象创建和依赖关系维护交给Spring管理。 AOP编程的支持。Spring提供面向切编程，可以方便的实现对序进行权限拦截、运监控等功能。 声明式事务的支持。只需要通过配置就可以完成对事务的管理，而无需手动编程。 方便程序的测试。Spring对Junit4支持，可以通过注解进行测试。 方便集成各种优秀框架。Spring不排斥各种优秀的开源框架，其内部提供了对各种优秀框架（如：Struts、Hibernate、MyBatis、Quartz等）的直接支持。 降低JavaEE API的使用难度。Spring 对 JavaEE开发中非常难用的一些API（JDBC、JavaMail、远程调用等），都提供了封装，使这些API应用难度大大降低。 3. Spring家族 Spring不仅代表框架（Spring Framework），也代表了与之相关的子项目，比如Spring Boot、Spring Cloud、Spring Data Spring倡导的现代Java设计： Spring Boot 构建一切 Spring Cloud 协调一切 Spring Cloud Data Flow 连接一切 这也体现了从单机到群集到云的变化趋势 4. Spring5的改变 改动点 改变的意义 一些反思 Java8+、Kotlin 语言在进步 要了解高版本JDK的新特性 支持WebFlux Java异步编程模式的崛起 全面落地尚需时日 去掉了很多支持 Portlet过时了、Velocity不维护了、JasperReport不流行了 库有很多，该怎么选？ 模板引擎Velocity不维护了，可以选择一些更新的模板引擎 后端报表JasperReport不流行了，现在报表一般都是在前端完成，例如百度的ECharts，后端只负责提供数据即可 5. Spring核心设计思想5.1. IoC（Inversion of Control）控制反转以前对象的创建由开发人员自己维护，依赖关系（类成员有其它类）也是自己注入。而spring可以直接完成对象的创建和依赖注入。所谓IoC就是反转了对象的创建方式和依赖注入方式，反转给spring去处理。 5.2. DI（Dependency Injection）依赖注入DI是一种技术，体现了IoC思想。 5.3. AOP（Aspect Oriented Programming）面向切面编程AOP：指在程序运行期间，将某段代码动态切入到指定方法的指定位置，这种方式，就叫面向切面编程 AOP是基于OOP基础之上的编程思想 AOP应用场景： 日志记录：与业务逻辑分离，以AOP的方式统一添加日志记录功能]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Eclipse Usage]]></title>
    <url>%2F2019%2F03%2F29%2FSoftware%2FEclipse%20Usage%2F</url>
    <content type="text"><![CDATA[1. Eclipse Mirror加速Eclipse 国内源有以下几个： 中国科学技术大学 http://mirrors.ustc.edu.cn/eclipse/ 北京理工大学 http://mirror.bit.edu.cn/eclipse/ 大连东软信息学院 http://mirrors.neusoft.edu.cn/eclipse/ 导出Availble Software Sites，作为备份12345678&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;bookmarks&gt; &lt;site url="http://download.eclipse.org/releases/2018-09" selected="true" name="2018-09"/&gt; &lt;site url="http://download.eclipse.org/eclipse/updates/4.9/R-4.9-201809060745" selected="true" name="Eclipse Project Repository for 2018-09"/&gt; &lt;site url="http://download.eclipse.org/releases/latest" selected="false" name="Latest Eclipse Release"/&gt; &lt;site url="http://download.eclipse.org/eclipse/updates/4.9" selected="true" name="The Eclipse Project Updates"/&gt; &lt;site url="http://download.eclipse.org/webtools/repository/photon" selected="true" name="The Eclipse Web Tools Platform (WTP) software repository"/&gt;&lt;/bookmarks&gt; 将URL中的 http://download.eclipse.org 替换为 http://mirror.bit.edu.cn/eclipse 12345678&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;bookmarks&gt; &lt;site url="http://mirror.bit.edu.cn/eclipse/releases/2018-09" selected="true" name="2018-09"/&gt; &lt;site url="http://mirror.bit.edu.cn/eclipse/eclipse/updates/4.9/R-4.9-201809060745" selected="true" name="Eclipse Project Repository for 2018-09"/&gt; &lt;site url="http://mirror.bit.edu.cn/eclipse/releases/latest" selected="false" name="Latest Eclipse Release"/&gt; &lt;site url="http://mirror.bit.edu.cn/eclipse/eclipse/updates/4.9" selected="true" name="The Eclipse Project Updates"/&gt; &lt;site url="http://mirror.bit.edu.cn/eclipse/webtools/repository/photon" selected="true" name="The Eclipse Web Tools Platform (WTP) software repository"/&gt;&lt;/bookmarks&gt; 将Availble Software Sites中的每一项全部删除，导入新的xml配置即可 2. Eclipse 下载直接在国内源下载Eclipse，速度比官网要快。下载之前可以先进入各个国内源网站，找一个速度最快的。 进入 http://mirror.bit.edu.cn/eclipse/technology/epp/downloads/release/ ，找到自己想要的版本 以eclipse-jee-2018-09为例 Win64版本：eclipse-jee-2018-09-RC1-win32-x86_64.zip Linux64版本：eclipse-jee-2018-09-RC1-linux-gtk-x86_64.tar.gz 下载完直接解压，双击eclipse.exe即可运行 3. Eclipse Maven配置3.1. 改用外置MavenEclipse默认使用内置Maven。现在改用外置的Maven，Preferences-&gt;Maven-&gt;Installations-&gt;点击Add添加，浏览外置Maven的安装路径 3.2. 设置settings.xmlPreferences-&gt;Maven-&gt;User Settings 设置Global Settings指定外置Maven的settings.xml 设置本地仓库路径 4. Eclipse 插件安装汇总4.1. 安装STS插件进入 https://spring.io/tools3/sts/all/ ，找到Update Site Archives 那么应该下载哪个包呢？以其中一个包名为例： springsource-tool-suite-3.9.8.RELEASE-e4.9.0-updatesite.zip 前面的3.9.8代表STS插件的版本 后面的e4.9.0代表Eclipse的版本 只要对准Eclipse的版本，下载对应的STS插件包即可。打开Eclipse，菜单栏Help-&gt;About Eclipse IDE，查看Version，就可以看到对应的版本号。我的Eclipse是4.9.0，就要下载springsource-tool-suite-3.9.8.RELEASE-e4.9.0-updatesite.zip 下面就是Eclipse离线安装插件的常规操作了。Help-&gt;Install New Software-&gt;点击Add-&gt;点击Archive，浏览插件zip，Name不必设置 然后会列出很多项，只要勾选名称带有 “Spring IDE” 的即可 Core / Spring IDE Extensions / Spring IDE Integrations / Spring IDE Resources / Spring IDE 切记去掉勾选”Contact all update-sites ….” ，否则会联网更新相关东西，没有必要，甚至可能导致安装失败 验证是否安装成功： Help-&gt;About Eclipse IDE-&gt;查看是否有Spring IDE Developers Preferences-&gt;查看是否有Spring选项 New Project-&gt;查看是否有Spring及SpringBoot选项 4.2. 安装Lombok插件到Lombok官网下载 lombok.jar，直接下载最新版本就可以https://projectlombok.org/download 将lombok.jar移动到Eclipse的安装路径，也就是与eclipse.exe / eclipse.ini放在同一路径下 编辑eclipse.ini，加入以下配置12-Xbootclasspath/a:lombok.jar-javaagent:lombok.jar 重启Eclipse，添加Lombok依赖，就可以使用Lombok注解了 4.3. 安装Alibaba P3C插件参考文档：P3C Github的wiki页面https://github.com/alibaba/p3c/wiki/Eclipse%E6%8F%92%E4%BB%B6%E4%BD%BF%E7%94%A8%E6%96%87%E6%A1%A3 Help-&gt;Install New Software打开插件安装界面 在Work with处直接输入Update Site地址https://p3c.alibaba.com/plugin/eclipse/update ，按回车，然后勾选Ali-CodeAnalysis，再一直点Next Next…按提示走下去就好。 然后就是提示重启了，安装完毕。注意”contact all update sites during install to find required software”去掉勾选，不然会很慢。 5. Eclipse Web开发5.1. Eclipse Tomcat Web环境配置5.1.1. 配置Tomcat服务器运行环境（Server Runtime Environments）Preferences-&gt;Server-&gt;Runtime Environments-&gt;Add-&gt;浏览Apache Tomcat目录，添加即可 5.1.2. 配置Tomcat服务器运行参数Window-&gt;Show View-&gt;打开Servers视图-&gt;添加Tomcat服务 添加完后，双击Tomcat v8.0 Server at localhost [Stopped]，出现配置界面。关键是Server Locations，选中Use Tomcat installation，Deploy Path设置为webapps 5.1.3. 为工程添加Tomcat Web运行依赖工程右键-&gt;Build Path-&gt;Add Library 添加Web App Libraries 添加Server Runtime-&gt;Tomcat 5.2. 创建Maven Web工程创建一个Maven工程，打包方式设置为war，就是一个Web工程 创建项目之后，pom.xml报错，是因为项目没有web.xml。一种方式是手动在/src/main/webapp目录下创建WEB-INF目录，并添加web.xml如下：12345678910&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;web-app xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns="http://java.sun.com/xml/ns/javaee" xsi:schemaLocation="http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd" version="2.5"&gt; &lt;display-name&gt;springmvc-learning&lt;/display-name&gt; &lt;welcome-file-list&gt; &lt;welcome-file&gt;index.html&lt;/welcome-file&gt; &lt;/welcome-file-list&gt;&lt;/web-app&gt; Eclipse-JEE版本提供了快速创建web.xml的方式。工程右键-&gt;Java EE Tools-&gt;Generate Deployment Descriptor Stub 5.2.1. 添加tomcat插件添加完插件，右键项目-&gt;Maven-&gt;Update Project-&gt;选项当前项目-&gt;OK12345678910111213&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;port&gt;8080&lt;/port&gt; &lt;path&gt;/&lt;/path&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 右键项目-&gt;Run As-&gt;Maven build…….-&gt;在Goals中输入1clean tomcat7:run 5.2.2. tomcat插件热部署所谓热部署就是在tomcat服务器不停机的情况下，将新的war包放上去，达到服务不中断，用户无察觉的目的 修改Tomcat安装目录下的的conf/tomcat-users.xml1234&lt;!-- 添加以下内容 --&gt;&lt;role rolename="manager-gui" /&gt;&lt;role rolename="manager-script" /&gt;&lt;user username="root" password="123456" roles="manager-gui, manager-script"/&gt; 运行startup.bat，先启动tomcat服务器 修改pom.xml中的tomcat插件配置12345678910111213&lt;plugin&gt; &lt;groupId&gt;org.apache.tomcat.maven&lt;/groupId&gt; &lt;artifactId&gt;tomcat7-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt; &lt;configuration&gt; &lt;port&gt;8080&lt;/port&gt; &lt;path&gt;/&lt;/path&gt; &lt;!-- tomcat的地址和端口，manager/text是固定的 --&gt; &lt;url&gt;http://localhost:8080/manager/text&lt;/url&gt; &lt;username&gt;root&lt;/username&gt; &lt;password&gt;123456&lt;/password&gt; &lt;/configuration&gt;&lt;/plugin&gt; 运行以下maven命令1clean tomcat7:redeploy -Dmaven.test.skip=true 5.3. 配置JSP默认编码Preferences-&gt;Web-&gt;JSP Files-&gt;设置Encoding为UTF-8（默认IOS-8859-1）]]></content>
      <tags>
        <tag>Eclipse</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 评分机制]]></title>
    <url>%2F2019%2F03%2F29%2FElasticSearch%2FElasticSearch-%E8%AF%84%E5%88%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[1. ElasticSearch6 评分机制1.1. IDF1IDF = ln(1 + (docCount - docFreq + 0.5) / (docFreq + 0.5)) docCount：总文档数（在分片数=1的情况下是这样） What does “docCount” mean in the Explain API? What does “docCount” and “docFreq” mean in the Explain API? docFreq：满足本条term的查询文档数目（当前term召回的文档数） IDF反映的是term的影响因子，如果docCount很大，docFreq很小，标示该term在doc之间具有很好的分辨力，当然IDF值也就越大 1.2. tfNorm1(freq * (k1 + 1)) / (freq + k1 * (1 - b + b * fieldLength / avgFieldLength)) freq: 当前term在当前doc的field中出现的次数k1: 调优参数，默认为1.2b: 调优参数，默认为0.75fieldLength: 当前doc的field长度avgFieldLength: 所有文档的field平均长度 tfNorm反映的该term在所有满足条件的doc中field中的重要性，一般来说，相同的freq下，field的长度越短，那么取值就越高 1.3. BM25]]></content>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase 运行环境搭建]]></title>
    <url>%2F2019%2F03%2F28%2Fhadoop%2FHBase-%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1. 安装HBase下载1wget https://mirrors.aliyun.com/apache/hbase/stable/hbase-1.2.6.1-bin.tar.gz 解压12tar -zxvf hbase-1.2.6-bin.tar.gz -C /usr/localmv /usr/local/hbase-1.2.6.1 /usr/local/hbase 编辑/etc/profile，添加以下内容123# hbase configurationexport HBASE_HOME=/usr/local/hbaseexport PATH=$PATH:$HBASE_HOME/bin 使配置生效1source /etc/profile 检测配置1hbase version 2. 伪分布式搭建2.1. 编辑conf/hbase-env.sh123456# 配置JAVA_HOMEexport JAVA_HOME=/usr/lib/java/jdk# 设置zookeeper。若为true，则使用hbase自带的zookeeper组件。若为false，则使用独立的zookeeperexport HBASE_MANAGES_ZK=true# 设置hbase的CLASSPATH，设置为本机Hadoop安装目录下的conf目录。方便找到hadoop的配置信息export HBASE_CLASSPATH=/usr/local/hadoop/conf 2.2. 编辑conf/hbase-site.xml1234567891011121314151617&lt;configuration&gt; &lt;!-- 指定HBase数据在HDFS上的存储路径 --&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://ubuntu:9000/hbase&lt;/value&gt; &lt;/property&gt; &lt;!-- 集群以分布式模式运行 --&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定ZK集群各种节点的地址，多个节点以逗号分隔 --&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;ubuntu&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2.3. 启动HBaseHBase依赖于HDFS，所以要先启动HDFS12start-dfs.shstart-hbase.sh 访问 Web UI，HBase1.x之前web端口是60010，之后修改为160101http://ubuntu:16010]]></content>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HBase 概述]]></title>
    <url>%2F2019%2F03%2F27%2Fhadoop%2Fhbase%2FHBase-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1. 什么是HBase简单来讲，HBase就是一个数据库。数据库不外乎两个作用：存储数据和管理数据 既然是数据库，已经有了RDBMS，如MySQL/Oracle，为什么还要HBase呢？ 以Oracle为例，如果一张表有上亿条数据，你再怎么建索引，查询也是慢的。也就是説，传统的RDBMS不适合存储海量数据，而HBase就是专门应对海量数据的数据库 在Apache HBase™官网，是这样介绍HBase的：Apache HBase is the Hadoop database, a distributed（分布式的）, scalable（可扩展的）, big data store Apache HBase官网还提及在什么情况下使用HBase：Use Apache HBase™ when you need random, realtime read/write access to your Big Data（当你需要对大数据进行随机、实时读写时，就可以使用HBase） 2. HBase起源HBase的原型是Google的BigTable论文，受到了该论文思想的启发，目前作为Hadoop的子项目来开发维护，用于支持结构化的数据存储。官方网站：http://hbase.apache.org 2006年Google发表BigTable白皮书 2006年开始开发HBase 2008年北京成功开奥运会，程序员默默地将HBase弄成了Hadoop的子项目 2010年HBase成为Apache顶级项目 现在很多公司二次开发出了很多发行版本，你也开始使用了 3. 基于Hadoop的HBase架构HBase的数据存储在RegionServer，RegionServer的数据最终存储在HDFS上 一开始可以暂时这样理解 HMater相当于HDFS的NameNode HRegionServer相当于HDFS的DataNode HBase内置有zookeeper，但一般我们会有其他的Zookeeper集群来监管master和regionserver，Zookeeper通过选举，保证任何时候，集群中只有一个活跃的HMaster，HMaster与HRegionServer 启动时会向ZooKeeper注册，存储所有HRegion的寻址入口，实时监控HRegionserver的上线和下线信息。并实时通知给HMaster，存储HBase的schema和table元数据，默认情况下，HBase 管理ZooKeeper 实例，Zookeeper的引入使得HMaster不再是单点故障。一般情况下会启动两个HMaster，非Active的HMaster会定期的和Active HMaster通信以获取其最新状态，从而保证它是实时更新的，因而如果启动了多个HMaster反而增加了Active HMaster的负担。 一个RegionServer可以包含多个HRegion，每个HRegion维护一个HLog，和多个HFiles以及其对应的MemStore。RegionServer运行于DataNode上，数量可以与DatNode数量一致，请参考如下架构图： 4. RDBMS和HBase对比结构对比 RDBMS HBase 数据库以表的形式存在 数据库以region的形式存在 支持FAT、NTFS、EXT、文件系统 支持HDFS文件系统 使用Commit log存储日志 使用WAL（Write-Ahead Logs）存储日志 参考系统是坐标系统 参考系统是Zookeeper 使用主键（PK） 使用行键（row key） 支持分区 支持分片 使用行、列、单元格 使用行、列、列族和单元格 数据以各种类型存储 数据全部以byte[]（字节数组）存储 RDBMS参考系统是坐标系统，即RDBMS运行在哪个操作系统，哪个系统就叫坐标系统 每条数据都有唯一的标识。RDBMS中是主键，HBase中是行键 RDBMS中，数据有各种类型。在HBase中，所有数据都是以字节数组的形式存储，无论是什么数据，都要先转化为byte[]，再存入HBase 功能对比 RDBMS HBase 支持向上扩展 支持向外扩展 使用SQL查询 使用API和MapReduce来访问HBase表数据 面向行，即每一行都是一个连续单元 面向列，即每一列都是一个连续的单元 数据总量依赖于服务器配置 数据总量不依赖具体某台机器，而取决于机器数量 具有ACID支持 HBase不支持ACID（Atomicity、Consistency、Isolation、rability） 适合结构化数据 适合结构化数据和非结构化数据 传统关系型数据库一般都是中心化的 一般都是分布式的 支持事务 HBase不支持事务 支持Join 不支持Join 5. HBase特征简要5.1. 自动故障处理和负载均衡HBase运行在HDFS上，所以HBase中的数据以多副本形式存放，数据也服从分布式存放，数据的恢复也可以得到保障。另外，HMaster和RegionServer也是多副本的。 5.2. 自动分区HBase表是由分布在多个RegionServer中的region组成的，这些RegionServer又分布在不同的DataNode上，如果一个region增长到了一个阈值，为了负载均衡和减少IO，HBase可以自动或手动干预的将region切分为更小的region，也称之为subregion。 5.3. 集成Hadoop/HDFS虽然HBase也可以运行在其他的分布式文件系统之上，但是与HDFS结合非常之方便，而且HDFS也非常之流行。 5.4. 实时随机大数据访问HBase采用log-structured merge-tree作为内部数据存储架构，这种架构会周期性地将小文件合并成大文件以减少磁盘访问同时减少NameNode压力。 5.5. MapReduceHBase内建支持MapReduce框架，更加方便快速，并行的处理数据。 5.6. Java APIHBase提供原声的Java API支持，方便开发。 5.7. 横向扩展HBase支持横向扩展，这就意味着如果现有服务器硬件性能出现瓶颈，不需要停掉现有集群提升硬件配置，而只需要在现有的正在运行的集群中添加新的机器节点即可，而且新的RegionServer一旦建立完毕，集群会开始重新调整。 5.8. 列存储HBase是面向列存储的，每个列都单独存储，所以在HBase中列是连续存储的，而行不是。 5.9. HBase ShellHBase提供了交互式命令行工具可以进行创建表、添加数据、扫描数据、删除数据等操作和其他一些管理命令。 6. HBase在集群中的定位HBase一种是作为存储的分布式文件系统，另一种是作为数据处理模型的MR框架。因为日常开发人员比较熟练的是结构化的数据进行处理，但是在HDFS直接存储的文件往往不具有结构化，所以催生出了HBase在HDFS上的操作。如果需要查询数据，只需要通过键值便可以成功访问。]]></content>
      <tags>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce Writable对象复用]]></title>
    <url>%2F2019%2F03%2F26%2Fhadoop%2FMapReduce-Writable%E5%AF%B9%E8%B1%A1%E5%A4%8D%E7%94%A8%2F</url>
    <content type="text"><![CDATA[1. Map和Reduce端 key/value Writable对象复用以WordCount为例，我一开始写MapReduce时，会犯这样的毛病，即所有的key/value都单独创建了一个Writable对象123456public void map(...) &#123; for (String word : words) &#123; // 问题所在：每次创建新的Writable对象，再写出 context.write(new Text(word), new IntWritable(1)); &#125;&#125; 但是查看官方WordCount源码时，发现key/value都是复用的123456789101112131415public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; &#123; // 创建 key/value Writable对象，用于复用 private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) &#123; word.set(itr.nextToken()); // 复用key/value对象，直接输出 context.write(word, one); &#125; &#125;&#125; 我一开始担心重用Writable对象，会导致数据的覆盖。实际上在执行context.write()之后，Writable对象就已经被序列化了。所以下一次Writable用新的数据覆盖时，不用担心旧数据已经丢失。 所以Writable对象能复用就复用，这样少new一些对象，能给JVM节省垃圾回收的开销。 2. Reduce端 Iterable values中的每个Writable对象的复用平时Reduce端编写程序时，我一般都是这么处理的，就是对values进行迭代，在迭代中完成业务处理，然后直接context.write()写出。这样确实没有发现什么奇怪的问题，程序一直也是正常运行的1234567protected void reduce(KEYIN key, Iterable&lt;VALUEIN&gt; values, Context context) throws IOException, InterruptedException &#123; // Do something .... for (VALUEIN value : values) &#123; // Do something .... context.write(k, v) &#125;&#125; 但是当我将每个value放到List中，再输出时，发现奇怪的现象12345678910protected void reduce(KEYIN key, Iterable&lt;VALUEIN&gt; values, Context context) throws IOException, InterruptedException &#123; // Do something .... for (VALUEIN value : values) &#123; list.add(value); // 输出value，看到每个value的数据是不同的 System.out.println(value); &#125; // 输出list中的每个value，看到所有value都是相同的 list.forEach(System.out::println);&#125; 遍历list时，输出的value都是相同的，而且都是values中的最后一个元素。结合开发经验，容易得知list中的每个引用指向同一个对象。反推回去，说明values中的每个引用指向同一对象。 如果每values中的每个引用指向同一对象，那么在values的for循环中，输出的value应该是相同的，但是事实上输出的数据是不同的。 这让我很诧异。查看values的类型，实际上是org.apache.hadoop.mapreduce.task.ReduceContextImpl$ValueIterable的一个实例。 查看ReduceContextImpl的源码，就会明白，原来每次执行迭代，都会反序列化得到新的value，但是value对象引用始终没有发生变化。 12345678910111213141516class ReduceContextImpl&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; extends TaskInputOutputContextImpl&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; implements ReduceContext&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; &#123; private KEYIN key; // current key private VALUEIN value; // current value @Override public VALUEIN next() &#123; // ...... // 反序列化得到新的value value = valueDeserializer.deserialize(value); // ...... return value; &#125;&#125; 所以如果想让list保存每一个value的数据，那么在每一次迭代时，就要保存value的对象副本1234for (VALUEIN value : values) &#123; // 保存value的副本 list.add(value.clone());&#125;]]></content>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce Join操作]]></title>
    <url>%2F2019%2F03%2F25%2Fhadoop%2FMapReduce-Join%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1. Reduce Join1.1. 工作原理Map端的主要工作：为来自不同表(文件)的key/value对打标签以区别不同来源的记录。然后用连接字段（两张表中相同的列）作为key，其余部分和新加的标志作为value，最后进行输出。 Reduce端的主要工作：在reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录(在map阶段已经打标志)分开，最后进行合并就ok了 1.2. Reduce Join 案例1.2.1. 数据说明订单数据表 order.txt 订单oid 产品pid 产品数量amount 1001 01 1 1234561001 01 11002 02 21003 03 31004 01 41005 02 51006 03 6 商品信息表 product.txt 产品pid 产品名称pname 01 小米 12301 小米02 华为03 格力 输出要求：将商品信息表中数据根据商品pid合并到订单数据表中 订单oid 产品名称pname 数量amount 1001 小米 1 1004 小米 4 1002 华为 2 1005 华为 5 1003 格力 3 1006 格力 6 1.2.2. 编程程序1.2.2.1. TableBean1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package mr.join;import lombok.Data;import org.apache.hadoop.io.Writable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;@Datapublic class TableBean implements Cloneable, Writable &#123; /** * 订单ID */ private String oid; /** * 产品ID */ private String pid; /** * 产品数量 */ private Integer amount; /** * 产品名称 */ private String pname; /** * 标记数据来源于哪个表 */ private String flag; @Override protected TableBean clone() &#123; TableBean tableBean = null; try &#123; tableBean = (TableBean) super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return tableBean; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeUTF(oid); out.writeUTF(pid); out.writeInt(amount); out.writeUTF(pname); out.writeUTF(flag); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.oid = in.readUTF(); this.pid = in.readUTF(); this.amount = in.readInt(); this.pname = in.readUTF(); this.flag = in.readUTF(); &#125; /** * 返回join之后要输出的表结构 * @return */ @Override public String toString() &#123; return oid + "\t" + pname + "\t" + amount; &#125;&#125; 1.2.2.2. 编写MapReduce123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168package mr.join;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.input.FileSplit;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.ArrayList;import java.util.List;public class JoinDriver &#123; public static final String ORDER_FLAG = "order"; public static final String PRODUCT_FLAG = "product"; public static class JoinMapper extends Mapper&lt;LongWritable, Text, Text, TableBean&gt; &#123; // 记录当前切片所属的文件名 private String filename; // &lt;key,value&gt; private Text k = new Text(); private TableBean tableBean = new TableBean(); /** * 在setup()阶段先获取切片所属的文件名 */ @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 获取文件切片信息 FileSplit inputSplit = (FileSplit) context.getInputSplit(); // 获取切片所属的文件名 filename = inputSplit.getPath().getName(); System.out.println("DEBUG Mapper setup: filename=" + filename); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); // 根据数据来源，在map阶段做不同的处理 if (filename.startsWith("order")) &#123; /* 当前切片数据来自 订单表 | 订单oid | 产品pid | 产品数量amount | | ---- | ---- | ------ | | 1001 | 01 | 1 | */ String[] split = line.split("\t"); tableBean.setOid(split[0]); tableBean.setPid(split[1]); tableBean.setAmount(Integer.valueOf(split[2])); // 注意：Writable通过out.writeUTF(field)等方法进行序列化时，要求field不能为NULL，否则会报空指针异常。所以一些没有数据的字段，要填充一些无意义的值 tableBean.setPname(""); // 标记当前数据来自order表 tableBean.setFlag(ORDER_FLAG); // key是两表相同的列（join的列），即pid k.set(split[1]); &#125; else &#123; /* 当前切片数据来自 产品表 | 产品pid | 产品名称pname | | ---- | ----- | | 01 | 小米 | */ String[] split = line.split("\t"); tableBean.setPid(split[0]); tableBean.setPname(split[1]); // 填充无意义值 tableBean.setOid(""); tableBean.setAmount(0); tableBean.setFlag(PRODUCT_FLAG); // key是两表相同的列（join的列），即pid k.set(split[0]); &#125; // 输出 context.write(k, tableBean); System.out.println(String.format("DEBUG Mapper map: k=%s, v=(%s,%s,%s,%d,%s)", k.toString(), tableBean.getPid(), tableBean.getPname(), tableBean.getOid(), tableBean.getAmount(), tableBean.getFlag())); &#125; &#125; public static class JoinReducer extends Reducer&lt;Text, TableBean, TableBean, NullWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;TableBean&gt; values, Context context) throws IOException, InterruptedException &#123; /* 产品表 join 订单表，是一对多的关系 MapReduce阶段的key取的是公共列pid，所以此时数据是以下情况 key: 产品pid，例如pid=01 values: &#123; TableBean1: pid=01 pname=小米 flag=product TableBean2: pid=01 oid=10001 amount=1 flag=order TableBean3: pid=01 oid=10001 amount=1 flag=order &#125; 现在要根据TableBean的flag字段，将数据分离： 如果flag=product，则用一个TableBean保留 如果flag=order，因为order有多个数据，所以要添加到List&lt;TableBean&gt; */ // 存放产品记录 TableBean product = new TableBean(); // 存放订单集合 List&lt;TableBean&gt; orderList = new ArrayList&lt;&gt;(); // 注意values存在Writable对象复用现象，所以保存数据时，要到tableBean进行clone for (TableBean tableBean : values) &#123; if (ORDER_FLAG.equals(tableBean.getFlag())) &#123; // 订单表数据 orderList.add(tableBean.clone()); &#125; else &#123; // 产品表数据 product = tableBean.clone(); &#125; &#125; for (TableBean tableBean : orderList) &#123; // join合并属性 tableBean.setPname(product.getPname()); // 输出 context.write(tableBean, NullWritable.get()); System.out.println(String.format("DEBUG Reducer reduce: k=(%s,%s,%s,%d,%s)", tableBean.getPid(), tableBean.getPname(), tableBean.getOid(), tableBean.getAmount(), tableBean.getFlag())); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setMapperClass(JoinMapper.class); job.setReducerClass(JoinReducer.class); job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(TableBean.class); job.setOutputKeyClass(TableBean.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 1.2.3. 运行将两张表数据上传到同一目录下1hdfs dfs -put order.txt product.txt /input 运行1hadoop jar join.jar mr.join.JoinDriver /input /output 查看结果 1234567$ hdfs dfs -cat /output/part*1004 小米 41001 小米 11005 华为 51002 华为 21006 格力 61003 格力 3 2. Map Join2.1. Reduce Join的缺点Reduce Join合并操作是在Reduce阶段完成。在Reduce端处理过多的表，非常容易产生数据倾斜。 一般来説，ReduceTask数量要少于MapTask数量，Reduce端计算压力大，计算能尽量在Map阶段完成，就不在Reduce阶段做。 Reduce Join无疑给Reduce带来更大的工作量，所以推荐在Map端实现数据的合并 2.2. 适用场景Map Join适用于一张表十分小（能直接将整张表读入内存），另一张表十分大（内存放不下）的场景 2.3. 编写程序12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394package mr.join;import org.apache.commons.lang.StringUtils;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.BufferedReader;import java.io.FileInputStream;import java.io.IOException;import java.io.InputStreamReader;import java.net.URI;import java.util.HashMap;import java.util.Map;public class JoinDriver &#123; public static class JoinMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; private Map&lt;String, String&gt; productMap = new HashMap&lt;&gt;(); private Text k = new Text(); @Override protected void setup(Context context) throws IOException, InterruptedException &#123; // 读取文件，保存到HashMap中 URI[] cacheFiles = context.getCacheFiles(); String path = cacheFiles[0].getPath().toString(); BufferedReader br = new BufferedReader(new InputStreamReader(new FileInputStream(path), "UTF-8")); String line = null; while ((StringUtils.isNotBlank(line = br.readLine()))) &#123; String[] split = line.split("\t"); productMap.put(split[0], split[1]); &#125; IOUtils.closeStream(br); &#125; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; /* map处理大表： oid | pid | amount */ String line = value.toString(); String[] split = line.split("\t"); String oid = split[0]; String pid = split[1]; String amount = split[2]; // 取出pname String pname = productMap.get(pid); // 拼接得到结果 String result = oid + "\t" + pname + "\t" + amount; k.set(result); context.write(k, NullWritable.get()); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setJarByClass(JoinDriver.class); job.setMapperClass(JoinMapper.class); /// job.setReducerClass(JoinReducer.class); 不用Reduce job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(NullWritable.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // DistributedCache，在job执行前，缓存普通文件到task运行节点的工作目录 job.addCacheFile(new URI("file:///root/product.txt")); // No reduce task job.setNumReduceTasks(0); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 2.4. 运行123mv product.txt /roothdfs dfs -put order.txt /hadoop jar join.jar mr.join.JoinDriver /order.txt /output 查看结果1234567$ hdfs dfs -cat /output/part*1001 小米 11002 华为 21003 格力 31004 小米 41005 华为 51006 格力 6]]></content>
      <tags>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch FunctionScore二次打分]]></title>
    <url>%2F2019%2F03%2F22%2FElasticSearch%2FElasticSearch-FunctionScore%E4%BA%8C%E6%AC%A1%E6%89%93%E5%88%86%2F</url>
    <content type="text"><![CDATA[ES 使用FunctionScore实现自定义评分 通过Function Score Query优化Elasticsearch搜索结果(综合排序) ES5.4中文文档 ES打分公式 控制相关度 (五) - function_score查询及field_value_factor，boost_mode，max_mode参数 - dm_vincent的专栏 - 博客频道 - CSDN.NET field_value_factor 一般只用于数字类型，而衰减函数一般只用于数字、位置和时间类型]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hexo 添加Latex支持]]></title>
    <url>%2F2019%2F03%2F22%2FHexo%2FHexo%20%E6%B7%BB%E5%8A%A0Latex%E6%94%AF%E6%8C%81%2F</url>
    <content type="text"><![CDATA[1. 安装Kramedhexo 默认的渲染引擎是 marked，但是 marked 不支持 mathjax。，所以需要更换Hexo的markdown渲染引擎为hexo-renderer-kramed引擎，后者支持mathjax公式输出 12cnpm uninstall hexo-renderer-marked --savecnpm install hexo-renderer-kramed --save]]></content>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch TF-IDF算法]]></title>
    <url>%2F2019%2F03%2F22%2FElasticSearch%2FElasticSearch-TF-IDF%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1. TF-IDF1.1. 什么是TF-IDF在一份给定的文件里，词频 (term frequency, TF) 指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被归一化（分子一般小于分母 区别于IDF），以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词频，而不管该词语重要与否。） 逆向文件频率 (inverse document frequency, IDF) 是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。 1.2. TF乘以IDF的意义]]></content>
      <tags>
        <tag>ElasticSearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Neo4j Cypher语句]]></title>
    <url>%2F2019%2F03%2F22%2F%E6%95%B0%E6%8D%AE%E5%BA%93%2FNeo4j%2FNeo4j%20Cypher%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[1. 查询操作1.1. 根据Label查询节点123// 查询所有label为Movie的节点MATCH (n: movie)RETURN n 1.2. 根据属性查询节点123// 指定nameMATCH (n: Movie &#123; name: &quot;幻海奇情&quot;, tag: &quot;喜剧&quot; &#125;)RETURN n 1.3. 查询节点的某个属性1MATCH (m : Movie) RETURN m.name 1.4. LIMIT查询12// 取前10条边MATCH path=(actor:Actor)-[:ActedIn]-&gt;(movie:Movie) RETURN path limit 10 1.5. ORDER BY查询1234// 升序MATCH (movie:Movie &#123;name:&quot;流浪地球&quot;&#125;)&lt;-[r:ActedIn]-(actor:Actor)-[:ActedIn]-&gt;(otherMovie:Movie) RETURN otherMovie ORDER BY otherMovie.year LIMIT 10// 降序MATCH (movie:Movie &#123;name:&quot;流浪地球&quot;&#125;)&lt;-[r:ActedIn]-(actor:Actor)-[:ActedIn]-&gt;(otherMovie:Movie) RETURN otherMovie ORDER BY otherMovie.year DESC LIMIT 10 2. 添加 / 修改操作2.1. 添加/修改节点属性1MATCH (t:Teacher) SET t.name=&apos;t1&apos; return t 2.2. 给节点添加Label1MATCH(t:Teacher) set t:Father return t 3. 删除操作3.1. 删除节点的属性1MATCH (t:Teacher) REMOVE t.name 3.2. 删除所有节点1234567// 删除所有节点MATCH (n)DELETE n// 删除所有Teacher节点MATCH (n:Teacher)DELETE n 3.3. 删除所有节点及其关系123456789// 删除所有节点及其关系MATCH (n)OPTIONAL MATCH (n)-[r]-()DELETE n,r// 删除所有Teacher节点及其关系MATCH (n:Teacher)OPTIONAL MATCH (n)-[r]-()DELETE n,r 4. 业务查询4.1. 查询某人演过的电影123456// 查询吴京演过的所有电影MATCH (actor:Actor &#123;name:&quot;吴京&quot;&#125;)-[:ActedIn]-&gt;(movie:Movie) return movie// 查询吴京及其演过的所有电影MATCH (actor:Actor &#123;name:&quot;吴京&quot;&#125;)-[:ActedIn]-&gt;(movie:Movie) return actor, movie// 返回整个路径MATCH path=(actor:Actor &#123;name:&quot;吴京&quot;&#125;)-[:ActedIn]-&gt;(movie:Movie) return path 4.2. 查询参演某电影的人还演了什么电影1234567// 查询参演&quot;流浪地球&quot;的演员，它们还演了哪些电影（不包括&quot;流浪地球&quot;）match (movie:Movie &#123;name:&quot;流浪地球&quot;&#125;)&lt;-[:ActedIn]-(actor:Actor)-[:ActedIn]-&gt;(otherMovie:Movie) return otherMovie// WHERE查询的形式MATCH (movie:Movie)&lt;-[r:ActedIn]-(actor:Actor)-[:ActedIn]-&gt;(otherMovie:Movie)WHERE movie.name = &quot;流浪地球&quot;RETURN otherMovie ORDER BY otherMovie.year LIMIT 10]]></content>
      <tags>
        <tag>Neo4j</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F03%2F19%2FNeo4j%2FNeo4j%20cypher%2F</url>
    <content type="text"><![CDATA[导出CSV导入CSV12LOAD CSV WITH HEADERS FROM &quot;file:///c:/node_test.csv&quot; AS itemMERGE (n:Person &#123;id:toInteger(item.id), name: item.name&#125;) 123LOAD CSV WITH HEADERS FROM &quot;file:///director_relation.csv&quot; AS lineMATCH (from:Director&#123;id:toInteger(line.directorId)&#125;),(to:Movie&#123;id:toInteger(line.movieId)&#125;)MERGE (from)-[r:DIRECTED&#123;id:toInteger(line.directorId),id:toInteger(line.movieId)&#125;]-&gt;(to) 参考1234https://blog.csdn.net/a772316182/article/details/82912961http://www.cnblogs.com/hwaggLee/p/5959716.htmlhttps://www.sogou.com/tupu/person.html 搜狗人物关系https://blog.csdn.net/qq_31748587/article/details/84286411 neo4j图形数据库第三弹——整合springboot（支持查询多节点） 12 neo4j-admin导入student.csv 123id,name1,s12,s2 Bolt1https://blog.csdn.net/zhanaolu4821/article/details/80940598 12tx.run(&quot;CREATE (n:Movie&#123;NAME:&#123;NAME&#125;, TITLE:&#123;TITLE&#125;&#125;)&quot;, Values.parameters(&quot;NAME&quot;, &quot;james&quot;, &quot;TITLE&quot;, &quot;King&quot;)); 123456tx.run(&quot;MATCH (p1:Person &#123; id: &#123;id1&#125; &#125;), (p2:Person &#123;id: &#123;id2&#125; &#125;) &quot; + &quot;CREATE (p1)-[r: &#123;relationName&#125; &#123;type: &#123;relationType&#125; &#125;]-&gt;(p2)&quot;, Values.parameters(&quot;id1&quot;, 1, &quot;id2&quot;, 2, &quot;relationName&quot;, &quot;绯闻&quot;, &quot;relationType&quot;, &quot;2&quot;)); 添加关系测试 1234CREATE (t1:Teacher &#123;id:1, name:&quot;t1&quot;&#125;)CREATE (s1:Student &#123;id:1, name:&quot;s1&quot;&#125;)MATCH (t:Teacher &#123;id: 1&#125;), (s:Student &#123;id: 1&#125;)CREATE (t)-[r:绯闻&#123;type: 0&#125;]-&gt;(s)]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F03%2F10%2F%E7%88%AC%E8%99%AB%2FScrapy-%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3%E7%94%B5%E5%BD%B1%2F</url>
    <content type="text"><![CDATA[参考资料 构建IP池 https://github.com/xiaosimao/IP_POOL/blob/master/custom_get_ip/get_ip_from_peauland.py 创建Scrapy工程1scrapy startproject douban 编辑scrapy.cfg12345[settings]default = douban.settings[deploy]project = douban 创建main.py123456789101112131415from scrapy.cmdline import executeimport sysimport os### 获取当前工程的路径# os.path.abspath(): 获取路径的绝对路径# os.path.dirname(): 获取路径的目录名project_dir = os.path.dirname(os.path.abspath(__file__))# 将工程目录添加到环境变量中sys.path.append(project_dir)# 通过execute执行系统命令，执行爬虫文件execute(["scrapy", "crawl", "movie_subject"]) 配置用户代理池middleware1pip install fake-useragent]]></content>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 同义词]]></title>
    <url>%2F2019%2F03%2F06%2FElasticSearch%2FElasticSearch-%E5%90%8C%E4%B9%89%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[参考： https://www.cnblogs.com/spectrelb/p/8038980.html https://www.jianshu.com/p/3e63f6739631 官方：https://www.elastic.co/guide/en/elasticsearch/reference/6.2/analysis-synonym-tokenfilter.html 1. 自定义同义词 token filter123456789101112131415161718192021222324252627282930313233343536PUT /myindex&#123; "settings": &#123; "analysis": &#123; // 自定义同义词token filter "filter": &#123; "chinese_synonym_filter": &#123; // type设置为"synonym" "type": "synonym", // 指定同义词文件 "synonyms_path": "analysis/chinese_synonym.txt" &#125; &#125;, // 自定义analyzer，在ik_max_word的基础上使用同义词filter "analyzer": &#123; "ik_max_word_synonym": &#123; "type": "custom", "tokenizer": "ik_max_word", "filter": ["chinese_synonym_filter"] &#125; &#125; &#125; &#125;, "mappings": &#123; "mytype": &#123; "properties": &#123; // 测试字段 "title": &#123; "type": "text", "analyzer": "ik_max_word_synonym" &#125; &#125; &#125; &#125;&#125; 测试12345GET myindex/_analyze&#123; "text": ["一个人无论怎么努力"], "analyzer": "ik_max_word_synonym"&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 基础]]></title>
    <url>%2F2019%2F03%2F06%2FElasticSearch%2FElasticSearch-%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[1. settings与概mappings settings是修改分片和副本数的 mappings是修改字段和类型的 2. query与filterfilter，仅仅只是按照搜索条件过滤出需要的数据而已，不计算任何相关度分数，对相关度没有任何影响query，会去计算每个document相对于搜索条件的相关度，并按照相关度进行排序 一般来说，如果你是在进行搜索，需要将最匹配搜索条件的数据先返回，那么用query；如果你只是要根据一些条件筛选出一部分数据，不关注其排序，那么用filter除非是你的这些搜索条件，你希望越符合这些搜索条件的document越排在前面返回，那么这些搜索条件要放在query中；如果你不希望一些搜索条件来影响你的document排序，那么就放在filter中即可 2.1. 性能对比filter，不需要计算相关度分数，不需要按照相关度分数进行排序，同时还有内置的自动cache最常使用filter的数据 query，相反，要计算相关度分数，按照分数进行排序，而且无法cache结果]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 分词]]></title>
    <url>%2F2019%2F03%2F06%2FElasticSearch%2FElasticSearch-%E5%88%86%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[1. 什么是分词器切分词语，normalization（提升recall召回率） 给你一段句子，然后将这段句子拆分成一个一个的单个的单词，同时对每个单词进行normalization（时态转换，单复数转换），分词器recall，召回率：搜索的时候，增加能够搜索到的结果的数量 character filter：在一段文本进行分词之前，先进行预处理，比如说最常见的就是，过滤html标签（hello —&gt; hello），&amp; —&gt; and（I&amp;you —&gt; I and you） tokenizer：分词，hello you and me —&gt; hello, you, and, metoken filter：lowercase，stop word，synonymom，dogs —&gt; dog，liked —&gt; like，Tom —&gt; tom，a/the/an —&gt; 干掉，mother —&gt; mom，small —&gt; little 一个分词器，很重要，将一段文本进行各种处理，最后处理好的结果才会拿去建立倒排索引 2. 分词流程 3. 内置分词器的介绍Set the shape to semi-transparent by calling set_trans(5) standard analyzer：set, the, shape, to, semi, transparent, by, calling, set_trans, 5（默认的是standard） simple analyzer：set, the, shape, to, semi, transparent, by, calling, set, trans whitespace analyzer：Set, the, shape, to, semi-transparent, by, calling, set_trans(5) language analyzer（特定的语言的分词器，比如说，english，英语分词器）：set, shape, semi, transpar, call, set_tran, 5 4. pinyin拼音分词pinyin分词github地址：https://github.com/medcl/elasticsearch-analysis-pinyin 注意：也支持将繁体转为拼音 4.1. 下载安装直接到github下载即可，注意pinyin分词的版本与ElasticSearch版本要对应 解压后，重命名目录，移动到ElasticSearch的plugins目录下12mv elasticsearch analysis-pinyinmv analysis-pinyin /usr/local/elasticsearch/plugins/ 重启ElasticSearch即可生效 4.2. keep_first_letter取每个中文的拼音首字母，串在一起 中华人民共和国 ==&gt; zhrmghg 4.3. keep_full_pinyin将每个中文转为拼音，单独分开 中华人民共和国 ==&gt; [zhong, hua, ren, men, gong, he, guo] 4.4. keep_joined_full_pinyin将每个中文转为拼音，串在一起 中华人民共和国 ==&gt; zhonghuarenmengongheguo 4.5. keep_separate_first_letter取每个中文的拼音首字母，单独分开 中华人民共和国 ==&gt; [z, h, r, m, g, h, g] limit_first_letter_length“limit_first_letter_length” = 3 &amp;&amp; “keep_first_letter” = true 中华人民共和国 ==&gt; zhr 5. 分词API5.1.1234567891011121314151617GET _analyze&#123; "text": ["中华人民共和国"], "analyzer": "ik_max_word"&#125;GET _analyze&#123; "text": ["中华人民共和国"], "analyzer": "pinyin"&#125;GET _analyze&#123; "text": ["中华人民ghg"], "analyzer": "pinyin"&#125; 6. 自定义分词官方参考：https://www.elastic.co/guide/en/elasticsearch/reference/6.2/analysis-custom-analyzer.html 6.1. 基本的自定义分词12345678910111213141516171819202122232425PUT myindex&#123; "settings": &#123; "analysis": &#123; "analyzer": &#123; // 名称随便取 "my_analyzer": &#123; "type": "custom", // 指定char_filter "char_filter": ["html_strip"], // 指定tokenizer "tokenizer": "standard", // 指定token_filter "filter": ["lowercase", "asciifolding"] &#125; &#125; &#125; &#125;&#125;GET /myindex/_analyze&#123; "text": ["&lt;h1&gt;zhejiang university&lt;/h1&gt;"], "analyzer": "my_analyzer"&#125; 6.2. 使用自定义的char_filter/tokenizer/token_filter123456789101112131415161718192021222324252627282930313233343536373839404142PUT myindex&#123; "settings": &#123; "analysis": &#123; "analyzer": &#123; "my_analyzer": &#123; "type": "custom", "char_filter": ["html_strip", "my_char_filter"], "tokenizer": "my_tokenizer", "filter": ["lowercase", "asciifolding", "my_english_stop_filter"] &#125; &#125;, "char_filter": &#123; "my_char_filter": &#123; "type": "mapping", "mappings": [ ":)=&gt; _hadppy_", ":(=&gt; _sad_" ] &#125; &#125;, "tokenizer": &#123; "my_tokenizer": &#123; "type": "pattern", "pattern": "[,.!?]" &#125; &#125;, "filter": &#123; "my_english_stop_filter": &#123; "type": "stop", "stopwords": "_english_" &#125; &#125; &#125; &#125;&#125;GET myindex/_analyze&#123; "text": ["I'm a :) and :( person!"], "analyzer": "my_analyzer"&#125; 7. 中文拼音混合搜索建议参考： 配置ik分词及pinyin分词使搜索同时支持中文和拼音搜索 https://blog.csdn.net/u013905744/article/details/80935846 https://www.jianshu.com/p/781fa2618680 https://cloud.tencent.com/developer/article/1327419 中文拼音混合： tokenizer用ik，token filter用pinyin_filter tokenizer用ik_smart，不必用ik_max_word 7.1. 创建中拼混合分词器12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152PUT test_index&#123; "settings": &#123; "number_of_shards": "1", "index": &#123; "analysis": &#123; "analyzer": &#123; "ik_smart_pinyin_analyzer": &#123; "type": "custom", "tokenizer": "ik_smart", "filter": "pinyin_filter" &#125; &#125;, "filter": &#123; "pinyin_filter": &#123; "type": "pinyin" &#125; &#125; &#125; &#125; &#125;&#125;DELETE test_indexPUT test_index/_mapping/test_type&#123; "properties": &#123; "lawbasis":&#123; "type": "text", "analyzer": "ik_max_word", "fields": &#123; "my_pinyin":&#123; "type":"text", "analyzer": "ik_smart_pinyin_analyzer" &#125; &#125; &#125; &#125;&#125;POST test_index/_analyze&#123; "text":"中华人民共和国", "analyzer": "ik_smart_pinyin_analyzer"&#125;POST test_index/_analyze&#123; "text":"道路挖掘", "analyzer": "ik_smart_pinyin_analyzer"&#125; 7.2. 搜索测试123456789101112131415161718POST test_index/test_type&#123; "lawbasis":"道路挖掘"&#125;POST test_index/test_type&#123; "lawbasis":"道路施工"&#125;GET test_index/test_type/_search&#123; "query":&#123; "match": &#123; "lawbasis.my_pinyin": "sg" &#125; &#125;&#125; 7.3. 对于fields的理解官方说明：https://www.elastic.co/guide/en/elasticsearch/reference/current/multi-fields.html 博文翻译：https://blog.csdn.net/qq_32165041/article/details/83688593]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Vue axios]]></title>
    <url>%2F2019%2F03%2F05%2FVue%2FVue-axios%2F</url>
    <content type="text"><![CDATA[参考资料： Axios中文说明 https://www.kancloud.cn/yunye/axios/234845 Axios介绍Axios 是一个基于 promise 的 HTTP 库，可以用在浏览器和 node.js 中 Features： 从浏览器中创建 XMLHttpRequests 从 node.js 创建 http 请求 支持 Promise API 拦截请求和响应 转换请求数据和响应数据 取消请求 自动转换 JSON 数据 客户端支持防御 XSRF]]></content>
      <categories>
        <category>Vue</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch JavaAPI]]></title>
    <url>%2F2019%2F03%2F04%2FElasticSearch%2FElasticSearch-JavaAPI%2F</url>
    <content type="text"><![CDATA[Java Client Version: 6.2.4 参考资料： Elasticsearch Java High Level REST Client https://segmentfault.com/a/1190000016830796?utm_source=tag-newest 1. 建立关闭ES连接1.1. 集群版123456789101112131415161718192021222324252627public class EsDemo &#123; TransportClient client = null; @Before public void setUp() throws UnknownHostException &#123; // 创建settings，指定ES集群名 Settings settings = Settings.builder().put("cluster.name", "my-application").build(); // 创建client，指定服务器地址，建立连接 client = new PreBuiltTransportClient(settings).addTransportAddresses(new TransportAddress (InetAddress.getByName("192.168.57.102"), 9300)); System.out.println(client); &#125; @Test public void test() &#123; GetResponse response = client.prepareGet("myindex", "person", "1").execute().actionGet(); System.out.println(response.getSourceAsString()); &#125; @After public void tearDown() &#123; // 关闭连接 if (client != null) &#123; client.close(); &#125; &#125;&#125; 1.2. 单机版12345678910111213141516171819202122232425public class EsDemo &#123; TransportClient client = null; @Before public void setUp() throws UnknownHostException &#123; // 创建client，指定服务器地址，建立连接。单机版的集群参数为Settings.EMPTY client = new PreBuiltTransportClient(Settings.EMPTY).addTransportAddresses(new TransportAddress (InetAddress.getByName("192.168.57.102"), 9300)); System.out.println(client); &#125; @Test public void test() &#123; GetResponse response = client.prepareGet("myindex", "person", "1").execute().actionGet(); System.out.println(response.getSourceAsString()); &#125; @After public void tearDown() &#123; // 关闭连接 if (client != null) &#123; client.close(); &#125; &#125;&#125; 2. 查询2.1. prepareGet(index, type, id) 根据id查询文档123456@Testpublic void prepareGet() &#123; // 指定 index/type/id GetResponse response = client.prepareGet("jobbole", "doc", "d8d38d06c5c1b720e6f010af156bb849").execute().actionGet(); System.out.println(response.getSourceAsString());&#125; 2.2. 添加文档12345678910111213@Testpublic void test() throws IOException &#123; // 构建一个文档 XContentBuilder doc = XContentFactory.jsonBuilder() .startObject() .field("title", "article1") .field("create_date", "2018-01-01") .endObject(); // 添加文档，指定 index/type/id IndexResponse response = client.prepareIndex("jobbole", "doc", UUID.randomUUID().toString()) .setSource(doc).get(); System.out.println(response.status());&#125; 2.3. 删除文档123456@Testpublic void testDelete() &#123; // 删除文档，指定 index/type/id DeleteResponse response = client.prepareDelete("jobbole", "doc", "d743fa3d-561c-4434-8e00-5aa08d128185").get(); System.out.println(response.status());&#125; 2.4. 局部更新文档123456789101112131415161718@Testpublic void testUpdate() throws Exception &#123; // 设置要局部更新的字段 XContentBuilder doc = XContentFactory.jsonBuilder().startObject() .field("title", "hello world") .field("content", "123456").endObject(); // 封装update请求 UpdateRequest request = new UpdateRequest(); request.index("jobbole") .type("doc") .id("1cd80e81-e402-492d-bd3f-07c0561fe632") .doc(doc); // 发送update请求 UpdateResponse response = client.update(request).get(); System.out.println(response.status());&#125; 2.5. upsert123456789101112131415161718192021@Testpublic void testUpSert() throws Exception &#123; String id = "465d5d46-433c-469e-a2d9-25c73652e568"; // 封装要添加的文档 IndexRequest indexRequest = new IndexRequest("jobbole", "doc", id) .source(XContentFactory.jsonBuilder().startObject() .field("title", "hello world") .field("content", "123456").endObject()); // 封装更新的文档 // upsert: 如果文档不存在，则执行添加操作，若存在，则局部更新 UpdateRequest updateRequest = new UpdateRequest("jobbole", "doc", id).doc( XContentFactory.jsonBuilder().startObject() .field("title", "aaabbb") .endObject()) .upsert(indexRequest); // 发送update请求 UpdateResponse response = client.update(updateRequest).get(); System.out.println(response.status());&#125; 2.6. mget批量查询1234567891011121314151617@Testpublic void multiGet() &#123; MultiGetResponse response = client.prepareMultiGet() // 指定要查询的各个文档的 index/type/id .add("jobbole", "doc", "d8d38d06c5c1b720e6f010af156bb849") .add("jobbole", "doc", "b29078ff2b754246daf901cba6a20190") // 执行get()进行批量查询 .get(); for (MultiGetItemResponse item : response) &#123; // 取出每次查询的响应结果 GetResponse get = item.getResponse(); // 输出响应结果的source if (get.isExists()) &#123; System.out.println(get.getSourceAsString()); &#125; &#125;&#125; 2.7. bulk批量添加123456789101112131415161718192021222324@Testpublic void bulkInsert() throws IOException &#123; BulkRequestBuilder bulkRequest = client.prepareBulk(); // 添加一次操作 bulkRequest.add(client.prepareIndex("jobbole", "doc", UUID.randomUUID().toString()) .setSource( XContentFactory.jsonBuilder().startObject() .field("title", "artile111") .endObject())); // 添加一次操作 bulkRequest.add(client.prepareIndex("jobbole", "doc", UUID.randomUUID().toString()) .setSource( XContentFactory.jsonBuilder().startObject() .field("title", "artile222") .endObject())); // 执行bulk BulkResponse response = bulkRequest.get(); System.out.println(response.status()); if (response.hasFailures()) &#123; System.out.println("失败"); &#125;&#125; 2.8. bulk批量条件删除1234567891011121314@Testpublic void test() &#123; BulkByScrollResponse response = DeleteByQueryAction.INSTANCE .newRequestBuilder(client) // 查询条件 .filter(QueryBuilders.matchQuery("title", "article")) // 指明index .source("jobbole") // 执行请求 .get(); // 获取删除的文档数 long deletedCount = response.getDeleted(); System.out.println(deletedCount);&#125; 2.9. matchAll查询12345678910111213141516171819202122@Testpublic void test() &#123; QueryBuilder queryBuilder = QueryBuilders.matchAllQuery(); SearchResponse response = // 指定 index client.prepareSearch("jobbole") .setQuery(queryBuilder) // 取前3条 .setSize(3) .get(); SearchHits hits = response.getHits(); for (SearchHit hit : hits) &#123; // 输出source的json形式 System.out.println(hit.getSourceAsString()); // 得到source的Map形式 Map&lt;String, Object&gt; map = hit.getSourceAsMap(); map.forEach((k, v) -&gt; &#123; System.out.println(k + ": " + v); &#125;); &#125;&#125; 2.10. match查询12345678910111213@Testpublic void test() &#123; // 指定match查询的 字段以及值 QueryBuilder queryBuilder = QueryBuilders.matchQuery("title", "linux"); SearchResponse response = client.prepareSearch("jobbole") .setQuery(queryBuilder) .setSize(3) .get(); SearchHits hits = response.getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125;&#125; 2.11. multiMatch查询1234567891011121314@Testpublic void test() &#123; // 根据title和tags字段 查询含有linux关键词 的文档。第1个参数是关键词，后面N个参数是各个field QueryBuilder queryBuilder = QueryBuilders.multiMatchQuery("linux", "title", "tags"); SearchResponse response = client.prepareSearch("jobbole") .setQuery(queryBuilder) .setSize(3) .get(); SearchHits hits = response.getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125;&#125; 2.12. term查询12345678910111213@Testpublic void test() &#123; // title字段含有linux QueryBuilder queryBuilder = QueryBuilders.termQuery("title", "linux"); SearchResponse response = client.prepareSearch("jobbole") .setQuery(queryBuilder) .setSize(3) .get(); SearchHits hits = response.getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125;&#125; 2.13. terms查询12345678910111213@Testpublic void test() &#123; // title字段含有 "linux"或"windows" QueryBuilder queryBuilder = QueryBuilders.termsQuery("title", "linux", "windows"); SearchResponse response = client.prepareSearch("jobbole") .setQuery(queryBuilder) .setSize(3) .get(); SearchHits hits = response.getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125;&#125; 2.14. range查询123456789101112131415161718@Testpublic void test() &#123; QueryBuilder queryBuilder = QueryBuilders // 指定field .rangeQuery("create_date") // 指定范围 .from("1990-01-01").to("2019-01-01") // 指定格式 .format("yyyy-MM-dd"); SearchResponse response = client.prepareSearch("jobbole") .setQuery(queryBuilder) .setSize(3) .get(); SearchHits hits = response.getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125;&#125; 2.15. prefix查询1234567891011121314@Testpublic void test() &#123; QueryBuilder queryBuilder = QueryBuilders .prefixQuery("title", "linux"); SearchResponse response = client.prepareSearch("jobbole") .setQuery(queryBuilder) .setSize(3) .get(); SearchHits hits = response.getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125;&#125; 2.16. wildcard查询1234567891011121314@Testpublic void test() &#123; QueryBuilder queryBuilder = QueryBuilders .wildcardQuery("title", "linux*"); SearchResponse response = client.prepareSearch("jobbole") .setQuery(queryBuilder) .setSize(3) .get(); SearchHits hits = response.getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125;&#125; 2.17. fuzzy查询12345678910111213@Testpublic void test() &#123; QueryBuilder queryBuilder = QueryBuilders .fuzzyQuery("title", "linnx"); SearchResponse response = client.prepareSearch("jobbole") .setQuery(queryBuilder) .get(); SearchHits hits = response.getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125;&#125; 2.18. type查询查询指定index/type下所有的文档 1234567891011121314@Testpublic void test() &#123; QueryBuilder queryBuilder = QueryBuilders // 指定 type .typeQuery("doc"); SearchResponse response = client.prepareSearch("jobbole") .setQuery(queryBuilder) .get(); SearchHits hits = response.getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125;&#125; 2.19. ids查询1234567891011121314151617@Testpublic void test() &#123; QueryBuilder queryBuilder = QueryBuilders .idsQuery() // 指定多个id .addIds("d8d38d06c5c1b720e6f010af156bb849", "b29078ff2b754246daf901cba6a20190", "8033832ada5440b9015d447b6c1f32b0"); SearchResponse response = client.prepareSearch("jobbole") .setQuery(queryBuilder) .get(); SearchHits hits = response.getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125;&#125; 2.20. common查询1234567891011@Testpublic void test() &#123; QueryBuilder queryBuilder = QueryBuilders.commonTermsQuery("title", "linux"); SearchResponse response = client.prepareSearch("jobbole") .setQuery(queryBuilder) .get(); SearchHits hits = response.getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125;&#125; 2.21. queryStringQuery123456789101112@Testpublic void test() &#123; // 对所有字段查询 包含"linux"但不包含"使用" QueryBuilder queryBuilder = QueryBuilders.queryStringQuery("+linux -使用"); SearchResponse response = client.prepareSearch("jobbole") .setQuery(queryBuilder) .get(); SearchHits hits = response.getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125;&#125; 2.22. simpleQueryStringQuery123456789101112@Testpublic void test() &#123; // 对所有字段查询 包含"linux"但不包含"使用"。不完全满足条件也能查到 QueryBuilder queryBuilder = QueryBuilders.simpleQueryStringQuery("+linux -使用"); SearchResponse response = client.prepareSearch("jobbole") .setQuery(queryBuilder) .get(); SearchHits hits = response.getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125;&#125; 3. 聚合查询3.1. max查询1234567891011121314@Testpublic void test() &#123; AggregationBuilder aggregationBuilder = AggregationBuilders // 聚合结果字段命名 .max("agg") // 指定field .field("like_num"); SearchResponse response = client .prepareSearch("jobbole") .addAggregation(aggregationBuilder) .get(); Max agg = response.getAggregations().get("agg"); System.out.println(agg.getValue());&#125; 3.2. min查询1234567891011121314@Testpublic void test() &#123; AggregationBuilder aggregationBuilder = AggregationBuilders // 聚合结果字段命名 .min("agg") // 指定field .field("like_num"); SearchResponse response = client .prepareSearch("jobbole") .addAggregation(aggregationBuilder) .get(); Min agg = response.getAggregations().get("agg"); System.out.println(agg.getValue());&#125; 3.3. avg查询1234567891011121314@Testpublic void test() &#123; AggregationBuilder aggregationBuilder = AggregationBuilders // 聚合结果字段命名 .avg("agg") // 指定field .field("like_num"); SearchResponse response = client .prepareSearch("jobbole") .addAggregation(aggregationBuilder) .get(); Avg agg = response.getAggregations().get("agg"); System.out.println(agg.getValue());&#125; 3.4. sum查询1234567891011121314@Testpublic void test() &#123; AggregationBuilder aggregationBuilder = AggregationBuilders // 聚合结果字段命名 .sum("agg") // 指定field .field("like_num"); SearchResponse response = client .prepareSearch("jobbole") .addAggregation(aggregationBuilder) .get(); Sum agg = response.getAggregations().get("agg"); System.out.println(agg.getValue());&#125; 3.5. cardinality查询1234567891011121314@Testpublic void test() &#123; AggregationBuilder aggregationBuilder = AggregationBuilders // 聚合结果字段命名 .cardinality("agg") // 指定field .field("like_num"); SearchResponse response = client .prepareSearch("jobbole") .addAggregation(aggregationBuilder) .get(); Cardinality agg = response.getAggregations().get("agg"); System.out.println(agg.getValue());&#125; 4. 组合查询1234567891011121314151617@Testpublic void test() &#123; QueryBuilder queryBuilder = QueryBuilders.boolQuery() .must(QueryBuilders.matchQuery("title", "linux")) .mustNot(QueryBuilders.matchQuery("title", "使用")) .should(QueryBuilders.matchQuery("title", "python")) .filter(QueryBuilders.rangeQuery("create_date") .from("1990-01-01").to("2019-01-01") .format("yyyy-MM-dd")); SearchResponse response = client.prepareSearch("jobbole") .setQuery(queryBuilder) .get(); SearchHits hits = response.getHits(); for (SearchHit hit : hits) &#123; System.out.println(hit.getSourceAsString()); &#125;&#125; 5. 查询建议5.1. completion查询建议12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Testpublic void test() &#123; CompletionSuggestionBuilder suggestionBuilder = // 查询completion字段 new CompletionSuggestionBuilder("suggest") // 设置要查询的文本，以及fuzziness .prefix("linux", Fuzziness.TWO); SearchResponse response = client.prepareSearch("jobbole") // 指定type .setTypes("doc") // 设置suggest，并给建议结果命名 .suggest(new SuggestBuilder().addSuggestion("mysuggest", suggestionBuilder)) // 取前10条 .setFrom(0).setSize(10) // 指定只返回"title"字段，第1个参数是include（要返回的），第2个参数是exclude（不允许返回的） .setFetchSource("title", null) .get(); Suggest suggest = response.getSuggest(); // 获取suggestion CompletionSuggestion suggestion = suggest.getSuggestion("mysuggest"); List&lt;CompletionSuggestion.Entry&gt; entries = suggestion.getEntries(); for (CompletionSuggestion.Entry entry : entries) &#123; List&lt;CompletionSuggestion.Entry.Option&gt; options = entry.getOptions(); for (CompletionSuggestion.Entry.Option option : options) &#123; System.out.println("--------------------------------------"); // 得分 float score = option.getScore(); System.out.println("score: " + score); // 命中的文档 SearchHit hit = option.getHit(); System.out.println(hit); System.out.println("文档id: " + hit.getId()); System.out.println("文档type: " + hit.getType()); System.out.println("文档score: " + hit.getScore()); System.out.println("文档source: " + hit.getSourceAsMap().get("title")); // suggest查询时的文本，即用户传入的文本 System.out.println("text: " + option.getText()); // 包括得分的基本信息 ScoreDoc doc = option.getDoc(); System.out.println(doc); &#125; &#125;&#125;]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 整合ElasticSearch]]></title>
    <url>%2F2019%2F03%2F03%2FSpringBoot%2FSpringBoot-%E6%95%B4%E5%90%88ElasticSearch%2F</url>
    <content type="text"><![CDATA[Spring Data Elasticsearch官方文档：https://docs.spring.io/spring-data/elasticsearch/docs/current/reference/html/ Spring Data ElasticSearch Github：https://github.com/spring-projects/spring-data-elasticsearch SpringBoot ElasticSearch文档：https://docs.spring.io/spring-boot/docs/2.1.3.RELEASE/reference/html/boot-features-nosql.html#boot-features-connecting-to-elasticsearch-rest 较好的博文： https://blog.csdn.net/xxs5258/article/details/81392810 https://blog.csdn.net/chennanymy/article/details/52663589 https://blog.csdn.net/LONG729564606/article/details/84348193 SpringBoot ElasticSearch相关操作 1. 环境搭建1.1. 添加依赖1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-elasticsearch&lt;/artifactId&gt;&lt;/dependency&gt; 通过IDE查看spring-boot-starter-data-elasticsearch依赖的Spring Data ElasticSearch版本123&lt;groupId&gt;org.springframework.data&lt;/groupId&gt;&lt;artifactId&gt;spring-data-elasticsearch&lt;/artifactId&gt;&lt;version&gt;3.1.5.RELEASE&lt;/version&gt; 进入 Spring Data ElasticSearch Github，查看Spring Data ElasticSearch版本与ElasticSearch版本的对应关系 spring data elasticsearch elasticsearch 3.2.x 6.5.0 3.1.x 6.2.2 3.0.x 5.5.0 2.1.x 2.4.0 2.0.x 2.2.0 1.3.x 1.5.2 如果与ElasticSearch版本号对不上，要重新定义版本号 1.2. 文档实体123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package cn.pancx.searchfinal.entity;import lombok.Data;import org.springframework.data.annotation.Id;import org.springframework.data.elasticsearch.annotations.CompletionField;import org.springframework.data.elasticsearch.annotations.Document;import org.springframework.data.elasticsearch.annotations.Field;import org.springframework.data.elasticsearch.annotations.FieldType;import java.util.Date;@Data@Document(indexName = "jobbole", type = "doc")public class Article &#123; @Id @Field(type = FieldType.Keyword) private String urlObjectId; @Field(type = FieldType.Date) private Date createDate; @Field(type = FieldType.Keyword) private String url; @Field(type = FieldType.Keyword) private String frontImageUrl; @Field(type = FieldType.Keyword) private String frontImagePath; @Field(type = FieldType.Integer) private Integer likeNum; @Field(type = FieldType.Integer) private Integer favorNum; @Field(type = FieldType.Integer) private Integer commentNum; @Field(type = FieldType.Text, analyzer = "ik_max_word") private String title; @Field(type = FieldType.Text, analyzer = "ik_max_word") private String tags; @Field(type = FieldType.Text, analyzer = "ik_max_word") private String content; @CompletionField(analyzer = "ik_max_word") private String suggest;&#125; 1.3. DAO1234567891011package cn.pancx.searchfinal.repository;import cn.pancx.searchfinal.entity.Article;import org.springframework.data.elasticsearch.repository.ElasticsearchRepository;import org.springframework.stereotype.Component;// 添加@Component或者@Repository注解@Componentpublic interface ArticleRepository extends ElasticsearchRepository&lt;Article, String&gt; &#123;&#125; 1.4. 测试12345678910111213141516171819202122232425262728293031323334package cn.pancx.searchfinal.repository;import cn.pancx.searchfinal.entity.Article;import org.elasticsearch.index.query.QueryBuilder;import org.elasticsearch.index.query.QueryBuilders;import org.junit.Test;import org.junit.runner.RunWith;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;import org.springframework.test.context.junit4.SpringRunner;@RunWith(SpringRunner.class)@SpringBootTestpublic class ArticleRepositoryTest &#123; @Autowired private ArticleRepository articleRepository; @Test public void save() &#123; Article article = new Article(); article.setUrlObjectId(UUID.randomUUID().toString()); article.setUrl("http://example.com"); articleRepository.save(article); &#125; @Test public void searchByQueryBuilder() &#123; QueryBuilder queryBuilder = QueryBuilders.matchQuery("title", "linux"); Iterable&lt;Article&gt; list = articleRepository.search(queryBuilder); list.forEach(article -&gt; &#123; System.out.println(article); &#125;); &#125;&#125; 2. SpringBoot ElasticSearch JPA API2.1. save 添加单个文档1234567@Testpublic void save() &#123; Article article = new Article(); article.setUrlObjectId(UUID.randomUUID().toString()); article.setUrl("http://example.com"); articleRepository.save(article);&#125; 2.2. saveAll 批量添加文档12345678910@Testpublic void saveAll() &#123; List&lt;Article&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 10; ++i) &#123; Article article = new Article(); article.setTitle("article" + i); list.add(article); &#125; articleRepository.saveAll(list);&#125; 2.3. findById12345678@Testpublic void findById() &#123; // 根据id查询文档 Optional&lt;Article&gt; optional = articleRepository.findById("d8d38d06c5c1b720e6f010af156bb849"); optional.ifPresent(article -&gt; &#123; System.out.println("Article: " + article); &#125;);&#125; 2.4. count123456@Testpublic void count() &#123; // 统计总文档数 long count = articleRepository.count(); System.out.println(count);&#125; 2.5. existsById1234567@Testpublic void existsById() &#123; boolean exists = articleRepository.existsById("14aadb73b3b5efd912e34780f8bf96cc"); System.out.println(exists); exists = articleRepository.existsById("zM07RmkBhZsqrmlIAXvq"); System.out.println(exists);&#125; 2.6. findAll123456@Testpublic void findAll() &#123; // 查询所有文档 Iterable&lt;Article&gt; list = articleRepository.findAll(); list.forEach(System.out::println);&#125; 2.7. findAll(pageable)12345678910111213141516171819@Testpublic void findAll() &#123; // 页码 int page = 2; // 页大小 int size = 5; // 构造分页对象 Pageable pageable = PageRequest.of(page - 1, size); // 分页查询 Page&lt;Article&gt; list = articleRepository.findAll(pageable); // 总页数 int totalPages = list.getTotalPages(); System.out.println("totalPages: " + totalPages); // 总记录数（未分页的总记录数） long totalElements = list.getTotalElements(); System.out.println("totalElements: " + totalElements); // 输出分页查询结果 list.forEach(System.out::println);&#125; 2.8. delete1234567@Testpublic void delete() &#123; Optional&lt;Article&gt; optional = articleRepository.findById("zc07RmkBhZsqrmlIAXvq"); optional.ifPresent(article -&gt; &#123; articleRepository.delete(article); &#125;);&#125; 2.9. deleteById1234@Testpublic void deleteById() &#123; articleRepository.deleteById("zM07RmkBhZsqrmlIAXvq");&#125; 2.10. deleteAll 批量删除12345@Testpublic void deleteAll() &#123; // 删除所有文档 articleRepository.deleteAll();&#125; 2.11. findByXxx12345@Testpublic void findByXxx() &#123; Article article = articleRepository.findByTitle("article1"); System.out.println(article);&#125; 3. Spring Data ElasticSearchTempalte API3.1. 创建索引和映射123456// 未经过检验@Testpublic void test() &#123; elasticsearchTemplate.createIndex(Article.class); elasticsearchTemplate.putMapping(Article.class);&#125; 3.2. match/sourceFilter12345678910111213@Testpublic void test() &#123; SearchQuery searchQuery = new NativeSearchQueryBuilder() .withQuery(QueryBuilders.matchQuery("title", "linux")) .withSourceFilter(new FetchSourceFilter(new String[]&#123;"title", "url", "createDate", "likeNum", "favorNum", "commentNum", "tags", "frontImageUrl"&#125;, null)) .build(); List&lt;Article&gt; articles = elasticsearchTemplate.queryForList(searchQuery, Article.class); articles.forEach(article -&gt; &#123; System.out.println("---------------------"); System.out.println(article); &#125;);&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot CORS]]></title>
    <url>%2F2019%2F03%2F03%2FSpringBoot%2FSpringBoot-CORS%2F</url>
    <content type="text"><![CDATA[1. CORS相关知识1.1. origin请求头origin: 1.2. 实现CORS的方法主要有以下3种方式： 注解驱动：@CrossOrigin，只能应用于Controller或者mapping方法 代码驱动：WebMvcConfigurer#addCorsMappings，可实现全局CORS 拦截器：CorsFilter，可实现全局CORS 2. Spring @CrossOrigin注解实现跨域2.1. @CrossOrigin源码解析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@Target(&#123; ElementType.METHOD, ElementType.TYPE &#125;)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface CrossOrigin &#123; // 默认允许来自所有域名的请求 String[] DEFAULT_ORIGINS = &#123; "*" &#125;; // 默认允许所有请求头 String[] DEFAULT_ALLOWED_HEADERS = &#123; "*" &#125;; boolean DEFAULT_ALLOW_CREDENTIALS = true; long DEFAULT_MAX_AGE = 1800; /** * value和origins等价 */ @AliasFor("origins") String[] value() default &#123;&#125;; /** * 所有支持域的集合，例如"http://domain1.com"。 * &lt;p&gt;这些值都显示在请求头中的Access-Control-Allow-Origin * "*"代表所有域的请求都支持 * &lt;p&gt;如果没有定义，所有请求的域都支持 * @see #value */ @AliasFor("value") String[] origins() default &#123;&#125;; /** * 允许请求头重的header，默认都支持 */ String[] allowedHeaders() default &#123;&#125;; /** * 响应头中允许访问的header，默认为空 */ String[] exposedHeaders() default &#123;&#125;; /** * 请求支持的方法，例如"&#123;RequestMethod.GET, RequestMethod.POST&#125;"&#125;。 * 默认支持RequestMapping中设置的方法 */ RequestMethod[] methods() default &#123;&#125;; /** * 是否允许cookie随请求发送，使用时必须指定具体的域 */ String allowCredentials() default ""; /** * 预请求的结果的有效期，默认30分钟（1800秒） */ long maxAge() default -1;&#125; 2.2. @CrossOrigin使用示例2.2.1. @CrossOrigin用于mapping方法@CrossOrigin用于mapping方法时，即仅针对该方法提供跨源支持 123456789101112131415161718192021222324252627282930313233343536373839404142@RestControllerpublic class HelloController &#123; // 默认不支持跨域 @GetMapping("/hello1") public String hello1() &#123; return "hello1"; &#125; // 默认允许所有origin // 默认允许当前映射对应的method，如使用了@GetMapping，就默认允许get请求 @CrossOrigin @GetMapping("/hello2") public String hello2() &#123; return "hello2"; &#125; /** * value与origins等价，可设置允许的origin，'*'是默认值，允许所有origin * @CrossOrigin("*") * @CrossOrigin(value = "*") * @CrossOrigin(origins = "*") */ @CrossOrigin("*") @GetMapping("/hello3") public String hello3() &#123; return "hello3"; &#125; // 设置允许的methods，因为默认与当前mapping的method一致，所以一般不用手动设置 @CrossOrigin(methods = &#123;RequestMethod.GET, RequestMethod.PUT&#125;) @GetMapping("/hello4") public String hello4() &#123; return "hello4"; &#125; // 设置预检请求缓存有效期，单位（秒）。默认1800秒 @CrossOrigin(maxAge = 3600) @GetMapping("/hello5") public String hello5() &#123; return "hello5"; &#125;&#125; 2.2.2. @CrossOrigin用于整个Controller123456789101112// @CrossOrigin应用于整个Controller的所有mapping方法@CrossOrigin// allowCredentials可以为true/false@CrossOrigin(allowCredentials = "true")@RestControllerpublic class HelloController &#123; @GetMapping("/hello1") public String hello1() &#123; return "hello1"; &#125;&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 查询]]></title>
    <url>%2F2019%2F03%2F03%2FElasticSearch%2FElasticSearch-%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[1. 查询分类 基本查询：使用elasticsearch内置查询条件进行查询 组合查询：把多个查询组合在一起进行复合查询。 过滤：查询同时，通过filter条件在不影响打分的情况下筛选数据。 2. 样例测试2.1. 建立索引1234567891011121314151617181920212223242526272829303132333435PUT lagou&#123; "mappings": &#123; "job": &#123; "properties": &#123; "title": &#123; "store": true, "type": "text", "analyzer": "ik_max_word" &#125;, "salary_min": &#123; "type": "integer" &#125;, "city": &#123; "store": true, "type": "keyword" &#125;, "company": &#123; "properties": &#123; "name": &#123; "type": "text" &#125;, "address": &#123; "type": "text" &#125; &#125; &#125;, "pub-date": &#123; "type": "date", "format": "yyyy-MM-dd" &#125; &#125; &#125; &#125;&#125; 2.2. 添加数据1234567891011121314151617181920212223242526272829303132333435363738394041424344PUT lagou/job/1&#123; "title":"Java后端研发", "salary_min":20000, "city":"北京", "Company":&#123; "name":"百度", "address":"北京" &#125;, "pub_date":"2018-10-28"&#125;PUT lagou/job/2&#123; "title":"Python分布式爬虫", "salary_min":20000, "city":"成都", "Company":&#123; "name":"美团", "address":"成都" &#125;, "pub_date":"2018-10-27"&#125;PUT lagou/job/3&#123; "title":"前端研发", "salary_min":20000, "city":"北京", "Company":&#123; "name":"阿里", "address":"北京" &#125;, "pub_date":"2018-10-28"&#125;PUT lagou/job/4&#123; "title": "Python后端", "salary_min": 20000, "city": "成都", "Company": &#123; "name": "美团", "address": "成都" &#125;, "pub_date": "2018-10-27"&#125; 3. 查询3.1. match查询12345678GET lagou/_search&#123; "query": &#123; "match": &#123; "title": "python" &#125; &#125;&#125; 3.2. match_phrase查询1234567891011GET lagou/_search&#123; "query": &#123; "match_phrase": &#123; "title": &#123; "query": "python爬虫", "slop": 3 &#125; &#125; &#125;&#125; 3.3. fuzzy查询12345678910111213GET /jobbole/doc/_search&#123; "query": &#123; "fuzzy": &#123; "title": &#123; "value": "linux", "fuzziness": 2, "prefix_length": 0 &#125; &#125; &#125;, "_source": "title"&#125; 4. 聚合查询4.1. sum查询12345678910111213141516GET /jobbole/doc/_search&#123; // size=0限定不返回任何文档。如果不设置，默认会返回所有的文档 "size": 0, // 聚合查询固定写"aggs" "aggs": &#123; // 聚合结果字段的名称可以随便取 "sum_like_num": &#123; // 聚合类型为sum "sum": &#123; // 要聚合的字段 "field": "like_num" &#125; &#125; &#125;&#125; 4.2. max查询1234567891011GET /jobbole/doc/_search&#123; "size": 0, "aggs": &#123; "max_like_num": &#123; "max": &#123; "field": "like_num" &#125; &#125; &#125;&#125; 4.3. min查询1234567891011GET /jobbole/doc/_search&#123; "size": 0, "aggs": &#123; "min_like_num": &#123; "min": &#123; "field": "like_num" &#125; &#125; &#125;&#125; 4.4. avg查询1234567891011GET /jobbole/doc/_search&#123; "size": 0, "aggs": &#123; "avg_like_num": &#123; "avg": &#123; "field": "like_num" &#125; &#125; &#125;&#125; 4.5. cardinality查询cardinality基数相当于SQL的distinct count，注意有个distinct，不是求文档总数。例如性别的cardinality为2，因为只有’男’和’女’ 1234567891011GET /jobbole/doc/_search&#123; "size": 0, "aggs": &#123; "card_like_num": &#123; "cardinality": &#123; "field": "like_num" &#125; &#125; &#125;&#125; 4.6. terms分组数量查询123456789101112GET /jobbole/doc/_search&#123; "size": 0, "aggs": &#123; "count_group_by_like_num": &#123; // terms指定根据哪个字段进行分组统计 "terms": &#123; "field": "like_num" &#125; &#125; &#125;&#125; 4.7. terms分组 + 聚合子查询1234567891011121314151617181920212223242526GET /jobbole/doc/_search&#123; "size": 0, "aggs": &#123; "group_like_num": &#123; "terms": &#123; // 根据like_num分缚 "field": "like_num", // 顺便根据每一组的avg_like_num排序 "order": &#123; "avg_like_num": "desc" &#125; &#125;, // 对每一组做聚合查询 "aggs": &#123; // 查询结果命名为avg_like_num，名称随意取 "avg_like_num": &#123; // 求like_num字段的平均值 "avg": &#123; "field": "like_num" &#125; &#125; &#125; &#125; &#125;&#125; 5. 查询建议5.1. completion123456789101112131415GET /jobbole/_search&#123; "suggest": &#123; "mysuggest": &#123; "prefix": "lnx", "completion": &#123; "field": "suggest", "fuzzy": &#123; "fuzziness": 2 &#125; &#125; &#125; &#125; , "_source": "title"&#125; 6. 高亮查询12345678910111213141516171819202122232425GET /jobbole/_search&#123; "from": 0, "size": 1, "query": &#123; "multi_match": &#123; "query": "linux", "fields": [ "content", "title" ] &#125; &#125;, "_source": "title", "highlight": &#123; "fields": &#123; "content": &#123; "fragment_size": 50 , "number_of_fragments": 200 &#125; &#125;, "pre_tags": "&lt;em&gt;", "post_tags": "&lt;/em&gt;" &#125;&#125;]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 数据类型]]></title>
    <url>%2F2019%2F03%2F03%2FElasticSearch%2FElasticSearch-%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[参考：https://www.cnblogs.com/lyq-biu/p/9866769.html 1. 数据类型可以接收的参数1.1. 简单概括 属性 描述 适合类型 store 值为yes表示存储，为no表示不存储，默认为no all index yes表示分析，no表示不分析，默认为true string null_value 如果字段为空，可以设置一个默认值，比如”人生” all analyzer 可以设置索引和搜索时用的分析器，默认使用的是standard分析器，还可以使用whitespace，simple，english all include_in_all 默认es为每个文档定义一个特殊域_all，它的作用是让每个字段被搜索到，如果不想让某个字段被搜索到，可以设置为false all format 时间格式字符串的模式 date 1.2. 字符串可以接收的参数 1.3. 数字型可以接收的参数 1.4. 日期型可以接收的参数 1.5. 布尔型可以接收的参数]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch 环境搭建]]></title>
    <url>%2F2019%2F03%2F02%2FElasticSearch%2FElasticSearch-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1. Windows ElasticSearch环境搭建1.1. 下载安装https://www.elastic.co/downloads/elasticsearch 官网下载zip或者tgz 1.2. 基本环境配置在安装目录下分别创建用于存放数据和日志的目录。es目录已经有logs目录了，所以再创建一个data目录，用于存放数据12mkdir datamkdir logs 编辑 config/elasticsearch.yml123456789101112# 修改数据存放目录path.datapath.data: C:\software\development environment\elasticsearch-6.2.4\data# 修改日志存放目录path.logspath.logs: C:\software\development environment\elasticsearch-6.2.4\logs# --------- 设置CORS，便于第三方插件客户端访问# 添加 开启CORS跨域访问。因为有的客户端是浏览器以AJAX的形式访问的，为了开发访问，开启跨域http.cors.enabled: true# 添加 允许所有IP跨域访问 http.cors.allow-origin: "*"# 添加 允许的请求方式http.cors.allow-methods: OPTIONS, HEAD, GET, POST, PUT, DELETE 1.3. 运行ES双击运行bin/elasticsearch.bat即可，会在前台运行。 也可以加上-d参数，在后台运行1elasticsearch.bat -d 浏览器访问9200端口 查看socket，可以看到在监听9200和93001netstat -ano jps查看进程123&gt; jps29072 Jps26452 Elasticsearch 1.4. 关闭ES关闭ES若是es的前台运行，则用ctrl + c来停止。若是es的后台运行，则用jps结合kill -9 pid来停止。 2. Linux ElasticSearch环境搭建2.1. 安装及配置12tar -zxvf elasticsearch-6.2.4.tar.gz -C /usr/localmv /usr/local/elasticsearch-6.2.4/ /usr/local/elasticsearch es目录已经有logs目录了，所以再创建一个data目录，用于存放数据1mkdir /usr/local/elasticsearch/data 编辑 /usr/local/elasticsearch/config/elasticsearch.yml12345678910111213141516# 修改数据存放目录path.datapath.data: /usr/local/elasticsearch/data# 修改日志存放目录path.logspath.logs: /usr/local/elasticsearch/logs# 修改监听的IP为0.0.0.0，以便其它主机的客户端访问network.host: 0.0.0.0# 修改HTTP端口，默认是9200，可以不设置http.port: 9200# --------- 设置CORS，便于第三方插件客户端访问# 添加 开启CORS跨域访问。因为有的客户端是浏览器以AJAX的形式访问的，为了开发访问，开启跨域http.cors.enabled: true# 添加 允许所有IP跨域访问 http.cors.allow-origin: "*"# 添加 允许的请求方式http.cors.allow-methods: OPTIONS, HEAD, GET, POST, PUT, DELETE 2.2. 运行ES1bin/elasticsearch 2.2.1. 错误1：禁止root运行直接以root用户运行，出现错误：1org.elasticsearch.bootstrap.StartupException: java.lang.RuntimeException: can not run elasticsearch as root 出于安全考虑，ES默认不允许以root身份运行。将目录的所有人修改为其它普通用户，再以该用户运行。123456# 添加用户，并修改目录所有者及所有组useradd es -p 123456chown es:es /usr/local/elasticsearch/ -R# 再以es用户运行su - es/usr/local/elasticsearch/bin/elasticsearch 想要root直接运行，可以在运行时加上参数1bin/elasticsearch -Des.insecure.allow.root=true 如果不想输入参数，可以直接编辑bin/elasticsearch，其实就是一个shell脚本文件。添加以下一行1ES_JAVA_OPTS="-Des.insecure.allow.root=true $ES_JAVA_OPTS" 2.3. 错误2：虚拟内存权限不足切换用户，运行es1234su - es/usr/local/elasticsearch/bin/elasticsearch# 也可以加上-d参数，在后台运行。运行过程较长，需要耐心等待。/usr/local/elasticsearch/bin/elasticsearch -d 提示错误1[1]: max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144] 提示用户可申请的虚拟内存大小权限不够，root身份执行以下命令来提高虚拟内存权限。1sysctl -w vm.max_map_count=262144 以上的 sysctl -w 只能临时解决问题。编辑 /etc/sysctl.conf，添加以下内容1vm.max_map_count=655360 编辑完/etc/sysctl.conf之后，再执行以下命令，使配置立即生效1sysctl -p]]></content>
      <categories>
        <category>ElasticSearch</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[SpringBoot 整合redis]]></title>
    <url>%2F2019%2F03%2F02%2FSpringBoot%2FSpringBoot-%E6%95%B4%E5%90%88redis%2F</url>
    <content type="text"><![CDATA[1. 参考资料官方整合redis参考文档：https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/#boot-features-redis 2. SpringBoot Redis相关配置123456789101112131415161718# Redis数据库索引（默认为0）spring.redis.database=0 # Redis服务器地址。默认值localhostspring.redis.host=localhost# Redis服务器连接端口。默认值6379spring.redis.port=6379 # Redis服务器连接密码（默认为空）spring.redis.password= # 连接池最大连接数（使用负值表示没有限制）。默认值8spring.redis.pool.max-active=8 # 连接池最大阻塞等待时间（使用负值表示没有限制）。默认值-1spring.redis.pool.max-wait=-1# 连接池中的最大空闲连接。默认值8spring.redis.pool.max-idle=8 # 连接池中的最小空闲连接。默认值0spring.redis.pool.min-idle=0 # 连接超时时间（毫秒）。默认值-1，表示永不超时spring.redis.timeout=0 3. SpringBoot 整合redis3.1. 添加依赖12345678&lt;!-- spring-boot-starter-data-redis 依赖 spring-data-redis spring-data-redis 依赖 jedis--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt; 3.2. 添加redis配置12spring.redis.host=192.168.57.111spring.redis.port=6379 3.3. 测试12345678910111213141516@RunWith(SpringRunner.class)@SpringBootTestpublic class RedisTest&#123; @Autowired private StringRedisTemplate redisTemplate; @Test public void testValueSet() &#123; redisTemplate.opsForValue().set("k1", "v1"); &#125; @Test public void testValueGet() &#123; redisTemplate.opsForValue().get("k1"); &#125;&#125;]]></content>
      <categories>
        <category>SpringBoot</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Scrapy 爬取知乎]]></title>
    <url>%2F2019%2F03%2F01%2F%E7%88%AC%E8%99%AB%2FScrapy-%E7%88%AC%E5%8F%96%E7%9F%A5%E4%B9%8E%2F</url>
    <content type="text"><![CDATA[1. 登录知乎1.1.创建 utils/zhihu_login_requests.py]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Scrapy 入门]]></title>
    <url>%2F2019%2F03%2F01%2F%E7%88%AC%E8%99%AB%2FScrapy-%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[1. Scrapy介绍1.1. 什么是ScrapyScrapy是通用型的爬虫框架。所谓”通用”，就是用Scrapy做任何爬虫项目，都是适合的 1.2. Scrapy与urllib对比 urlib只是一个库，Scrapy是一个框架 urlib适合写单个爬虫文件，Scrapy适合写中大型爬虫项目 urllib写简单爬虫文件，比较方便，但是不稳定，要亲自处理很多异常 1.3. Scrapy架构 Engine引擎从爬虫文件Spider读取Request，交给Scheduler调度 Scheduler让Downloader向目标URL发起请求 发起请求后，通过Request预先定义好的callback，将得到的Response作为callback的参数，交给Spider去处理 Spider根据响应结果，解析HTML，将爬到的数据封装成item item交给一个个pipeline去完成后续处理，如存入数据库 2. 安装Scrapy2.2. Windows 安装安装Scrapy会遇到不少错误，建议按照以下步骤安装。 先升级pip1python -m pip install --upgrade pip Scrapy依赖Twisted，但是pip无法直接安装Twisted，需要以wheel的方式安装，所以先安装wheel1pip install wheel 安装lxml。Scrapy依赖lxml解析HTML和XML。如果发现用pip无法安装lxml，则像后面安装Twisted一样，进入python lfd网站，下载对应的二进制wheel包进行安装1pip install lxml 安装Twisted，如果直接通过pip install Twisted安装，会发现出错：12building &apos;twisted.test.raiser&apos; extensionerror: Microsoft Visual C++ 14.0 is required. Get it with &quot;Microsoft Visual C++ Build Tools&quot;: 解决方法是通过wheel安装。进入https://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted 下载对应的Twisted版本。我的环境是python37的64位版本，所以下载 Twisted-18.9.0-cp37-cp37m-win_amd64.whl。其中cp37对应python37，win_amd64对应64位版本。注意32/64位不是看当前系统的版本，而是以python的版本为准，版本信息可以通过python命令查看。123$ python # [MSC v.1914 64 bit (AMD64)] 说明是64位Python 3.7.0 (v3.7.0:1bf9cc5093, Jun 27 2018, 04:59:51) [MSC v.1914 64 bit (AMD64)] on win32 下载完之后，通过wheel安装Twisted：1pip install Twisted-18.9.0-cp37-cp37m-win_amd64.whl 最后用pip安装scrapy1pip install scrapy 查看是否安装成功1scrapy version 2.3. Linux 安装升级pip1python -m pip install --upgrade pip 安装wheel和lxml12pip install wheelpip install lxml 安装Twisted，若不成功，则用wheel安装1pip install Twisted 安装scrapy1pip install scrapy 查看scrapy是否安装成功，报以下错误12$ scrapy verionAttributeError: module &apos;lib&apos; has no attribute &apos;Cryptography_HAS_SSL_ST&apos; 3. Scrapy环境搭建3.1. 创建Scrapy工程scrapy startproject &lt;工程名&gt; 创建Scrapy工程1scrapy startproject myspider 在当前目录下生成一个&lt;工程名&gt;目录，内容如下123456789101112131415&gt; tree /F /A myspider| scrapy.cfg # 项目配置文件|\---myspider # 核心目录，与工程目录名称一致 | items.py # 定义爬虫数据实体类 | middlewares.py # 爬虫中间件文件。如设置IP代理池、 | pipelines.py # 对爬取得到的实体对象进行处理，如输出、写入数据库等 | settings.py # 爬虫配置文件 | __init__.py | +---spiders # 存放你的爬虫文件 | | __init__.py | | | \---__pycache__ \---__pycache__ 4. Scrapy指令Scrapy指令根据作用域可分为两类： 全局指令: 在任何路径下都能执行 项目指令: 只能在Scrapy项目中才能执行 4.1. 全局指令4.1.1. 查看全局指令在Scrapy项目外，输入scrapy1234567891011121314151617181920&gt; scrapyScrapy 1.6.0 - no active projectUsage: scrapy &lt;command&gt; [options] [args]# 以下都是全局指令Available commands: bench Run quick benchmark test fetch Fetch a URL using the Scrapy downloader genspider Generate new spider using pre-defined templates runspider Run a self-contained spider (without creating a project) settings Get settings values shell Interactive scraping console startproject Create new project version Print Scrapy version view Open URL in browser, as seen by Scrapy [ more ] More commands available when run from project directoryUse "scrapy &lt;command&gt; -h" to see more info about a command 4.1.2. fetch 获取某个网页scrapy fetch &lt;url&gt; 通过Scrapy downloader获取某个网页1scrapy fetch https://www.baidu.com 4.1.3. runspider 脱离Scrapy项目运行爬虫文件一般爬虫文件是在Scrapy项目中运行的。有时创建Scrapy项目觉得麻烦，想直接运行下个爬虫文件，可以通过runspider指令 编写一个爬虫文件 spider_demo.py1234567891011import scrapyclass SpiderDemo(scrapy.Spider): # 给spider取个name name = 'baidu' # 要爬的域名 allowed_domains = ['baidu.com'] # 起始爬取的URL start_urls = ['https://www.baidu.com/'] def parse(self, response): btn = response.css('#su').get() print(btn) scrapy runspider &lt;爬虫文件&gt; 运行指定的爬虫1scrapy runspider spider_demo.py 4.1.4. settings 查看全局或者项目的settings.py配置在全局执行，就是或者全局配置。在Scrapy项目中执行，就是当前项目settings.py的配置1scrapy settings --get BOT_NAME 4.1.5. shell 交互式爬取scrapy shell &lt;要爬取的URL&gt; 开启交互式python终端，用来调试爬虫代码1scrapy shell http://www.baidu.com 4.1.6. version 查看版本1scrapy version 4.1.7. view 下载到本地爬取scrapy view 4.2. 项目指令4.2.1. 查看项目指令在Scrapy项目中，输入scrapy，去掉全局指令，剩下的就是项目指令123456789101112131415161718192021&gt; scrapyScrapy 1.6.0 - project: myspiderUsage: scrapy &lt;command&gt; [options] [args]Available commands: bench Run quick benchmark test check Check spider contracts crawl Run a spider edit Edit spider fetch Fetch a URL using the Scrapy downloader genspider Generate new spider using pre-defined templates list List available spiders parse Parse URL (using its spider) and print the results runspider Run a self-contained spider (without creating a project) settings Get settings values shell Interactive scraping console startproject Create new project version Print Scrapy version view Open URL in browser, as seen by Scrapy 4.2.2. check4.2.3. crawl 运行某个爬虫文件scrapy crawl &lt;爬虫名&gt; 运行爬虫文件 4.2.4. edit 编辑某个某虫文件4.2.5. list 列出当前项目可使用的爬虫文件1scrapy list 5. Scrapy部署流程5.1. 服务器安装scrapyd安装scrapyd1pip install scrapyd 5.2. 客户端安装scrapyd-client安装scrapyd-client1pip install scrapyd-client 5.3. 服务器启动scrapyd服务再修改scrapyd的配置文件 /usr/local/lib/python3.5/dist-packages/scrapyd/default_scrapyd.conf12# 修改监听的IP，默认是localhostbind_address = 0.0.0.0 再重启scrapyd服务 直接运行scrapyd命令，会监听localhost:68001scrapyd 5.4. 客户端部署爬虫到服务器进入Scrapy项目的目录，编辑scrapy.cfg12345# ===================== 以下是原本的内容[deploy]#url = http://localhost:6800/project = myproject# ===================== 以下是修改之后的内容 6. scrapyd命令使用6.1. 查看帮助1scrapyd --help 7. Linux Scrapy后台运行Scrapy12]]></content>
      <categories>
        <category>Scrapy</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python pip]]></title>
    <url>%2F2019%2F02%2F28%2Fpython%2Fpython-pip%2F</url>
    <content type="text"><![CDATA[1. pip命令1.1. 列出已安装的包1pip list]]></content>
      <categories>
        <category>python</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[搜房网 环境搭建]]></title>
    <url>%2F2019%2F02%2F27%2Fproject%2F%E6%90%9C%E6%88%BF%E7%BD%91%2F%E6%90%9C%E6%88%BF%E7%BD%91-%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1. 环境要求 JDK 1.8 + Maven + IDEA MySQL 5.7 ElasticSearch 6.2.4 SpringBoot 1.5.7 2. 数据库环境创建数据库 名称xunwu 编码utf8mb4 导入 resources/db/xunwu.sql 3. 后端框架搭建Spring Initializr Lombok, DevTools Web JPA Security Thymeleaf 4. 数据源配置12345678spring: datasource: # 如果不用type指定数据源，那么多数据源要加上前缀配置 hikari: jdbc-url: jdbc:mysql://localhost:3306/se?useUnicode=true&amp;characterEncoding=utf-8 username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver 5. SpringBoot2 关闭 Security HTTP Base身份验证如果想设置身份验证信息，可以在application.yml中设置如下信息12345spring: security: user: name: root password: 123456 开发时总要验证，太麻烦，所以关闭1234567// 添加exclude = &#123;SecurityAutoConfiguration.class&#125;@SpringBootApplication(exclude = &#123;SecurityAutoConfiguration.class&#125;)public class Application &#123; public static void main(String[] args) &#123; SpringApplication.run(Application.class, args); &#125;&#125; 集成H2内存数据库（未完成）不想直接从MySQL访问数据，直接从H2内存数据库获取 1234# application.ymlspring: profiles: active: dev # 激活dev 禁止Thymeleaf缓存1234spring: thymeleaf: mode: HTML cache: false SpringBoot devtools热加载spring为开发者提供了一个名为spring-boot-devtools的模块来使Spring Boot应用支持热部署，提高开发者的开发效率，无需手动重启Spring Boot应用 当我们修改了Java类后，IDEA默认是不自动编译的，而spring-boot-devtools又是监测classpath下的文件发生变化才会重启应用，所以需要设置IDEA的自动编译： ctrl + shift + alt + /,选择Registry,勾上 Compiler autoMake allow when app running 重启IDEA即可生效 默认修改静态资源也会热加载，这是不必要的。所以添加配置1spring.devtools.restart.exclude=templates/**,static/** thymeleaf视图能不直接使用@RestController@RestController使得所有返回json对象，不会跳转视图。使用普通的@Controller即可，要返回JSON的再加@ResponseBody]]></content>
      <tags>
        <tag>搜房网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MapReduce 运行原理]]></title>
    <url>%2F2019%2F02%2F25%2Fhadoop%2FMapReduce-%E8%BF%90%E8%A1%8C%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[1. InputFormat数据输入阶段1.1. 数据切片与MapTask并行度决定机制问题引出：MapTask的并行度决定Map阶段的任务处理并发度，进而影响到整个Job的处理速度。思考：1G的数据，启动8个MapTask，可以提高集群的并发处理能力。那么1K的数据，也启动8个MapTask，会提高集群性能吗？MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？ 概念区分： 数据块：Block是HDFS物理上把数据分成一块一块。 数据切片：只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储 1.2. Job提交流程源码详解12345678910111213141516171819202122232425262728293031323334waitForCompletion()submit();// 1建立连接 connect(); // 1）创建提交Job的代理 new Cluster(getConfiguration()); // （1）判断是本地yarn还是远程 initialize(jobTrackAddr, conf); // 2 提交jobsubmitter.submitJobInternal(Job.this, cluster) // 1）创建给集群提交数据的Stag路径 Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf); // 2）获取jobid ，并创建Job路径 JobID jobId = submitClient.getNewJobID(); // 3）拷贝jar包到集群 copyAndConfigureFiles(job, submitJobDir); rUploader.uploadFiles(job, jobSubmitDir); // 4）计算切片，生成切片规划文件 writeSplits(job, submitJobDir); maps = writeNewSplits(job, jobSubmitDir); input.getSplits(job);// 5）向Stag路径写XML配置文件 writeConf(conf, submitJobFile); conf.writeXml(out);// 6）提交Job,返回提交状态 status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials()); 1.3. FileInputFormat切片源码解析 找到你数据存储的目录 开始遍历处理（规划切片）目录下的每一个文件 遍历第一个文件ss.txt 获取文件大小fs.sizeOf(ss.txt) 计算切片大小 computeSliteSize(Math.max(minSize,Math.max(maxSize,blocksize))) = blocksize = 128M（集群模式下是128M，本地模式是32M）。本地模式运行时，HDFS认为一台机器性能不怎么好，所以就设置为32M。默认情况下，切片大小=blocksize，集群就是128M。 开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片） 将切片信息写到一个切片规划文件中 整个切片的核心过程在getSplit()方法中完成 InputSplit只记录了切片的元数据信息，比如起始位置、长度以及所在的节点列表等 提交切片规划文件到yarn上，yarn上的MrAppMaster就可以根据切片规划文件计算开启MapTask个数 1.4. FileInputFormat切片机制 简单地按照文件的内容长度进行切片 切片大小，默认等于block大小。MapReduce本地模式32M，运行在集群上128M 切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 假设有两个文件 12file1.txt 320Mfile2.txt 10M 要切成4片 1234file1.txt.split1 -- 0~128file1.txt.split2 -- 128~256file1.txt.split3 -- 256~320file2.txt.split1 -- 0~10M 1.4.1. FileInputFormat切片大小的参数配置（1）通过分析源码，在FileInputFormat中，计算切片大小的逻辑：Math.max(minSize, Math.min(maxSize, blockSize)); 切片主要由这几个值来运算决定 mapreduce.input.fileinputformat.split.minsize=1 默认值为1 mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue 默认值Long.MAXValue 因此，默认情况下，切片大小=blocksize （2）切片大小设置 maxsize（切片最大值）：参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。 minsize （切片最小值）：参数调的比blockSize大，则可以让切片变得比blocksize还大。 （3）获取切片信息API 1234// 根据文件类型获取切片信息FileSplit inputSplit = (FileSplit) context.getInputSplit();// 获取切片的文件名称String name = inputSplit.getPath().getName(); 1.5. CombineTextInputFormat切片机制框架默认的TextInputFormat切片机制是对任务按文件规划切片，不管文件多小，都会是一个单独的切片，都会交给一个MapTask，这样如果有大量小文件，就会产生大量的MapTask，处理效率极其低下 1.5.1. 应用场景CombineTextInputFormat用于小文件过多的场景，它可以将多个小文件从逻辑上规划到一个切片中，这样，多个小文件就可以交给一个MapTask处理 1.5.2. 虚拟存储切片最大值设置1CombineTextInputFormat.setMaxInputSplitSize(job, 4194304); // 4m 注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。 1.5.3. 切片机制生成切片过程包括：虚拟存储过程和切片过程二部分。 （1）虚拟存储过程： 将输入目录下所有文件大小，依次和设置的setMaxInputSplitSize值比较，如果不大于设置的最大值，逻辑上划分一个块。如果输入文件大于设置的最大值且大于两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值2倍，此时将文件均分成2个虚拟存储块（防止出现太小切片）。 例如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分成一个4M。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的小的虚拟存储文件，所以将剩余的4.02M文件切分成（2.01M和2.01M）两个文件。 （2）切片过程： 判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个切片。 如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。 测试举例：有4个小文件大小分别为1.7M、5.1M、3.4M以及6.8M这四个小文件，则虚拟存储之后形成6个文件块，大小分别为： 1.7M，（2.55M、2.55M），3.4M以及（3.4M、3.4M） 最终会形成3个切片，大小分别为： （1.7+2.55）M，（2.55+3.4）M，（3.4+3.4）M 1.6. TextlnputFormatTextlnputFormat是默认的FilelnputFomiat实现类。按行读取每条记录。键是存储该行在整个文件中的 起始字节偏移量，LongWritable类型。值是这行的内容，不包括任何行终止符（换行符和回车符）， Text类型。 以下是一个示例，比如，一个分片包含了如下4条文本记录。 1234Rich learning formIntelligent learning engineLearning more convenientFrom the real demand for more close to the enterprise 每条记录表示为以下键/值对： 1234(0,Rich learning form)(19,Intelligent learning engine)(47,Learning more convenient)(72,From the real demand for more close to the enterprise) 1.7. KeyValueTextInputFormat每一行均为一条记录，被分隔符分割为key，value。可以通过在驱动类中设置conf.set(KeyValueLineRecordReader.KEY_VALUE_SEPERATOR, &quot;\t&quot;);来设定分隔符。默认分隔符是tab 以下是一个示例，输入是一个包含4条记录的分片。其中——&gt;表示一个（水平方向的）制表符1234line1——&gt;Rich learning formline2——&gt;Intelligent learning engineline3——&gt;Learning more convenientline4——&gt;From the real demand for more close to the enterprise 每条记录表示为以下键/值对：此时的键是每行排在制表符之前的Text序列1234(line1,Rich learning form)(line2,Intelligent learning engine)(line3,Learning more convenient)(line4,From the real demand for more close to the enterprise) 2. MapTask和ReduceTask数量 MapTask数量：等于切片数 ReduceTask数量：等于分区数 一般来説，ReduceTask数量要少于MapTask数量，所以计算能在map中完成，就尽量不要拖到reduce再做。 3. Shuffle机制Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle 3.1. Partition分区问题引出：要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区） 3.1.1. 默认partition分区默认的分区策略取决于key.hashCode()和numReduceTasks， 1234567public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt; &#123; public int getPartition(K key, V value, int numReduceTasks) &#123; // &amp; Integer.MAX_VALUE 是取低32位数据 // 默认numReduceTasks=1，所以取余后返回0，即默认是partition0 return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; 分区数量就是ReduceTask的数量，即源代码中的numReduceTasks 结果要对numReduceTasks取余，所以各个分区的编号分别为0、1、2、……、numReduceTasks-1 在没有修改numReduceTasks的情况下，numReduceTasks默认值为1，所以getPartition()返回值对1取余，结果恒定为0，所以就只有1个分区 3.2. 排序排序是MapReduce框架中最重要的操作之一。MapTask和ReduceTask均会对数据（按照key）进行排序。 㧋操作属于Hadoop的默认行为。任何MapReduce应用程序中的数据均会被排序，而不管逻辑上是否需要。 默认排序是按照字典顺序排序，且实现该排序的方法是快速排序 + 归并排序。生成每个有序的小文件使用快速排序，将有序小文件合并为有序大文件使用归并排序 对于MapTask，它会将处理的结果暂时放到一个缓冲区中，当缓冲区使用率达到一定阈值后（默认环形缓冲区大小是100M，阈值是80%），再对缓冲区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，生成一个有序的文件。而当数据处理完毕后，它会通过归并排序对磁盘上所有文件进行一次合并，以将这些文件合并成一个大的有序文件。 对于ReduceTask，它从每个MapTask上远程拷贝相应的数据文件（注意是ReduceTask主动到MapTask拉取数据，而不是MapTask主动将数据送往ReduceTask），如果文件大小超过一定阈值，则溢写磁盘上，否则存储在内存中。如果磁盘上文件数目达到一定阈值，则进行一次归并排序以生成一个更大文件（阶段性的归并排序）；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据溢写到磁盘上。当所有数据拷贝完毕后，ReduceTask统一对内存和磁盘上的所有数据进行一次归并排序。 3.2.1. 排序分类1）部分排序：MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部排序。文件数由numReduceTasks决定 2）全排序：全排序：最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask。 如何用Hadoop产生一个全局排序的文件？最简单的方法是使用一个分区，即设置numReduceTask=1。但该方法在处理大型文件时效率极低，因为一台机器必须处理所有输出文件，从而完全丧失了MapReduce所提供的并行架构。 替代方案：首先创建一系列排好序的文件；其次，串联这些文件；最后，生成一个全局排序的文件。主要思路是使用一个分区来描述输出的全局排序。例如：可以为上述文件创建3个分区，在第一分区中，记录的单词首字母a-g，第二分区记录单词首字母h-n, 第三分区记录单词首字母o-z。 3）辅助排序：（GroupingComparator分组）在Reduce端对key进行排序。应用于：在接收的key为bean对象时，想让一个或几个字段相同（全部 字段比较不相同）的key进入到同一个reduce方法时，可以采用分组序。 Mapreduce框架在记录到达Reducer之前按key对记录排序，但key所对应的value并没有被排序。甚至在不同的执行轮次中，这些值的排序也不固定，因为它们来自不同的map任务且这些map任务在不同轮次中完成时间各不相同。一般来说，大多数MapReduce程序会避免让reduce函数依赖于值的排序。但是，有时也需要通过特定的方法对键进行排序和分组等以实现对值的排序。 4）二次排序：在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。相同，如果判断条件为3个，即三次排序； 如果判断条件为N个，即N次排序]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 数据序列化]]></title>
    <url>%2F2019%2F02%2F25%2Fhadoop%2FHadoop-%E6%95%B0%E6%8D%AE%E5%BA%8F%E5%88%97%E5%8C%96%2F</url>
    <content type="text"><![CDATA[1. Hadoop序列化1.1. 什么是序列化序列化就是把内存中的对象转换成字节序列（二进制流）以便于存储到磁盘（持久化）和网络传输。 反序列化就是将字节序列转换成内存中的对象。 1.2. 为什么要序列化原本对象只能存在于内存中 想把对象保存到磁盘，要先序列化，把对象转为二进制数据的形式保存到磁盘。 想把对象通过网络传输给远程计算机，要先序列化，把对象转为二进制数据的形式传输给远程永无止息器。 序列化还有跨语言、跨平台的好处。例如Java对象序列化后，二进制流可以由PHP客户端接收，再按一定的规则将二进制流反序列化为PHP中的对象 Hadoop涉及数据的存储和传输，所以要引入序列化 1.3. Hadoop为什么不使用Java自带的Serializable序列化Java的序列化是一个重量级序列化框架（Serializable)，一个对象被序列化后，会附带很多额外的信息（各种校验信息、Header、继承体系等），不便于在网络中高效传输。所以，Hadoop自己开发了一套序列化机制（Writable)。 1.4. Hadoop序列化的特点Hadoop中各个节点的通信是通过远程调用（RPC）实现的，那么RPC序列化要求具有以下特点：1）紧凑：紧凑的格式能让我们充分利用网络带宽，而带宽是数据中心最稀缺的资2）快速：进程通信形成了分布式系统的骨架，所以需要尽量减少序列化和反序列化的性能开销，这是基本的；3）可扩展：随着通信协议的升级而可升级（支持通信协议的变化）；4）互操作：能支持不同语言写的客户端和服务端进行交互；例如Java对象序列化后，二进制流可以由PHP客户端接收，再按一定的规则将二进制流反序列化为PHP中的对象 1.5. Hadoop中常用的序列化类型 Java类型 Hadoop Writable类型 boolean BooleanWritable byte ByteWritable int IntWritable float FloatWritable long LongWritable double DoubleWritable String Text map MapWritable array ArrayWritable 自定义bean的序列化在企业开发中往往常用的基本序列化类型不能满足所有需求，比如在Hadoop框架内部传递一个bean对象，那么该对象就需要实现序列化接口 具体实现bean对象序列化步骤如下7步]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MapReduce FlowSum编程案例]]></title>
    <url>%2F2019%2F02%2F25%2Fhadoop%2FMapReduce-FlowSum%E7%BC%96%E7%A8%8B%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[1. 官方WordCount源码分析1.1. 添加依赖1234567891011121314151617181920212223&lt;!-- 3个基本依赖 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-examples&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 1.2. 查看WordCount源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package org.apache.hadoop.examples;import java.io.IOException;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;public class WordCount &#123; public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; &#123; /** * 输出的value是常量1 */ private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) &#123; word.set(itr.nextToken()); context.write(word, one); &#125; &#125; &#125; public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context ) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; result.set(sum); context.write(key, result); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); if (otherArgs.length &lt; 2) &#123; System.err.println("Usage: wordcount &lt;in&gt; [&lt;in&gt;...] &lt;out&gt;"); System.exit(2); &#125; // 过时源码 Job job = new Job(conf, "word count"); Job job = Job.getInstance(conf, "word count"); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); for (int i = 0; i &lt; otherArgs.length - 1; ++i) &#123; FileInputFormat.addInputPath(job, new Path(otherArgs[i])); &#125; FileOutputFormat.setOutputPath(job, new Path(otherArgs[otherArgs.length - 1])); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 2. FlowSum总流量统计目的：学习自定义序列化Bean 2.1. 输入数据输入数据 phone_data.txt 的内容如下： 123456789101112131415161718192021221 13736230513 192.196.100.1 www.atguigu.com 2481 24681 2002 13846544121 192.196.100.2 264 0 2003 13956435636 192.196.100.3 132 1512 2004 13966251146 192.168.100.1 240 0 4045 18271575951 192.168.100.2 www.atguigu.com 1527 2106 2006 84188413 192.168.100.3 www.atguigu.com 4116 1432 2007 13590439668 192.168.100.4 1116 954 2008 15910133277 192.168.100.5 www.hao123.com 3156 2936 2009 13729199489 192.168.100.6 240 0 20010 13630577991 192.168.100.7 www.shouhu.com 6960 690 20011 15043685818 192.168.100.8 www.baidu.com 3659 3538 20012 15959002129 192.168.100.9 www.atguigu.com 1938 180 50013 13560439638 192.168.100.10 918 4938 20014 13470253144 192.168.100.11 180 180 20015 13682846555 192.168.100.12 www.qq.com 1938 2910 20016 13992314666 192.168.100.13 www.gaga.com 3008 3720 20017 13509468723 192.168.100.14 www.qinghua.com 7335 110349 40418 18390173782 192.168.100.15 www.sogou.com 9531 2412 20019 13975057813 192.168.100.16 www.baidu.com 11058 48243 20020 13768778790 192.168.100.17 120 120 20021 13568436656 192.168.100.18 www.alibaba.com 2481 24681 20022 13568436656 192.168.100.19 1116 954 200 输入数据格式如下，每个字段之间以 制表符分隔 id 手机号码 ip地址 上行流量 下行流量 网络状态码 7 13560436666 120.196.100.99 1116 954 200 注意同一手机号可能有多条记录，要记得累加起来 1221 13568436656 192.168.100.18 www.alibaba.com 2481 24681 20022 13568436656 192.168.100.19 1116 954 200 2.2. 需求说明统计每一个手机号耗费的总上行流量、下行流量、总流量。期望输出数据格式如下： 手机号码 上行流量 下行流量 总流量（上行+下行） 13560436666 1116 120.196.100.99 2070 2.3. 需求分析程序实现的功能是： 1SELECT SUM(上行流量), SUM(下行流量)、SUM(上行流量+下行流量) GROUP BY 手机号 Input阶段：使用默认的TextInputFormat，按行读取 Map阶段： 读取一行数据，切分字段 提取出手机号、上行流量、下行流量 输出 &lt;手机号，bean（上行流量，下行流量，总流量）&gt; Reduce阶段： 累加所有的上行流量、下行流量、得到总流量 输出 &lt;手机号，bean（上行流量，下行流量，总流量）&gt; 2.4. 编写MapReduce程序2.4.1. 编写流量统计的Bean12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package cn.pancx.mr.flowsum;import org.apache.hadoop.io.Writable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;// 1. 实现Writable接口public class FlowBean implements Writable &#123; /** * 上行流量 */ private long upFlow; /** * 下行流量 */ private long downFlow; /** * 总流量（上行流量 + 下行流量） */ private long sumFlow; // 2. 反序列化时，需要反射调用空参构造函数，所以必须有一个空参构造函数 public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; // 3. 实现序列化方法 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; // 4. 实现反序列化方法 @Override public void readFields(DataInput in) throws IOException &#123; // 反序列化方法读顺序必须和写序列化方法的写顺序必须一致 this.upFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong(); &#125; // 5. 编写toString方法，方便后续输出观察 @Override public String toString() &#123; return upFlow + "\t" + downFlow + "\t" + sumFlow; &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125;&#125; 2.4.2. 编写Mapper/Reducer/Driver123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293package cn.pancx.mr.flowsum;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class FlowSumDriver &#123; public static class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt; &#123; FlowBean v = new FlowBean(); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割字段 String[] fields = line.split("\t"); // 3 封装对象 // 取出手机号码 String phoneNum = fields[1]; // 取出上行流量和下行流量 long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); k.set(phoneNum); v.setDownFlow(downFlow); v.setUpFlow(upFlow); v.setSumFlow(upFlow + downFlow); // 4 写出 context.write(k, v); &#125; &#125; public static class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123; long sumUpFlow = 0; long sumDownFlow = 0; // 1 遍历所用bean，将其中的上行流量，下行流量分别累加 for (FlowBean flowBean : values) &#123; sumUpFlow += flowBean.getUpFlow(); sumDownFlow += flowBean.getDownFlow(); &#125; // 2 封装对象 FlowBean resultBean = new FlowBean(sumUpFlow, sumDownFlow); // 3 写出 context.write(key, resultBean); &#125; &#125; public static void main(String[] args) throws Exception &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowSumDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 2.5. 上传到服务器运行本地打包，再重命名为flowsum.jar 1mvn package 上传数据到HDFS 12hdfs dfs -mkdir /inputhdfs dfs -put phone_data.txt /input/ 运行MapReduce 1hadoop jar flowsum.jar cn.pancx.mr.flowsum.FlowSumDriver /input /output 查看结果 12345678910111213141516171819202122$ hdfs dfs -cat /output/part*13470253144 180 180 36013509468723 7335 110349 11768413560439638 918 4938 585613568436656 3597 25635 2923213590439668 1116 954 207013630577991 6960 690 765013682846555 1938 2910 484813729199489 240 0 24013736230513 2481 24681 2716213768778790 120 120 24013846544121 264 0 26413956435636 132 1512 164413966251146 240 0 24013975057813 11058 48243 5930113992314666 3008 3720 672815043685818 3659 3538 719715910133277 3156 2936 609215959002129 1938 180 211818271575951 1527 2106 363318390173782 9531 2412 1194384188413 4116 1432 5548 3. FlowSum扩展：手机号按前3位分区目的：学习自定义分区，以及分区数的设置 3.1. 需求说明统计每一个手机号耗费的总上行流量、下行流量、总流量。期望输出数据格式如下： 手机号码 上行流量 下行流量 总流量（上行+下行） 13560436666 1116 954 2070 分区要求：手机号136、137、138、139开头都分别放到一个独立的4个文件中，其他开头的放到一个文件中 3.2. 需求分析Map和Reduce的业务逻辑都没有变，唯一要做的就是对结果进行分区 增加一个自定义分区类CustomPartitioner，分出以下5个区： 12345136 分区0137 分区1138 分区2139 分区3other 分区4 再在Driver中添加相应设置 1234// 指定自定义分区job.setPartitionerClass(CustomPartitioner.class);// 同时指定相应数量的reduce taskjob.setNumReduceTasks(5); 3.3. 编写MapReduce程序123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124package cn.pancx.mr.flowsum;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class FlowSumDriver &#123; /** * 自定义分区类 */ public static class CustomPartitioner extends Partitioner&lt;Text, FlowBean&gt; &#123; @Override public int getPartition(Text key, FlowBean flowBean, int numPartitions) &#123; // 1 获取电话号码的前三位 String preNum = key.toString().substring(0, 3); // 2 根据前三位进行分区 int partition = 4; if ("136".equals(preNum)) &#123; partition = 0; &#125; else if ("137".equals(preNum)) &#123; partition = 1; &#125; else if ("138".equals(preNum)) &#123; partition = 2; &#125; else if ("139".equals(preNum)) &#123; partition = 3; &#125; return partition; &#125; &#125; public static class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt; &#123; FlowBean v = new FlowBean(); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割字段 String[] fields = line.split("\t"); // 3 封装对象 // 取出手机号码 String phoneNum = fields[1]; // 取出上行流量和下行流量 long upFlow = Long.parseLong(fields[fields.length - 3]); long downFlow = Long.parseLong(fields[fields.length - 2]); k.set(phoneNum); v.setDownFlow(downFlow); v.setUpFlow(upFlow); v.setSumFlow(upFlow + downFlow); // 4 写出 context.write(k, v); &#125; &#125; public static class FlowCountReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Context context) throws IOException, InterruptedException &#123; long sumUpFlow = 0; long sumDownFlow = 0; // 1 遍历所用bean，将其中的上行流量，下行流量分别累加 for (FlowBean flowBean : values) &#123; sumUpFlow += flowBean.getUpFlow(); sumDownFlow += flowBean.getDownFlow(); &#125; // 2 封装对象 FlowBean resultBean = new FlowBean(sumUpFlow, sumDownFlow); // 3 写出 context.write(key, resultBean); &#125; &#125; public static void main(String[] args) throws Exception &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowSumDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(FlowBean.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 8 指定自定义数据分区 job.setPartitionerClass(CustomPartitioner.class); // 9 同时指定相应数量的reduce task job.setNumReduceTasks(5); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 3.4. 上传到服务器运行1hadoop jar flowsum.jar cn.pancx.mr.flowsum.FlowSumDriver /input /output 查看分区结果 123456789101112131415161718192021222324252627282930$ hdfs dfs -cat /output/part*013630577991 6960 690 765013682846555 1938 2910 4848$ hdfs dfs -cat /output/part*113729199489 240 0 24013736230513 2481 24681 2716213768778790 120 120 240$ hdfs dfs -cat /output/part*213846544121 264 0 264$ hdfs dfs -cat /output/part*313956435636 132 1512 164413966251146 240 0 24013975057813 11058 48243 5930113992314666 3008 3720 6728$ hdfs dfs -cat /output/part*413470253144 180 180 36013509468723 7335 110349 11768413560439638 918 4938 585613568436656 3597 25635 2923213590439668 1116 954 207015043685818 3659 3538 719715910133277 3156 2936 609215959002129 1938 180 211818271575951 1527 2106 363318390173782 9531 2412 1194384188413 4116 1432 5548 3.5. 测试调整numReduceTasks对结果的影响设置为1 1job.setNumReduceTasks(1); 运行，结果相当于只有1个分区 12345678910111213141516171819202122232425$ hdfs dfs -ls -R /output-rw-r--r-- 1 root supergroup 0 2019-02-26 20:29 /output/_SUCCESS-rw-r--r-- 1 root supergroup 550 2019-02-26 20:29 /output/part-r-00000$ hdfs dfs -cat /output/part*13470253144 180 180 36013509468723 7335 110349 11768413560439638 918 4938 585613568436656 3597 25635 2923213590439668 1116 954 207013630577991 6960 690 765013682846555 1938 2910 484813729199489 240 0 24013736230513 2481 24681 2716213768778790 120 120 24013846544121 264 0 26413956435636 132 1512 164413966251146 240 0 24013975057813 11058 48243 5930113992314666 3008 3720 672815043685818 3659 3538 719715910133277 3156 2936 609215959002129 1938 180 211818271575951 1527 2106 363318390173782 9531 2412 1194384188413 4116 1432 5548 设置为2，小于实际分区数 1job.setNumReduceTasks(2); 运行，出现异常 1java.io.IOException: Illegal partition for 13846544121 (2) 设置为6，大于实际分区数 1job.setNumReduceTasks(6); 运行，结果生成6个文件，最后1个为空，前5个有结果 12345678$ hdfs dfs -ls -R /output-rw-r--r-- 1 root supergroup 0 2019-02-26 20:33 /output/_SUCCESS-rw-r--r-- 1 root supergroup 53 2019-02-26 20:33 /output/part-r-00000-rw-r--r-- 1 root supergroup 75 2019-02-26 20:33 /output/part-r-00001-rw-r--r-- 1 root supergroup 22 2019-02-26 20:33 /output/part-r-00002-rw-r--r-- 1 root supergroup 105 2019-02-26 20:33 /output/part-r-00003-rw-r--r-- 1 root supergroup 295 2019-02-26 20:33 /output/part-r-00004-rw-r--r-- 1 root supergroup 0 2019-02-26 20:33 /output/part-r-00005 # 空结果 总结： 如果ReduceTask的数量 &gt; getPartition的结果数，则会多产生几个空的输出文件 如果1 &lt; ReduceTask的数量 &lt; getPartition的结果数，则有一部分分区数据无处安放，抛出异常 如果ReduceTask的数量=1 ,则不管MapTask端输出多少个分区文件，最终结 ReduceTask都交给这一个ReduceTask，最终也就只会产生一个结果文件part-r-00000 分区号必须从零开始，逐一累加 4. FlowSum总流量统计结果全排序4.1. 输入数据输入数据就是FlowSum总流量的统计结果 12345678910111213141516171819202113470253144 180 180 36013509468723 7335 110349 11768413560439638 918 4938 585613568436656 3597 25635 2923213590439668 1116 954 207013630577991 6960 690 765013682846555 1938 2910 484813729199489 240 0 24013736230513 2481 24681 2716213768778790 120 120 24013846544121 264 0 26413956435636 132 1512 164413966251146 240 0 24013975057813 11058 48243 5930113992314666 3008 3720 672815043685818 3659 3538 719715910133277 3156 2936 609215959002129 1938 180 211818271575951 1527 2106 363318390173782 9531 2412 1194384188413 4116 1432 5548 输入数据格式如下，每个字段之间以 制表符分隔 手机号码 上行流量 下行流量 总流量（上行+下行） 13560436666 1116 954 2070 4.2. 需求说明输出与输入格式相同，但是要求按照总流量降序排序 4.3. 需求分析FlowBean实现WritableComparable接口，重写compareTo方法，实现总流量降序排序 在Mapper中，输出&lt;FlowBean，手机号&gt;，这样Shuffle就能对key进行排序了 在Reducer中，接收到的根据FlowBean排序过的。直接输出&lt;手机号，FlowBean&gt;即可。注意在排序过程中，可能有两条记录的总流量是一样大的，所以valueIn可能对应多个手机号，统一循环输出所有的&lt;手机号，FlowBean&gt;即可。 4.4. 编写程序4.4.1. FlowBean123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package cn.pancx.mr.flowsum;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;// 1. 实现WritableComparable接口public class FlowBean implements WritableComparable&lt;FlowBean&gt; &#123; /** * 上行流量 */ private long upFlow; /** * 下行流量 */ private long downFlow; /** * 总流量（上行流量 + 下行流量） */ private long sumFlow; // 2. 反序列化时，需要反射调用空参构造函数，所以必须有一个空参构造函数 public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; // 3. 实现序列化方法 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; // 4. 实现反序列化方法 @Override public void readFields(DataInput in) throws IOException &#123; // 反序列化方法读顺序必须和写序列化方法的写顺序必须一致 this.upFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong(); &#125; // 5. 编写toString方法，方便后续输出观察 @Override public String toString() &#123; return upFlow + "\t" + downFlow + "\t" + sumFlow; &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125; /** * 6、重写compareTo方法 * @param o * @return */ @Override public int compareTo(FlowBean o) &#123; int result; if (sumFlow &gt; o.getSumFlow()) &#123; result = -1; &#125; else if (sumFlow &lt; o.getSumFlow()) &#123; result = 1; &#125; else &#123; result = 0; &#125; return result; &#125;&#125; 4.4.2. 编写MapReduce12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182package cn.pancx.mr.flowsum;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class FlowSumDriver &#123; public static class FlowCountMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt; &#123; FlowBean k = new FlowBean(); Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割字段 String[] fields = line.split("\t"); // 3 封装对象 // 取出手机号码 String phoneNum = fields[0]; // 取出上行流量和下行流量 long upFlow = Long.parseLong(fields[1]); long downFlow = Long.parseLong(fields[2]); long sumFlow = Long.parseLong(fields[3]); k.setUpFlow(upFlow); k.setDownFlow(downFlow); k.setSumFlow(sumFlow); v.set(phoneNum); // 4 写出 context.write(k, v); &#125; &#125; public static class FlowCountReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; &#123; @Override protected void reduce(FlowBean bean, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; for (Text phone : values) &#123; context.write(phone, bean); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowSumDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 4.5. 运行输出结果 12345678910111213141516171819202122$ hdfs dfs -cat /output/part*13509468723 7335 110349 11768413975057813 11058 48243 5930113568436656 3597 25635 2923213736230513 2481 24681 2716218390173782 9531 2412 1194313630577991 6960 690 765015043685818 3659 3538 719713992314666 3008 3720 672815910133277 3156 2936 609213560439638 918 4938 585684188413 4116 1432 554813682846555 1938 2910 484818271575951 1527 2106 363315959002129 1938 180 211813590439668 1116 954 207013956435636 132 1512 164413470253144 180 180 36013846544121 264 0 26413966251146 240 0 24013768778790 120 120 24013729199489 240 0 240 5. FlowSum总流量统计结果分区排序5.1. 输入数据输入数据就是FlowSum总流量的统计结果 12345678910111213141516171819202113470253144 180 180 36013509468723 7335 110349 11768413560439638 918 4938 585613568436656 3597 25635 2923213590439668 1116 954 207013630577991 6960 690 765013682846555 1938 2910 484813729199489 240 0 24013736230513 2481 24681 2716213768778790 120 120 24013846544121 264 0 26413956435636 132 1512 164413966251146 240 0 24013975057813 11058 48243 5930113992314666 3008 3720 672815043685818 3659 3538 719715910133277 3156 2936 609215959002129 1938 180 211818271575951 1527 2106 363318390173782 9531 2412 1194384188413 4116 1432 5548 输入数据格式如下，每个字段之间以 制表符分隔 手机号码 上行流量 下行流量 总流量（上行+下行） 13560436666 1116 954 2070 5.2. 需求说明输出与输入格式相同，但是要求按照总流量降序排序，并且根据手机号前3位分区：手机号136、137、138、139开头都分别放到一个独立的4个文件中，其他开头的放到一个文件中 5.3. 需求分析与”FlowSum总流量统计结果全排序”的思路一致，加上自定义分区即可 在Mapper中，输出&lt;FlowBean，手机号&gt;，这样Shuffle就能对key进行排序了 在Reducer中，接收到的根据FlowBean排序过的。直接输出&lt;手机号，FlowBean&gt;即可。注意在排序过程中，可能有两条记录的总流量是一样大的，所以valueIn可能对应多个手机号，统一循环输出所有的&lt;手机号，FlowBean&gt;即可。 5.4. 编写程序5.4.1. FlowBean实现WritableComparable 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100package cn.pancx.mr.flowsum;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;// 1. 实现WritableComparable接口public class FlowBean implements WritableComparable&lt;FlowBean&gt; &#123; /** * 上行流量 */ private long upFlow; /** * 下行流量 */ private long downFlow; /** * 总流量（上行流量 + 下行流量） */ private long sumFlow; // 2. 反序列化时，需要反射调用空参构造函数，所以必须有一个空参构造函数 public FlowBean() &#123; super(); &#125; public FlowBean(long upFlow, long downFlow) &#123; super(); this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; // 3. 实现序列化方法 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; // 4. 实现反序列化方法 @Override public void readFields(DataInput in) throws IOException &#123; // 反序列化方法读顺序必须和写序列化方法的写顺序必须一致 this.upFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong(); &#125; // 5. 编写toString方法，方便后续输出观察 @Override public String toString() &#123; return upFlow + "\t" + downFlow + "\t" + sumFlow; &#125; public long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; public long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125; /** * 重写compareTo方法 * @param o * @return */ @Override public int compareTo(FlowBean o) &#123; int result; if (sumFlow &gt; o.getSumFlow()) &#123; result = -1; &#125; else if (sumFlow &lt; o.getSumFlow()) &#123; result = 1; &#125; else &#123; result = 0; &#125; return result; &#125;&#125; 5.4.2. 编写MapReduce123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112package cn.pancx.mr.flowsum;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Partitioner;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class FlowSumDriver &#123; /** * 自定义分区类 */ public static class CustomPartitioner extends Partitioner&lt;FlowBean, Text&gt; &#123; @Override public int getPartition(FlowBean flowBean, Text phone, int numPartitions) &#123; // 1 获取电话号码的前三位 String preNum = phone.toString().substring(0, 3); // 2 根据前三位进行分区 int partition = 4; if ("136".equals(preNum)) &#123; partition = 0; &#125; else if ("137".equals(preNum)) &#123; partition = 1; &#125; else if ("138".equals(preNum)) &#123; partition = 2; &#125; else if ("139".equals(preNum)) &#123; partition = 3; &#125; return partition; &#125; &#125; public static class FlowCountMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt; &#123; FlowBean k = new FlowBean(); Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; // 1 获取一行 String line = value.toString(); // 2 切割字段 String[] fields = line.split("\t"); // 3 封装对象 // 取出手机号码 String phoneNum = fields[0]; // 取出上行流量和下行流量 long upFlow = Long.parseLong(fields[1]); long downFlow = Long.parseLong(fields[2]); long sumFlow = Long.parseLong(fields[3]); k.setUpFlow(upFlow); k.setDownFlow(downFlow); k.setSumFlow(sumFlow); v.set(phoneNum); // 4 写出 context.write(k, v); &#125; &#125; public static class FlowCountReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; &#123; @Override protected void reduce(FlowBean bean, Iterable&lt;Text&gt; values, Context context) throws IOException, InterruptedException &#123; for (Text phone : values) &#123; context.write(phone, bean); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; // 1 获取配置信息，或者job对象实例 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 6 指定本程序的jar包所在的本地路径 job.setJarByClass(FlowSumDriver.class); // 2 指定本业务job要使用的mapper/Reducer业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReducer.class); // 3 指定mapper输出数据的kv类型 job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); // 4 指定最终输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); // 5 指定job的输入原始文件所在目录 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 8 指定自定义数据分区 job.setPartitionerClass(CustomPartitioner.class); // 9 同时指定相应数量的reduce task job.setNumReduceTasks(5); // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125; 5.5. 运行123456789101112131415161718192021222324252627282930$ hdfs dfs -cat /output/part*013630577991 6960 690 765013682846555 1938 2910 4848$ hdfs dfs -cat /output/part*113736230513 2481 24681 2716213729199489 240 0 24013768778790 120 120 240$ hdfs dfs -cat /output/part*213846544121 264 0 264$ hdfs dfs -cat /output/part*313975057813 11058 48243 5930113992314666 3008 3720 672813956435636 132 1512 164413966251146 240 0 240$ hdfs dfs -cat /output/part*413509468723 7335 110349 11768413568436656 3597 25635 2923218390173782 9531 2412 1194315043685818 3659 3538 719715910133277 3156 2936 609213560439638 918 4938 585684188413 4116 1432 554818271575951 1527 2106 363315959002129 1938 180 211813590439668 1116 954 207013470253144 180 180 360 6. FlowSum总流量统计结果TopN目的：练习TopN 6.1. 输入数据输入数据就是FlowSum总流量的统计结果 12345678910111213141516171819202113470253144 180 180 36013509468723 7335 110349 11768413560439638 918 4938 585613568436656 3597 25635 2923213590439668 1116 954 207013630577991 6960 690 765013682846555 1938 2910 484813729199489 240 0 24013736230513 2481 24681 2716213768778790 120 120 24013846544121 264 0 26413956435636 132 1512 164413966251146 240 0 24013975057813 11058 48243 5930113992314666 3008 3720 672815043685818 3659 3538 719715910133277 3156 2936 609215959002129 1938 180 211818271575951 1527 2106 363318390173782 9531 2412 1194384188413 4116 1432 5548 6.2. 需求说明根据总流量进行降序排序，取TopN条记录 6.3. 需求分析要根据总流量降序排序，所以在Mapper中，输出。其中FlowBean实现WritableComparable方法，根据总流量降序排序 取TopN思路1：在Map阶段取TopN。在执行Mapper的map()时，用TreeMap收集TopN个。在执行地Mapper的clearup()时，将TreeMap所有的输出 取TopN思路2：在Map阶段直接输出。在Reduce阶段也是直接输出。但是要设置一个变量，统计输出了，达到TopN个就不再输出 两种思路对比：一般来説，ReduceTask数量要少于MapTask数量，所以计算能在map中完成，就尽量不要拖到reduce再做。所以思路1更好 6.4. 编写程序6.4.1. FlowBean12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849package mr.flowsum;import lombok.Data;import org.apache.hadoop.io.WritableComparable;import java.io.DataInput;import java.io.DataOutput;import java.io.IOException;@Datapublic class FlowBean implements WritableComparable&lt;FlowBean&gt; &#123; private long upFlow; private long downFlow; private long sumFlow; @Override public int compareTo(FlowBean o) &#123; if (sumFlow &gt; o.sumFlow) &#123; return -1; &#125; else if (sumFlow &lt; o.sumFlow) &#123; return 1; &#125; else &#123; return 0; &#125; &#125; @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow); &#125; @Override public void readFields(DataInput in) throws IOException &#123; this.upFlow = in.readLong(); this.downFlow = in.readLong(); this.sumFlow = in.readLong(); &#125; @Override public String toString() &#123; return upFlow + "\t" + downFlow + "\t" + sumFlow; &#125;&#125; 6.4.2. 编写MapReduce（思路1：在Map阶段处理TopN）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495package mr.flowsum;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;import java.util.TreeMap;public class FlowSumDriver &#123; public static class FlowSumMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt; &#123; public static final Integer TOPN = 10; private TreeMap&lt;FlowBean, Text&gt; treeMap = new TreeMap&lt;&gt;(); /// private FlowBean k = new FlowBean(); 注意：这里key对象不要复用 // 复用时将key添加到treeMap，会视为同一个对象，不断覆盖，使得treeMap始终只有一个元素 private Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] split = line.split("\t"); String phoneNum = split[0]; // 每次创建一个新的key对象，这样放入TreeMap时，不会发生覆盖 FlowBean k = new FlowBean(); k.setUpFlow(Long.valueOf(split[1])); k.setDownFlow(Long.valueOf(split[2])); k.setSumFlow(Long.valueOf(split[3])); v.set(phoneNum); /// context.write(k, v); 注意：不在map中直接输出，再是将 &lt;k,v&gt;存入TreeMap treeMap.put(k, v); System.out.println("DEBUG Mapper map: " + k + "----" + v); // TreeMap中只取key最大的TOPN个，因为key（FlowBean）是根据总流量降序排序的，所以是取总流量前10个 if (treeMap.size() &gt; TOPN) &#123; treeMap.remove(treeMap.lastKey()); &#125; &#125; @Override protected void cleanup(Context context) &#123; // 不在map阶段输出，而是在clearup输出TreeMap的所有元素 treeMap.forEach((k, v) -&gt; &#123; try &#123; System.out.println("DEBUG Mapper cleanup: " + k + "----" + v); context.write(k, v); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;); &#125; &#125; public static class FlowSumReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; &#123; @Override protected void reduce(FlowBean flowBean, Iterable&lt;Text&gt; phoneNums, Context context) throws IOException, InterruptedException &#123; for (Text phone : phoneNums) &#123; context.write(phone, flowBean); System.out.println("DEBUG Reducer reduce: " + phone + "----" + flowBean); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setMapperClass(FlowSumMapper.class); job.setReducerClass(FlowSumReducer.class); job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 6.4.3. 编写MapReduce（思路2：在Reduce阶段处理TopN）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package mr.flowsum;import com.sun.tools.javac.comp.Flow;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import java.io.IOException;public class FlowSumDriver &#123; public static class FlowSumMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt; &#123; private FlowBean k = new FlowBean(); private Text v = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] split = line.split("\t"); String phoneNum = split[0]; k.setUpFlow(Long.valueOf(split[1])); k.setDownFlow(Long.valueOf(split[2])); k.setSumFlow(Long.valueOf(split[3])); v.set(phoneNum); context.write(k, v); System.out.println("DEBUG Mapper map: " + k + "---" + v); &#125; &#125; public static class FlowSumReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt; &#123; public static final int TOPN = 10; // 用一个变量统计已经输出了多少个&lt;key,value&gt; private int count = 0; @Override protected void reduce(FlowBean flowBean, Iterable&lt;Text&gt; phoneNums, Context context) throws IOException, InterruptedException &#123; for (Text phoneNum : phoneNums) &#123; // 已经输出TOPN个，后面的就不再输出 if (count &gt;= TOPN) &#123; break; &#125; context.write(phoneNum, flowBean); ++count; System.out.println("DEBUG Reducer reduce: " + phoneNum + "---" + flowBean); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); job.setMapperClass(FlowSumMapper.class); job.setReducerClass(FlowSumReducer.class); job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MapReduce 概述]]></title>
    <url>%2F2019%2F02%2F25%2Fhadoop%2FMapReduce%20%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1. 什么是MapReduce1.1. MapReduce定义MapReduce是一个分布式运算程序的编程框架，是用户开发”基于Hadoop的数据分析应用”的核心框架。 MapReduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个Hadoop集群上。 1.2. MapReduce优点易于编程 它简单的实现些接口，就可以完成个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写 —个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。 良好的扩展性 当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。 高容错性 其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，完全是由Hadoop内部完成的 适合PB级以上海量数据的离线处理 可以实现上千台服务器集群并发工作，提供数据处理能力 1.3. MapReduce缺点不擅长实时计算 无法像Mysql一样，在毫秒或者秒级内返回结果 不擅长流式计算 流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的 不擅长DAG（有向图）计算 多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MaoReduce并不是不能做，而是使用后每个MapReduce作业的输出结果都会写入到磁盘，造成大量的磁盘IO，导致性能非常低下 2. MapReduce核心思想 1）分布式的运算程序往往需要分成至少2个阶段。 2）第一个阶段的MapTask并发实例，完全并行运行，互不相干。 3）第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出。 4）MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。 总结：分析WordCount数据流走向深入理解MapReduce核心思想。 3. MapReduce进程一个完整的MapReduce程序在分布式运行时有三类实例进程： 1）MrAppMaster：负责整个程序的过程调度以及状态协调 2）MapTask：负责Map阶段的整个数据处理流程。即负责”分” 3）ReduceTask：负责Reduce阶段的整个数据处理流程。即负责”合” MapReduce编程规范用户编写程序分成三个部分：Mapper，Reducer 和 Driver 1）Mapper阶段 ​ (1) 用户自定义Mapper要继承自己的父类 ​ (2) Mapper的输入数据是KV对的形式（KV的类型可自定义） ​ (3) Mapper中的业务逻辑写在map()方法中 ​ (4) Mapper的输出数据是KV对的形式（KV的类型可自定义） ​ (5) map()方法（MapTask进程）对每一个调用一次 2）Reducer阶段 ​ (1) 用户自定义的Reducer要继承自己的父类 ​ (2) Reducer的输入数据类型对应Mapper的输出数据类型，也是KV ​ (3) Reducer的业务逻辑卸载reduce()方法中 ​ (4) ReduceTask进程对每一组相同k的组调用一次reduce()方法 3）Driver阶段 ​ 相当于YARN集群的客户端，用于提交我们整个程序到YARN集群，提交的是封装了MapReduce程序相关运行参数的job对象 4. YARN架构 4.1. 各个组件的作用ResourceManager 处理客户端请求 启动/监控ApplicationMaster 监控NodeManager 资源分配与调度 NodeManager 单个节点上的资源管理 处理来自ResourceManager的命令 处理来自ApplicationMaster的命令 ApplicationMaster 数据切分 为应用程序申请资源，并分配给内部任务 任务监控与容错 Container 对任务运行环境的抽象，封装了CPU内存等多维资源以及环境变量、启动命令等任务运 行相关的信息. Map、Reduce等任务都是运行在Container上 类比： ResourceManager：部门经理。整个集群资源的老大。 NodeManager：项目组长。单个节点资源的老大。 ApplicationMaster：项目经理。单个Job资源的老大。 Container：组员。封闭资源信息。 4.2. ResourceManagerResourceManager是全局的资源管理器，整个集群只有一个，负责集群资源的统一管理和调度分配。 4.2.1. ResourceManager的功能 处理客户端请求 启动/监控ApplicationMaster 监控NodeManager 资源分配与调度 4.3. NodeManagerNodeManager可以有多个节点，负责单个节点资源管理和使用 NodeManager一般与DataNode在同一个节点 NodeManager的功能： 单个节点上的资源管理 处理来自ResourceManager的命令 处理来自ApplicationMaster的命令 NodeManager管理抽象容器Container，这些容器代表着可供一个特定应用程序使用的针对每个节点的资源 NodeManager定时地向RM汇报节点上的资源使用情况和各个Container的运行状态 4.4. Application Master管理一个在YARN内运行的应用程序的每个实例 功能： 数据切分 为应用程序申请资源，并分配给内部任务 任务监控与容错 负责协调来自ResourceManager的资源，开通过NodeManager监视容器的执行和资源使用（CPU,内存等的资源分配） 4.5. ContainerYARN中的资源抽象，封装某个节点上多维度资源，如内存，CPU,自盘，网络等，当AM想RM申请资源时，RM向AM返回的资源便是用Container表示的 YARN 会为每个任务分配一个Container,且该任务只能使用Container中描述的资源 5. YARN 资源管理资源调度和资源隔离是YARN作为一个资源管理系统，最重要和最基础的两个功能。 资源调度由ResourceManager完成 资源隔离由各个NodeManager实现 ResourceManager将某个NodeManager上资源分配给任务（这就是所谓的”资源调度”）后，NodeManager需按照要求为任务提供相应的资源，甚至保证这些资源应具有独占性，为任务运行提供基础的保证，这就是所谓的资源隔离。 当谈及到资源时，我们通常指内存，CPU和IO三种资源。Hadoop YARN同时支持内存和CPU两种资源的调度。 内存资源的多少会会决定任务的生死，如果内存不够，任务可能运行失败；相比之下，CPU资源则不同，它只会决定任务运行的快慢，不会对生死产生影响。 YARN允许用户配置每个节点上可用的物理内存资源，注意，这里是“可用的”，因为一个节点上的内存会被若干个服务共享，比如一部分给YARN，一部分给HDFS，一部分给HBase等，YARN配置的知识自己可以使用的，配置参数如下： yarn.nodemanager.resource.memory-mb表示该节点上YARN可使用的物理内存总量，默认是8192（MB），注意，如果你的节点内存资源不够8GB,则需要调减这个值，而YARN不会只能的探测节点的物理内存总量。 yarn.nodemanager.vmem-pmem-ratio任务每使用1MB物理内存，最多可使用虚拟内存量，默认是2.1 yarn.nodemanager.pmem-check-enabled是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true。 yarn.nodemanager.vmem-check-enabled是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true。 yarn.scheduler.minimum-allocation-mb单个任务可申请的最少物理内存量，默认是1024(MB),如果一个任务申请的物理内存量少于该值，则该对应的值改为这个数。 yarn.scheduler.maximum-allocation-mb单个任务可申请的最多物理内存量，默认是8192（MB）. 目前的CPU被划分成虚拟CPU（CPU virtual Core）,这里的虚拟CPU是YARN自己引入的概念，初衷是，考虑到不同节点的CPU性能可能不同，每个CPU具有的计算能力也是不一样的，比如某个物理CPU的计算机能力可能是另外一个物理CPU的2倍，这时候，你可以通过为第一个物理CPU多配置几个虚拟CPU弥补这种差异。用户提交作业时，可硬指定没干过任务需要的虚拟CPU个数。在YARN中，CPU相关配置参数如下： yarn.nodemanager.resource.cpu-vcores表示该节点上YANR可使用的虚拟CPU个数，默认是8，注意，目前推荐将该为与物理CPU核数数目相同。如果你的节点CPU核数不够8个，则需要调减小这个值，而YARN不会智能的探测节点的物理CPU总数。 yarn.scheduler.minimum-allocation-vcore单个任务可申请的最小虚拟CPU个数，默认是1，如果一个任务申请的CPU个数少于该数，则该对应的值改为这个数。 yarn.scheduler.maximum-allocation-vcores 单个任务可申请的最大虚拟CPU个数 6. MapReduce架构离线计算框架 MapReduce 将计算过程分为两个阶段，map和reduce map 阶段并行处理输入数据 reduce 阶段对map 结果进行汇总。 shuffle 连接map 和Reduce 两个阶段 map task 将数据写到本地磁盘 reduce task 从每个map TASK 上读取一份数据 仅适合 离线批处理 具有很好的容错性和扩展性 适合简单的批处理任务 缺点明显 启动开销大，过多使用磁盘导致效率底下等。 7. MapReduce On YARN 客户端向YARN中提交应用程序/作业给ResourceManager，其中包括ApplicaitonMaster程序、启动ApplicationMaster的命令、用户程序等； ResourceManager为作业分配第一个Container，并与对应的NodeManager通信，要求它在这个Containter中启动该作业的ApplicationMaster（App Mstr）； ApplicationMaster首先向ResourceManager注册，这样用户可以直接通过ResourceManager查询作业的运行状态；然后它将为各个任务申请资源并监控任务的运行状态，直到运行结束。即重复步骤4-7； ApplicationMaster采用轮询的方式通过RPC请求向ResourceManager申请和领取资源； 一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务； 各个NodeManager分别在Container中启动Map任务（Map Task），或者启动Reduce任务（Reduce Task），或者都启动（如果本机资源足够的话）； 各个任务通过RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicaitonMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务； 在作业运行过程中，用户可随时通过RPC向ApplicationMaster查询作业当前运行状态； 作业完成后，ApplicationMaster向ResourceManager注销并关闭自己； 8. MapReduce各个阶段MapReduce将计算过程分为两个阶段：Map（分）和Reduce（合） 1）Map阶段并行处理输入数据 2）Reduce阶段对Map结果进行汇总]]></content>
      <categories>
        <category>Hadoop</category>
        <category>MapReduce</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS 快照管理]]></title>
    <url>%2F2019%2F02%2F25%2Fhadoop%2FHDFS-%E5%BF%AB%E7%85%A7%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[1. HDFS快照管理快照相当于对目录做一个备份。并不会立即复制所有文件，而是记录文件变化 1.1. 开启快照功能开启指定目录的快照功能 12$ hdfs dfsadmin -allowSnapshot /inputAllowing snaphot on /input succeeded 1.2. 创建快照-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;] 创建一个快照 12$ hdfs dfs -createSnapshot /inputCreated snapshot /input/.snapshot/s20190225-152050.714 默认是看不到隐藏文件的 123456789101112$ hdfs dfs -ls -R /input # 看不到.snapshot目录，实际是存在的-rw-r--r-- 3 root supergroup 280 2019-02-25 14:55 /input/1.txt-rw-r--r-- 3 root supergroup 58 2019-02-25 15:17 /input/2.txt-rw-r--r-- 3 root supergroup 4 2019-02-25 15:17 /input/3.txt-rw-r--r-- 3 root supergroup 4 2019-02-25 15:18 /input/4.txt$ hdfs dfs -ls -R /input/.snapshot # 直接查看drwxr-xr-x - root supergroup 0 2019-02-25 15:20 /input/.snapshot/s20190225-152050.714-rw-r--r-- 3 root supergroup 280 2019-02-25 14:55 /input/.snapshot/s20190225-152050.714/1.txt-rw-r--r-- 3 root supergroup 58 2019-02-25 15:17 /input/.snapshot/s20190225-152050.714/2.txt-rw-r--r-- 3 root supergroup 4 2019-02-25 15:17 /input/.snapshot/s20190225-152050.714/3.txt-rw-r--r-- 3 root supergroup 4 2019-02-25 15:18 /input/.snapshot/s20190225-152050.714/4.txt 删除一个文件 1hdfs dfs -rm -r -f /input/1.txt 再创建一个快照，可指定名称 12$ hdfs dfs -createSnapshot /input snapshot2Created snapshot /input/.snapshot/snapshot2 1.3. 列出有快照的目录列出当前用户有快照的目录 12$ hdfs lsSnapshottableDirdrwxr-xr-x 0 root supergroup 0 2019-02-25 15:20 1 65536 /input 1.4. 重命名快照-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt; 12hdfs dfs -renameSnapshot /input snapshot2 snapshot3 # snapshot2重命名为snapshot3 hdfs dfs -renameSnapshot /input snapshot3 snapshot2 # 恢复修改 1.5. 比较快照不同以及目录变化snapshotDiff用法：hdfs snapshotDiff &lt;snapshotDir&gt; &lt;from&gt; &lt;to&gt; &lt;snapshotDir&gt;：有快照的目录 &lt;from/to&gt;： 可以是 . ，表示该目录的当前状态 也可以是.snapshot/snapshot_name，代表该目录的某个快照。其中.snapshot可以省略，简写为snapshot_name 比较两个快照的不同 123456789$ hdfs snapshotDiff /input s20190225-152050.714 snapshot2Difference between snapshot s20190225-152050.714 and snapshot snapshot2 under directory /input:M . # 当前目录发生变化- ./1.txt # 后者比前者少了1.txt$ hdfs snapshotDiff /input snapshot2 s20190225-152050.714 # 参数的一下Difference between snapshot snapshot2 and snapshot s20190225-152050.714 under directory /input:M .+ ./1.txt # 后者比前者多了1.txt 比较当前状态与快照的不同 12345678910111213141516171819202122232425$ hdfs dfs -rm -r -f /input/2.txt$ hdfs dfs -put input/5.txt /input/$ hdfs dfs -rm -r -f /input/3.txt$ hdfs dfs -put input/3.txt /input$ hdfs dfs -appendToFile - /input/4.txthello world^D # CTRL+D$ hdfs snapshotDiff /input snapshot2 . # 后者比前者Difference between snapshot snapshot2 and current directory under directory /input:M .+ ./3.txt+ ./5.txt- ./2.txt- ./3.txtM ./4.txt$ hdfs snapshotDiff /input . snapshot2 # 后者比前者Difference between current directory and snapshot snapshot2 under directory /input:M .- ./3.txt- ./5.txt+ ./2.txt+ ./3.txtM ./4.txt 1.6. 恢复快照这些的恢复快照不能一次性回到原来的状态，只能手动拷贝。将快照目录复制出来即可 12345$ hdfs dfs -cp /input/.snapshot/snapshot2 / # 复制snapshot2目录$ hdfs dfs -ls -R /snapshot2-rw-r--r-- 3 root supergroup 58 2019-02-25 16:13 /snapshot2/2.txt-rw-r--r-- 3 root supergroup 4 2019-02-25 16:13 /snapshot2/3.txt-rw-r--r-- 3 root supergroup 4 2019-02-25 16:13 /snapshot2/4.txt 1.7. 删除快照-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt; 12hdfs dfs -deleteSnapshot /input s20190225-152050.714hdfs dfs -deleteSnapshot /input snapshot2 1.8. 禁用快照hdfs dfsadmin [-disallowSnapshot &lt;snapshotDir&gt;] 12345678910$ hdfs dfs -createSnapshot /input snapshot2 # 先创建一个快照Created snapshot /input/.snapshot/snapshot2$ hdfs dfsadmin -disallowSnapshot /input # 尝试禁用，失败，如果当前目录有快照，则无法禁止disallowSnapshot: The directory /input has snapshot(s). Please redo the operation after removing all the snapshots.$ hdfs dfs -deleteSnapshot /input snapshot2 # 先删除该目录的所有快照$ hdfs dfsadmin -disallowSnapshot /input # 再禁用Disallowing snaphot on /input succeeded]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS 回收站]]></title>
    <url>%2F2019%2F02%2F25%2Fhadoop%2FHDFS-%E5%9B%9E%E6%94%B6%E7%AB%99%2F</url>
    <content type="text"><![CDATA[1. HDFS 回收站回收站是HDFS2.X的新特性。与windows回收站一样，存在的意义是防止用户误删一些数据。假设误删了一些数据，还可以从回收站将数据恢复 HDFS默认是将回收站功能关闭的，因为实际生产环境中，是很少执行删除操作的，可能是半年删一次、一年删一次。 1.1. 开启回收站编辑 etc/hadoop/core-site.xml，并分发给集群 123456789101112&lt;property&gt; &lt;!-- 配置垃圾回收时间为1分钟，单位是minute。默认值为0，表示禁用回收站功能 --&gt; &lt;name&gt;fs.trash.interval&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 检查垃回收时间是否达到fs.trash.interval的时间间隔 默认值为0，表示与fs.trash.interval时间相等 要求 fs.trash.checkpoint.interval &lt;= fs.trash.interval --&gt; &lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt; &lt;value&gt;0&lt;/value&gt;&lt;/property&gt; 1.2. 删除文件测试不必重启HDFS。直接删除一个文件。因为当前client的用户是root，所以HDFS会将文件放到用户家目录下的一个.Trash目录下 123$ hdfs dfs -rm -r -f /input/1.txt19/02/25 14:29:25 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 1 minutes, Emptier interval = 0 minutes.Moved: 'hdfs://hadoop1:9000/input/1.txt' to trash at: hdfs://hadoop1:9000/user/root/.Trash/Current 查看用户目录 12345# hdfs dfs -ls -R /user/rootdrwx------ - root supergroup 0 2019-02-25 14:29 /user/root/.Trashdrwx------ - root supergroup 0 2019-02-25 14:29 /user/root/.Trash/Currentdrwx------ - root supergroup 0 2019-02-25 14:29 /user/root/.Trash/Current/input-rw-r--r-- 3 root supergroup 280 2019-02-25 13:40 /user/root/.Trash/Current/input/1.txt 通过WebUI查看，禁止访问用户目录。因为使用时WebUI，用户默认是dr.who。一般文件和目录对于other用户都有读权限，但是用户目录禁止other用户访问 一种方法是修改user目录的mode属性，但不是很妥当。可以修改 etc/hadoop/core-site.xml 12345&lt;property&gt; &lt;!-- WebUI的用户 --&gt; &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt; &lt;value&gt;root&lt;/value&gt;&lt;/property&gt; 重启HDFS 12stop-dfs.shstart-dfs.sh 可以通过WebUI查看了 一分钟后，回收站中Current中的数据就消失了 但是数据并不是完全消失，而是会在.Trash目录下生成另一个名为yyyyMMddHHmmss的目录，将数据移动到该目录下。 经过一段时间后，yyyyMMddHHmmss目录也会消失。 1.3. 清空回收站执行以后命令，会立即清空.Trash/Current目录，将文件移动到yyyyMMddHHmmss目录下，经过一段时间，yyyyMMddHHmmss目录被HDFS消除。 1hdfs dfs -expunge]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS 小文件存储]]></title>
    <url>%2F2019%2F02%2F25%2Fhadoop%2FHDFS-%E5%B0%8F%E6%96%87%E4%BB%B6%E5%AD%98%E5%82%A8%2F</url>
    <content type="text"><![CDATA[1. HDFS 存储小文件的弊端HDFS中每个文件、目录、数据块的元数据存储大约占150字节，不管文件是大是小，因此HDFS存储小文件会非常低效。存储大量小文件会耗尽NameNode中的大部分内存。 但注意，存储小文件所需要的磁盘容量和数据块的大小无关。例如，一个1MB的文件设置为128MB的块存储，实际使用的是1MB的磁盘空间，而不是128MB。 2. 优化小文件存储2.1. 解决方法1：小文件归档HDFS存档文件（HAR文件），是HDFS2.X的新特性，是一个更高效的文件存档工具，它将文件存入HDFS块，在减少NameNode内存使用的同时，允许对文件进行透明的访问。 具体说来，HDFS存档文件对内还是个一个独立文件，对NameNode而言却是个整体，减少了 NameNode的内存。 例如：将1.txt、2.txt、3.txt归档成一个har文件。对于NameNode而言只有一个har文件。但是client还是可以访问其中一个单独的文件 2.1.1. 启动HDFS和YARN归档是一个MapReduce程序，如果已经配置了MapReduce运行在YARN上，就要启动YARN 12start-dfs.shstart-yarn.sh 2.1.2. 执行归档操作12# 将/input作为输入目录，/output作为输出目录，打包的文件名为input.har（强制要求以har为后缀）hadoop archive -archiveName input.har -p /input /output 查看结果目录。用har协议才能查看har归档的内容 12345678910111213141516$ hdfs dfs -ls /output # 生成一个input.har目录，就是归档Found 1 itemsdrwxr-xr-x - root supergroup 0 2019-02-25 13:41 /output/input.har$ hdfs dfs -ls /output/input.har # 查看归档中的内容，看不到原来的各个文件Found 4 items-rw-r--r-- 3 root supergroup 0 2019-02-25 13:41 /output/input.har/_SUCCESS-rw-r--r-- 5 root supergroup 252 2019-02-25 13:41 /output/input.har/_index-rw-r--r-- 5 root supergroup 23 2019-02-25 13:41 /output/input.har/_masterindex-rw-r--r-- 3 root supergroup 342 2019-02-25 13:41 /output/input.har/part-0$ hdfs dfs -ls har:///output/input.har # 必须用har协议，才能看到input.har归档中的内容Found 3 items-rw-r--r-- 3 root supergroup 280 2019-02-25 13:40 har:///output/input.har/1.txt-rw-r--r-- 3 root supergroup 58 2019-02-25 13:40 har:///output/input.har/2.txt-rw-r--r-- 3 root supergroup 4 2019-02-25 13:40 har:///output/input.har/3.txt]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS 集群间数据拷贝]]></title>
    <url>%2F2019%2F02%2F25%2Fhadoop%2FHDFS-%E9%9B%86%E7%BE%A4%E9%97%B4%E6%95%B0%E6%8D%AE%E6%8B%B7%E8%B4%9D%2F</url>
    <content type="text"><![CDATA[1. 集群间数据拷贝集群间数据拷贝是HDFS2.X的新特性 采用distcp命令实现两个Hadoop集群之间的递归数据复制 1hadoop distcp hdfs://hadoop1:9000/1.txt hdfs://hadoop2:9000/2.txt]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS DataNode多目录]]></title>
    <url>%2F2019%2F02%2F25%2Fhadoop%2FHDFS-DataNode%E5%A4%9A%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[1. DataNode多目录配置DataNode也可以配置成多个目录。多目录不是以副本的形式存储数据，而是以分片的形式存放。各具目录共同组成完整的DataNode数据 1.1. 编辑 hdfs-site.xml添加以下配置 1234&lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data1,file:/usr/local/hadoop/tmp/dfs/data2&lt;/value&gt;&lt;/property&gt; 再分发 hdfs-site.xml 1.2. 重新格式化NameNode删除各个节点的logs和tmp目录，关闭各个服务，格式化NameNode 1hdfs namenode -format 1.3. 启动HDFS一开始DataNode还没有数据 12345678$ tree tmp/dfs/tmp/dfs/└── name └── current ├── fsimage_0000000000000000000 ├── fsimage_0000000000000000000.md5 ├── seen_txid └── VERSION 启动HDFS 1start-dfs.sh 再查看DataNode数据，看到生成data1和data2 12345678910111213141516171819202122232425262728293031$ tree -a tmp/dfs/tmp/dfs/├── data1│ ├── current│ │ ├── BP-441466473-192.168.57.101-1551067074181│ │ │ ├── current│ │ │ │ ├── finalized│ │ │ │ ├── rbw│ │ │ │ └── VERSION│ │ │ ├── dncp_block_verification.log.curr│ │ │ └── tmp│ │ └── VERSION│ └── in_use.lock├── data2│ ├── current│ │ ├── BP-441466473-192.168.57.101-1551067074181│ │ │ ├── current│ │ │ │ ├── finalized│ │ │ │ ├── rbw│ │ │ │ └── VERSION│ │ │ └── tmp│ │ └── VERSION│ └── in_use.lock└── name ├── current │ ├── edits_inprogress_0000000000000000001 │ ├── fsimage_0000000000000000000 │ ├── fsimage_0000000000000000000.md5 │ ├── seen_txid │ └── VERSION └── in_use.lock 1.4. 上传文件测试上传文件 1hdfs dfs -put 1.txt / 查看DataNode数据，看到数据只是存放到data1 123456789101112131415161718192021222324252627282930313233343536373839$ tree -a tmp/dfs/tmp/dfs/├── data1│ ├── current│ │ ├── BP-441466473-192.168.57.101-1551067074181│ │ │ ├── current│ │ │ │ ├── finalized│ │ │ │ │ └── subdir0│ │ │ │ │ └── subdir0│ │ │ │ │ ├── blk_1073741825 # 数据存放到data1│ │ │ │ │ └── blk_1073741825_1001.meta│ │ │ │ ├── rbw│ │ │ │ └── VERSION│ │ │ ├── dncp_block_verification.log.curr│ │ │ ├── dncp_block_verification.log.prev│ │ │ └── tmp│ │ └── VERSION│ └── in_use.lock├── data2│ ├── current│ │ ├── BP-441466473-192.168.57.101-1551067074181│ │ │ ├── current│ │ │ │ ├── finalized│ │ │ │ ├── rbw│ │ │ │ └── VERSION│ │ │ └── tmp│ │ └── VERSION│ └── in_use.lock└── name ├── current │ ├── edits_0000000000000000001-0000000000000000002 │ ├── edits_inprogress_0000000000000000003 │ ├── fsimage_0000000000000000000 │ ├── fsimage_0000000000000000000.md5 │ ├── fsimage_0000000000000000002 │ ├── fsimage_0000000000000000002.md5 │ ├── seen_txid │ └── VERSION └── in_use.lock 再上传一个文件 1hdfs dfs -put 2.txt / 查看DataNode数据，看到数据只是存放到data2 12345678910111213141516171819202122232425262728293031323334353637383940414243$ tree -a tmp/dfs/tmp/dfs/├── data1│ ├── current│ │ ├── BP-441466473-192.168.57.101-1551067074181│ │ │ ├── current│ │ │ │ ├── finalized│ │ │ │ │ └── subdir0│ │ │ │ │ └── subdir0│ │ │ │ │ ├── blk_1073741825│ │ │ │ │ └── blk_1073741825_1001.meta│ │ │ │ ├── rbw│ │ │ │ └── VERSION│ │ │ ├── dncp_block_verification.log.curr│ │ │ ├── dncp_block_verification.log.prev│ │ │ └── tmp│ │ └── VERSION│ └── in_use.lock├── data2│ ├── current│ │ ├── BP-441466473-192.168.57.101-1551067074181│ │ │ ├── current│ │ │ │ ├── finalized│ │ │ │ │ └── subdir0│ │ │ │ │ └── subdir0│ │ │ │ │ ├── blk_1073741826 # 数据存放到data2│ │ │ │ │ └── blk_1073741826_1002.meta│ │ │ │ ├── rbw│ │ │ │ └── VERSION│ │ │ └── tmp│ │ └── VERSION│ └── in_use.lock└── name ├── current │ ├── edits_0000000000000000001-0000000000000000002 │ ├── edits_inprogress_0000000000000000003 │ ├── fsimage_0000000000000000000 │ ├── fsimage_0000000000000000000.md5 │ ├── fsimage_0000000000000000002 │ ├── fsimage_0000000000000000002.md5 │ ├── seen_txid │ └── VERSION └── in_use.lock]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS DataNode动态添加与删除]]></title>
    <url>%2F2019%2F02%2F24%2Fhadoop%2FHDFS-DataNode%E5%8A%A8%E6%80%81%E6%B7%BB%E5%8A%A0%E4%B8%8E%E5%88%A0%E9%99%A4%2F</url>
    <content type="text"><![CDATA[1. 动态添加DataNode在不关闭HDFS和YARN的情况下，动态添加DataNode 1.1. 先准备一台服务器hadoop4 hostname hadoop1 hadoop2 hadoop3 hadoop4 ip 192.168.57.101 192.168.57.102 192.168.57.103 192.168.57.104 system ubuntu16.04 ubuntu16.04 ubuntu16.04 ubuntu16.04 HDFS NameNode DataNode DataNode SecondaryNameNode DataNode DataNode YARN NodeManager ResourceManager NodeManager NodeManager NodeManager hadoop4配置静态ip、hostname 修改集群脚本：再多循环一个节点 各节点hosts添加hadoop4的记录 NameNode和ResourceManager配置SSH免密码登录到hadoop4 复制集群jdk、Hadoop到hadoop4 hadoop4删除hadoop同步得到的logs和data目录，一定要删除 hadoop4配置环境变量 查看web ui，现在只有3个DataNode 1.2. 动态添加DataNode启动服务。因为core-site.xml中已经指明了NameNode的地址，所以启动DataNode后，会直接向NameNode注册 1hadoop-daemon.sh start datanode 查看进程。如果发现datanode没有启动，很可能是该节点原本的logs和tmp目录忘了删除了 123# jps2644 Jps2573 DataNode 查看WebUI，可以看到DataNode添加成功 再启动nodemanager。因为yarn-site.xml已经指定了ResourceManager，所以启动nodemanager时会向其注册 1yarn-daemon.sh start nodemanager ResourceManager查看nodemanager 1yarn node -list 1.3. 集群的再平衡动态添加了DataNode，可能其它DataNode数据很多，但是新的DataNode没有数据，导致不平衡。此时NameNode执行以下命令，实现数据块的再平衡 1start-balancer.sh 1.4. 测试上传文件在新节点上执行上传文件 1hdfs dfs -put 2.txt / 可以看到数据有3个副本。因为当前client是在datanode节点上执行的，根据就近原则，一定有一个副本是放在当前节点上 1.5. 编辑slaves在slaves中添加节点，以便下次HDFS重启能一起启动hadoop4。 1234hadoop1hadoop2hadoop3hadoop4 2. 动态解除DataNode2.1. 通过白名单解除DataNodeNameNode创建etc/hadoop/dfs.hosts，文件名无所谓，但是最好取官方默认的名称。写入以下内容，一行一个节点，只有白单名中的DataNode才有权连接NameNode，所以不添加hadoop4节点。注意不能有多余的空行，每行不能有多余的空格 123hadoop1hadoop2hadoop3 在NameNode的hdfs-site.xml中增加dfs.hosts配置 12345&lt;property&gt; &lt;!-- 指定白名单文件的路径 --&gt; &lt;name&gt;dfs.hosts&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/etc/hadoop/dfs.hosts&lt;/value&gt;&lt;/property&gt; 分发hdfs-site.xml 1xsync etc/hadoop/hdfs-site.xml refreshNodes动态刷新dfs.hosts和dfs.hosts.exclude配置，无需重启NameNode 1hdfs dfsadmin -refreshNodes 刷新后，查看WebUI，hadoop4被删除了 hadoop4查看进程 123$ jps # DataNode被关闭了，只剩下NodeManager2721 NodeManager2907 Jps NameNode执行再平衡 1start-balancer.sh 一般解除节点是通过黑名单，而不是白名单，所以不推荐使用白名单的方式来解除节点 2.2. 恢复刚才被白名单解除的DataNodeNameNode编辑dfs.hosts，添加hadoop4 1234hadoop1hadoop2hadoop3hadoop4 NameNode刷新 1hdfs dfsadmin -refreshNodes hadoop4启动DataNode 1hadoop-daemon.sh stop datanode 查看web ui，看到hadoop4恢复了 2.3. 通过黑名单解除DataNode先去掉白单名 编辑hdfs-site.xml，删除dfs.hosts配置。分发hdfs-site.xml NameNode刷新hdfs dfsadmin -refreshNodes NameNode创建etc/hadoop/dfs.hosts.exclude，文件名无所谓，但是最好取官方默认的名称。每一行代表黑名单中的节点。现在要将hadoop4加入黑名单 1hadoop4 NameNode编辑hdfs-site.xml，添加dfs.hosts.exclude属性 12345&lt;property&gt; &lt;!-- 指定黑名单文件的路径 --&gt; &lt;name&gt;dfs.hosts.exclude&lt;/name&gt; &lt;value&gt;/usr/local/hadoop/etc/hadoop/dfs.hosts.exclude&lt;/value&gt;&lt;/property 分发 1xsync etc/hadoop/hdfs-site.xml NameNode刷新 1hdfs dfsadmin -refreshNodes 查看web ui，看到状态是Decommission In Progress，表示正在解除hadoop4。此时HDFS会把hadoop4中的数据块复制到其它DataNode上，hadoop4中的数据越多，该过程就越漫长 过一段时间后，状态变为Decommissioned，即正式解除 可以看到hadoop4将块副本拷贝给hadoop1 NameNode执行再平衡 1start-balancer.sh]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS NameNode多目录]]></title>
    <url>%2F2019%2F02%2F24%2Fhadoop%2FHDFS-NameNode%E5%A4%9A%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[1. NameNode多目录NameNode的数据目录可以配置成多个，且每个目录存放内容相同，相当于对数据做备份，增加了可靠性 1.1. 编辑hdfs-site.xml修改配置 1234&lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///$&#123;hadoop.tmp.dir&#125;/dfs/name1,file:///$&#123;hadoop.tmp.dir&#125;/dfs/name2&lt;/value&gt;&lt;/property&gt; 1.2. 重新格式化NameNode关闭服务 12stop-dfs.shstop-yarn.sh 所有节点删除数据和日志 12rm -rf /usr/local/hadoop/tmprm -rf /usr/local/hadoop/logs/ 格式化 1hdfs namenode -format 查看生成的目录 1234567891011121314$ tree /usr/local/hadoop/tmp/dfs//usr/local/hadoop/tmp/dfs/├── name1│ └── current│ ├── fsimage_0000000000000000000│ ├── fsimage_0000000000000000000.md5│ ├── seen_txid│ └── VERSION└── name2 # 与name1完全一致 └── current ├── fsimage_0000000000000000000 ├── fsimage_0000000000000000000.md5 ├── seen_txid └── VERSION 1.3. 启动HDFS并查看NameNode启动服务 1start-dfs.sh 上传文件 1hdfs dfs -put 1.txt / 查看目录树，name1与name2仍完全一致 12345678910111213141516171819202122232425262728293031323334353637383940$ tree -a /usr/local/hadoop/tmp/dfs/ /usr/local/hadoop/tmp/dfs/├── data│ ├── current│ │ ├── BP-774990081-192.168.57.101-1551014303666│ │ │ ├── current│ │ │ │ ├── finalized│ │ │ │ │ └── subdir0│ │ │ │ │ └── subdir0│ │ │ │ │ ├── blk_1073741825│ │ │ │ │ └── blk_1073741825_1001.meta│ │ │ │ ├── rbw│ │ │ │ └── VERSION│ │ │ ├── dncp_block_verification.log.curr│ │ │ ├── dncp_block_verification.log.prev│ │ │ └── tmp│ │ └── VERSION│ └── in_use.lock├── name1│ ├── current│ │ ├── edits_0000000000000000001-0000000000000000008│ │ ├── edits_inprogress_0000000000000000009│ │ ├── fsimage_0000000000000000000│ │ ├── fsimage_0000000000000000000.md5│ │ ├── fsimage_0000000000000000008│ │ ├── fsimage_0000000000000000008.md5│ │ ├── seen_txid│ │ └── VERSION│ └── in_use.lock└── name2 ├── current │ ├── edits_0000000000000000001-0000000000000000008 │ ├── edits_inprogress_0000000000000000009 │ ├── fsimage_0000000000000000000 │ ├── fsimage_0000000000000000000.md5 │ ├── fsimage_0000000000000000008 │ ├── fsimage_0000000000000000008.md5 │ ├── seen_txid │ └── VERSION └── in_use.lock]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS NameNode启动过程及安全模式]]></title>
    <url>%2F2019%2F02%2F24%2Fhadoop%2FHDFS-NameNode%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%E5%8F%8A%E5%AE%89%E5%85%A8%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1. NameNode启动过程NameNode启动过程： 读取fsimage和edits到内存中 执行edits的各项操作，更新内存中的元数据，存在内存中的元数据支持客户端的读操作 将内存中的元数据写到新的fsimage文件中 创建一个新的空的edits文件。同时启动HDFS的安全模式，监听DataNode的请求 启动DataNode 向NameNode注册 发送BlockReport。发送完之后，关闭HDFS的安全模式。 之后HDFS的所有操作都记录到新的edits中 注意：系统中的数据块的位置并不是甶NameNode维护的，而是以块列表的形式存储在DataNode中。在系统的正常操作期间，NameNode会在内存中保留所有块位置的映射信息。在安全模式下，各个DataNode会向 NameNode发送最新的块列表信息，NameNode了解到足够多的块位置信息之后，即可高效运行文件系统。 HDFS的安全模式（SafeMode）： 等待DataNode向NameNode发送BlockReport。在安全模式期间，客户端只能读取HDFS，但是不能写HDFS（创建文件夹、上传文件、删除文件 都不能操作）。 汇总所有的BlockReport。一开始NameNode加载完fsimage和edits时，就得知整个文件系统总共有多少个block（total blocks）。从不断接收到的BlockReport中，不断累计得到所有的DataNode上总共有多少block（reported blocks）。计算 reported blocks / total blocks，如果比值 &gt;= 99.9%，安全模式才会退出。而这里99.9%的块的要求是副本数满足最小副本数 dfs.replication.min（默认值为1）。 安全模式退出后，客户端就能对HDFS执行写操作了。 特殊情况：刚刚格式化完HDFS，因为还没有任何块，所以NameNode不会进入安全模式。 2. 验证安全模式的存在关闭HDFS所有服务，只启动namenode 1hadoop-daemon.sh start namenode 查看namenode的web ui，可以看到NameNode处于SafeMode。 total blocks有3个，接收到的reported blocks只有0个 只有当 reported blocks / total blocks &gt;= 0.9990，才会关闭SafeMode 启动datanode 1hadoop-daemon.sh start datanode 启动后，在短时间内再次查看namenode的web ui，可以看到NameNode仍处于SafeMode。total blocks有3个，接收到的reported blocks也有3个，所以 reported blocks / total blocks = 1 &gt;= 0.9990，已经触发了关闭SafeMode的条件。因此在30秒之后，NameNode将自动关闭SafeMode 经过30秒后，再次查看namenode的web ui 3. 为什么达到关闭SafeMode的条件，还要缓冲30秒？服务器求稳不求快，给30秒让HDFS稳定一下 4. 手动操作SafeMode1234hdfs dfsadmin -safemode get # 查看是否处于安全模式hdfs dfsadmin -safemode enter # 手动进入安全模式hdfs dfsadmin -safemode leave # 强制退出安全模式hdfs dfsadmin -safemode wait # 挂起shell，直到安全模式退出]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS NameNode元数据丢失故障处理]]></title>
    <url>%2F2019%2F02%2F24%2Fhadoop%2FHDFS-NameNode%E6%95%85%E9%9A%9C%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[1. NameNode元数据丢失故障处理NameNode故障后，可以采用如下两种方法恢复数据 1.1. 方法一：将SecondaryNameNode中数据拷贝到NameNode存储数据的目录先模拟NameNode出现故障 123456$ jps40528 Jps38147 NameNode38310 DataNode$ kill -9 38147 # 强制杀死NameNode$ rm -rf /usr/local/hadoop/tmp/dfs/name/* # 删除NameNode所有数据 拷贝SecondaryNameNode中数据到原NameNode存储数据目录 1scp -r root@hadoop3:/usr/local/hadoop/tmp/dfs/namesecondary/* /usr/local/hadoop/tmp/dfs/name/ 启动NameNode，此时数据已恢复 1hadoop-daemon.sh start namenode 方法二：使用-importCheckpoint选项启动NameNode守护进程，从而将SecondaryNameNode中数据拷贝到NameNode目录中先模拟NameNode出现故障 123456$ jps40528 Jps38147 NameNode38310 DataNode$ kill -9 38147 # 强制杀死NameNode$ rm -rf /usr/local/hadoop/tmp/dfs/name/* # 删除NameNode所有数据 若SecondaryNameNode不和NameNode在一个主机节点上，需要将SecondaryNameNode存储数据的目录拷贝到NameNode存储数据的平级目录。如果在同一节点上，两个组件的数据目录默认就是平级目录，不必操作 123$ scp -r root@hadoop3:/usr/local/hadoop/tmp/dfs/namesecondary /usr/local/hadoop/tmp/dfs$ ls /usr/local/hadoop/tmp/dfs # namesecondary与name此时位于同一目录data name namesecondary 再删除namesecondary中的in_use.lock文件 123$ ls /usr/local/hadoop/tmp/dfs/namesecondary/current in_use.lock$ rm -rf /usr/local/hadoop/tmp/dfs/namesecondary/in_use.lock # 删除 导入检查点数据（等待一会ctrl+c结束掉）。这一步不知道为什么出现错误FATAL namenode.NameNode: Failed to start namenode.java.lang.IllegalArgumentException: URI has an authority component，目前还没找到原因。 1hdfs namenode -importCheckpoint 再启动namenode 1hadoop-daemon.sh start namenode]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS 数据流过程]]></title>
    <url>%2F2019%2F02%2F24%2Fhadoop%2FHDFS-%E6%95%B0%E6%8D%AE%E6%B5%81%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1. HDFS 写数据流程1.1. 剖析文件写入 1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。 2）NameNode返回是否可以上传。 3）客户端请求第一个 Block上传到哪几个DataNode服务器上。 4）NameNode返回3个DataNode节点，分别为dn1、dn2、dn3。（NameNode优先选择 距离client近的、负载小的DataNode） 5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。 6）dn1、dn2、dn3逐级应答客户端。 7）客户端开始往dn1上传第一个Block（先从磁盘读取数据放到一个本地内存缓存），以Packet为单位，dn1收到一个Packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。 8）当一个Block传输完成之后，客户端再次请求NameNode上传第二个Block的服务器。（重复执行3-7步）。 1.2. 网络拓扑-节点距离计算在HDFS写数据的过程中，NameNode会选择距离待上传数据最近距离的DataNode接收数据。那么这个最近距离怎么计算呢？ 节点距离：两个节点到达最近的共同祖先的距离总和。 2. HDFS 读数据流程 1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。 2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。 3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以Packet为单位来做校验）。 4）客户端以Packet为单位接收，先在本地缓存，然后写入目标文件。]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS Java客户端]]></title>
    <url>%2F2019%2F02%2F24%2Fhadoop%2FHDFS-Java%E5%AE%A2%E6%88%B7%E7%AB%AF%2F</url>
    <content type="text"><![CDATA[1. Windows JavaClient环境搭建1.1. client添加HDFS节点的hostname记录编辑系统hosts文件，添加以下记录。client请求NameNode操作HDFS时，NameNode会让client根据DataNode的hostname去找DataNode，请求执行相应的操作。如果client不知道DataNode的hostname对应的IP，就无法操作。client可以直接用IP访问NameNode，所以NameNode的hostname记录不一定要有，但是DataNode一定要有 123192.168.57.101 hadoop1192.168.57.102 hadoop2192.168.57.103 hadoop3 1.2. 导入依赖添加hadoop-common、hadoop-client、hadoop-hdfs三个依赖，版本与服务器的Hadoop要对应。 1234567891011121314151617181920212223242526&lt;properties&gt; &lt;hadoop.version&gt;2.6.5&lt;/hadoop.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.3. 添加HDFS配置文件无论是HDFS Shell还是JavaClient，执行操作时都会读取默认配置文件（core-default.xml、hdfs-default.xml）和自定义配置文件（core-site.xml和hdfs-site.xml） 默认配置文件已经存在于依赖的jar包中。所以，只需要将Hadoop服务器的HDFS自定义配置文件（core-site.xml和hdfs-site.xml）复制到 src/main/resources目录下 1.4. 解决HDFS用户权限问题客户端在操作HDFS时，默认是以执行客户端的系统用户作为操作HDFS的用户 Linux中，root用户执行HDFS Shell时，操作HDFS的用户就是root Windows JavaClient执行操作时，操作的用户就是windows当前用户，如Administrator 而HDFS的文件系统和Linux文件系统一样，是有权限的，所以客户端不一定有足够的权限来执行相应的操作。 1234567891011$ hdfs dfs -ls -d / # 查看根目录的ownership和modedrwxr-xr-x - root root 0 2019-02-24 00:43 /$ hdfs dfs -ls -R / # 查看所有文件的ownership和mode-rw-r--r-- 10 root root 115 2019-02-23 23:42 /1.txt-rw-r--r-- 3 root root 3 2019-02-23 23:35 /2.txt-rw-r--r-- 3 root root 3 2019-02-23 23:35 /3.txt-rw-r--r-- 3 root root 199635269 2019-02-23 23:54 /hadoop-2.6.5.tar.gzdrwxr-xr-x - root root 0 2019-02-23 21:30 /input-rwxr-xr-x 3 root root 280 2019-02-23 21:30 /input/1.txt-rwxr-xr-x 3 root root 58 2019-02-23 21:30 /input/2.txt 1.4.1. 测试运行 JavaAPI1234567891011121314151617181920212223242526public class HdfsClientDemo &#123; public static final String HDFS_PATH = "hdfs://hadoop1:9000"; FileSystem fs = null; @Before public void setUp() throws Exception &#123; Configuration conf = new Configuration(); fs = FileSystem.get(new URI(HDFS_PATH), conf); &#125; @Test public void testRead() throws Exception &#123; FileStatus fileStatus = fs.getFileStatus(new Path("/")); System.out.println(fileStatus); &#125; @Test public void testWrite() throws Exception &#123; fs.mkdirs(new Path("/demo")); &#125; @After public void tearDown() throws IOException &#123; fs.close(); &#125;&#125; 运行 testRead，程序正常执行。运行 testWrite，程序报错： 1org.apache.hadoop.security.AccessControlException: Permission denied: user=Administrator, access=WRITE, inode="/":root:root:drwxr-xr-x 执行JavaClient的是windows用户Administrator，而HDFS根目录的ownership是root:root，mode是drwxr-xr-x。其它用户有读权限，所以testRead能正常运行。但是其它用户没有写权限，所以执行testWrite就失败了。 1.4.2. 解决方式1：修改所有HDFS文件的所有者或mode（不推荐）修改HDFS文件的所有者为运行client的windows用户，这样client就能执行写操作了 1hdfs dfs -chown -R Administrator / 也可以修改HDFS文件的mode，为所有用户添加写权限，也可以直接设置为777，这样所有用户都能执行任何操作了 1hdfs dfs -chmod -R +w / # 为所有用户添加写权限 因为涉及文件信息的修改，所以不推荐以上方式 1.4.3. 解决方式2：编辑hdfs.site.xml，不对用户权限进行检测（不推荐）服务器端编辑hdfs.site.xml，添加以下内容，再给javaclient同步一份。 12345&lt;property&gt; &lt;!-- 不检查用户权限。任何用户可以对HDFS做任何操作 --&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt; 不检查用户权限。任何用户可以对HDFS做任何操作，不安全，所以不推荐这样做。 1.4.4. 解决方式3：client配置HADOOP_USER_NAME环境变量（推荐）client配置HADOOP_USER_NAME环境变量之后，操作HDFS的用户不再是运行client的系统用户了，而是${HADOOP_USER_NAME}。因为现在HDFS上的文件所有者是root，所以配置HADOOP_USER_NAME的值为root。 方式一：添加环境变量 方式二：在JavaClient中添加环境变量 1234567@Beforepublic void setUp() throws Exception &#123; // 添加环境变量 System.setProperty("HADOOP_USER_NAME", "root"); Configuration conf = new Configuration(); fs = FileSystem.get(new URI(HDFS_PATH), conf);&#125; 1.4.5. 解决方式4：JavaAPI指定用户名123456@Beforepublic void setUp() throws Exception &#123; Configuration conf = new Configuration(); // 第3个参数是操作HDFS的用户名 fs = FileSystem.get(new URI(HDFS_PATH), conf, "root");&#125; 1.5. 配置hadoop-common-bin运行以下测试程序，第一条语句会出现错误 java.io.IOException: (null) entry in command string: null chmod 0644 1234567@Testpublic void testCopyToLocalFile() throws IOException &#123; // 默认useRawLocalFileSystem为false，执行以下语句报错: (null) entry in command string: null chmod 0644 fs.copyToLocalFile(new Path("/1.txt"), new Path("C:/2.txt")); // 第4个参数是useRawLocalFileSystem，设置为true，才能成功下载到windows磁盘 // fs.copyToLocalFile(false, new Path("/1.txt"), new Path("C:/2.txt"), true);&#125; hadoop-common-bin是hadoop在windows环境下编译的，用来在windows下运行hadoop程序的工具。下载大版本号相近的即可：https://github.com/amihalik/hadoop-common-2.6.0-bin 解压后的目录结构如下： 1234567891011121314.├── bin │ ├── hadoop.dll│ ├── hadoop.exp│ ├── hadoop.iobj│ ├── hadoop.ipdb│ ├── hadoop.lib│ ├── hadoop.pdb│ ├── libwinutils.lib│ ├── winutils.exe│ ├── winutils.iobj│ ├── winutils.ipdb│ └── winutils.pdb└── hadoop.dll 将该目录添加到环境变量中，取名HADOOP_HOME 将bin目录添加到PATH中，因为执行时用到 winutils.exe 1%HADOOP_HOME%\bin 重启IDE，使IDE能读取到新的环境变量。再次运行testCopyToLocalFile，可以看到useRawLocalFileSystem为false也能成功将HDFS的文件下载到windows本地磁盘。 2. HDFS JavaAPI2.1. 连接HDFS连接HDFS只要指定NameNode的地址即可。 方式1：在core-site.xml中预先定义好NameNode的地址fs.defaultFS，或者直接复制服务器端的core-site.xml 123456@Beforepublic void setUp() throws Exception &#123; // Configuration在创建时，会到classpath读取HDFS相关配置文件（core-site.xml、hdfs-site.xml等） Configuration conf = new Configuration(); fs = FileSystem.get(conf);&#125; 方式2：通过Configuration的API设置fs.defaultFS 123456@Beforepublic void setUp() throws Exception &#123; Configuration conf = new Configuration(); conf.set("fs.defaultFS", "hdfs://hadoop1:9000"); fs = FileSystem.get(conf);&#125; 方式3：通过FileSystem.get(URI uri, Configuration conf)得到fs对象，第1个参数是HDFS的URI 123456public static final String HDFS_PATH = "hdfs://hadoop1:9000";@Beforepublic void setUp() throws Exception &#123; Configuration conf = new Configuration(); fs = FileSystem.get(new URI(HDFS_PATH), conf);&#125; 无论是哪种方式连接，程序结束时都要记得关闭连接 1234@Afterpublic void tearDown() throws IOException &#123; fs.close();&#125; 2.2. mkdirs 创建目录1234567891011121314@Testpublic void testMkdirs() throws Exception &#123; /* 方法：mkdirs 参数： f:指定要创建的文件夹路径，可以为相对路径。 permission: 指定创建文件的权限，默认755。 返回值：如果创建成功则返回true；否则返回false。 */ // 创建目录 fs.mkdirs(new Path("/dirA")); // 支持递归创建目录 fs.mkdirs(new Path("/dir1/dir2/dir3"));&#125; 2.3. createNewFile 创建空文件12345678910111213141516171819@Testpublic void testCreateNewFile() throws IOException &#123; /* 方法: createNewFile 参数: f:指定要创建文件的路径，可以为相对路径 返回值: 如果创建成功返回true。否则返回false 在HDFS上创建空文件 若文件存在，则返回false 若文件不存在，则创建并返回true 若文件目录不存在，自动创建目录，再创建文件 */ boolean result = fs.createNewFile(new Path("/1.txt")); System.out.println(result); // 若d1目录不存在，自动创建d1目录，再创建文件 // fs.createNewFile(new Path("/d1/demo.txt"));&#125; 2.4. append 追加数据123456789101112131415161718@Testpublic void testAppend() throws IOException &#123; /* 方法: append 参数： f:指定要写出文件的路径，可以为相对路径。 bufferSize: 缓冲区大小 返回值：如果创建成功获得FSDataOutputStream输出流，否则出现异常信息 错误: could only be replicated to 0 nodes instead of minReplication (=1). 客户端无法与Datanode进行通信。因为客户端为Datanode收到的IP是内部IP而不是公共IP */ // 向文件追加数据 FSDataOutputStream fos = fs.append(new Path("/1.txt")); fos.write("HelloWorld\n".getBytes()); fos.close();&#125; 2.5. copyFromLocalFile 从本地上传文件12345678910111213@Testpublic void testCopyFromLocalFile() throws IOException &#123; /* 方法：copyFromLocalFile 参数： delSrc:是否删除本地文件，默认true。 overwrite:当目标文件存在的时候，是否覆盖，默认true。 srcs/src:本地文件，可以指定为数组或者单个文件。 dst:集群存储文件。 返回值：无，如果操作失败，会产生异常信息。 */ fs.copyFromLocalFile(new Path("C:/1.txt"), new Path("/d1/1.txt"));&#125; 2.6. copyToLocalFile 下载HDFS文件到本地123456789101112@Testpublic void testCopyToLocalFile() throws IOException &#123; /* 参数: delSrc - whether to delete the src src - path dst - path useRawLocalFileSystem - whether to use RawLocalFileSystem as local file system or not. 若为true，则不会产生crc校验文件，反之会产生 */// fs.copyToLocalFile(new Path("/1.txt"), new Path("C:/2.txt")); fs.copyToLocalFile(false, new Path("/1.txt"), new Path("C:/2.txt"), true);&#125; 2.7. delete 删除文件或目录123456789101112131415161718@Testpublic void testDelete() throws Exception &#123; /* 方法: delete 参数 f - 要删除的文件或目录 recursive - 递归删除标志。对于删除文件，该值为true/false无所谓。对于删除目录，表示是否递归删除 返回值：如果文件不存在，则返回false。如果指定recursive为false，而且要删除的文件夹不为空，那么抛出异常，如果删除成功返回true。 其他删除方法： deleteOnExit: 如果存在则返回true，并标记删除，如果不存在，则返回false。 * */ // 删除1.txt fs.delete(new Path("/demo.txt"), true); // 删除空目录dir1 fs.delete(new Path("/dir1"), false); // 递归删除dir1目录 fs.delete(new Path("/dir1"), true);&#125; 2.8. rename 从HDFS一个路径移动到另一个路径123456789@Testpublic void testRename() throws Exception &#123; // 将dir1重命名为dir2 fs.rename(new Path("/dir1"), new Path("/dir2")); // 移动1.txt fs.rename(new Path("/1.txt"), new Path("/dir1/1.txt")); // 移动1.txt，并重命名 fs.rename(new Path("/dir1/1.txt"), new Path("/2.txt"));&#125; 2.9. exists 判断文件是否存在123456@Testpublic void testExists() throws Exception &#123; // 判断1.txt是否存在 boolean exists = fs.exists(new Path("/1.txt")); System.out.println(exists);&#125; 2.10. getFileStatus 获取文件属性123456789101112131415161718192021222324252627282930313233343536373839@Testpublic void testGetFileStatus() throws IOException &#123; /* 方法：getFileStatus 参数： f:要获取状态属性指定的文件路径，可以为绝对路径。 返回值：如果获取文件属性成功，则返回FileStatus对象。否则发生异常信息。 其他类似方法： listStatus: 递归的获取文件属性信息。 */ FileStatus fileStatus = fs.getFileStatus(new Path("/demo.txt")); // 获取文件大小（字节） long len = fileStatus.getLen(); // 获取权限位 FsPermission permission = fileStatus.getPermission(); // 获取所有者 String owner = fileStatus.getOwner(); // 获取所有组 String group = fileStatus.getGroup(); // 获取绝对路径 Path path = fileStatus.getPath(); // 获取最后一次修改的时间戳 long modificationTime = fileStatus.getModificationTime(); // 获取最后一次访问的时间戳 long accessTime = fileStatus.getAccessTime(); // 获取块大小 long blockSize = fileStatus.getBlockSize(); // 判断是不是目录 boolean isDirectory = fileStatus.isDirectory(); // 判断是不是文件 boolean file = fileStatus.isFile(); // 获取副本数 short replication = fileStatus.getReplication(); System.out.println(fileStatus);&#125; 2.11. listStatus 列出目录下所有文件的属性123456789101112131415161718192021222324252627282930@Testpublic void testListStatus() throws Exception &#123; FileStatus[] listStatus = fs.listStatus(new Path("/")); for (FileStatus fileStatus : listStatus) &#123; // 获取文件大小（字节） long len = fileStatus.getLen(); // 获取权限位 FsPermission permission = fileStatus.getPermission(); // 获取所有者 String owner = fileStatus.getOwner(); // 获取所有组 String group = fileStatus.getGroup(); // 获取绝对路径 Path path = fileStatus.getPath(); // 获取最后一次修改的时间戳 long modificationTime = fileStatus.getModificationTime(); // 获取最后一次访问的时间戳 long accessTime = fileStatus.getAccessTime(); // 获取块大小 long blockSize = fileStatus.getBlockSize(); // 判断是不是目录 boolean isDirectory = fileStatus.isDirectory(); // 判断是不是文件 boolean file = fileStatus.isFile(); System.out.println(fileStatus); &#125;&#125; 2.12. listFiles 列出目录下所有文件的属性作用与listStatus()类似 1234567891011121314151617181920212223242526272829303132333435363738@Testpublic void testListFiles() throws IOException &#123; RemoteIterator&lt;LocatedFileStatus&gt; iterator = fs.listFiles(new Path("/"), true); while (iterator.hasNext()) &#123; LocatedFileStatus fileStatus = iterator.next(); // 获取文件大小（字节） long len = fileStatus.getLen(); // 获取权限位 FsPermission permission = fileStatus.getPermission(); // 获取所有者 String owner = fileStatus.getOwner(); // 获取所有组 String group = fileStatus.getGroup(); // 获取文件名 String path = fileStatus.getPath().getName(); // 获取最后一次修改的时间戳 long modificationTime = fileStatus.getModificationTime(); // 获取最后一次访问的时间戳 long accessTime = fileStatus.getAccessTime(); // 获取块大小 long blockSize = fileStatus.getBlockSize(); // 判断是不是目录 boolean isDirectory = fileStatus.isDirectory(); // 判断是不是文件 boolean file = fileStatus.isFile(); // 获取副本数 short replication = fileStatus.getReplication(); // 获取各个块副本的位置信息 BlockLocation[] blockLocations = fileStatus.getBlockLocations(); for (BlockLocation blockLocation : blockLocations) &#123; // 该块在哪些主机上 String[] hosts = blockLocation.getHosts(); System.out.println(Arrays.toString(hosts)); &#125; System.out.println(path); &#125;&#125; 2.13. create 通过IO流上传文件到HDFS创建文件并写入数据 1234567891011121314151617181920212223242526272829@Testpublic void testCreate() throws IOException &#123; /* 方法：create 参数： f:指定要创建文件的路径，可以为相对路径。 permission:指定文件权限，默认为644(rw-r--r--)。 overwrite: 是否覆盖，默认覆盖。 bufferSize: 进行写过程中缓存区大小，默认4096。 replication: 备份个数，默认3。 blockSize: 块大小，默认128MB。 progress: 进程通知对象，默认为空。 返回值：如果创建成功，返回FSDataOutputStream对象；否则出现异常信息。 */ // 创建文件并写入数据 FSDataOutputStream fos = fs.create(new Path("/demo.txt")); BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(fos)); bw.write("你好"); bw.newLine(); bw.close(); // 创建，指定副本数为1（只向一个datanode写入数据） // FSDataOutputStream fos = fs.create(new Path("/demo.txt"), (short) 1); // 创建，若文件存在则覆盖 // FSDataOutputStream fos = fs.create(new Path("/demo.txt"), true);&#125; 可实现文件上传。因为是IO流的形式，所以不限于上传本地文件，也可以是上传其它地方的文件到HDFS 123456789@Testpublic void testCreateUpload() throws Exception &#123; // 创建本地输入流 FileInputStream in = new FileInputStream("E:/1.txt"); // 创建hdfs输出流 FSDataOutputStream out = fs.create(new Path("/dir1/1.txt")); // 读取本地文件数据，输出到hdfs IOUtils.copyBytes(in, out, 1024, true);&#125; 2.14. open 通过IO流下载文件1234567891011121314151617181920212223@Testpublic void testOpen() throws Exception &#123; /* 方法：open 参数： f:指定要读取的文件路径，可以为相对路径。 bufferSize: 缓冲区大小。 返回值：如果创建成功获得FSDataInputStream输出流，否则出现异常信息。 */ // 创建hdfs输入流 FSDataInputStream in = fs.open(new Path("/demo.txt")); // 读取hdfs数据，输出到控制台 // IOUtils.copyBytes(in, System.out, 1024, true); // 使用BufferedReader读取 BufferedReader br = new BufferedReader(new InputStreamReader(in)); String line = null; while ((line = br.readLine()) != null) &#123; System.out.println(line); &#125;&#125; HDFS JavaClient操作实例分块下载文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * 下载第一块 * * @throws IOException * @throws InterruptedException * @throws URISyntaxException */@Testpublic void readFileSeek1() throws IOException, InterruptedException, URISyntaxException &#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu"); // 2 获取输入流 FSDataInputStream fis = fs.open(new Path("/hadoop-2.7.2.tar.gz")); // 3 创建输出流 FileOutputStream fos = new FileOutputStream(new File("e:/hadoop-2.7.2.tar.gz.part1")); // 4 流的拷贝 byte[] buf = new byte[1024]; for (int i = 0; i &lt; 1024 * 128; i++) &#123; fis.read(buf); fos.write(buf); &#125; // 5关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos); fs.close();&#125;/** * 下载第2块 * @throws IOException * @throws InterruptedException * @throws URISyntaxException */@Testpublic void readFileSeek2() throws IOException, InterruptedException, URISyntaxException &#123; // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fs = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "atguigu"); // 2 打开输入流 FSDataInputStream fis = fs.open(new Path("/hadoop-2.7.2.tar.gz")); // 3 定位输入数据位置 fis.seek(1024 * 1024 * 128); // 4 创建输出流 FileOutputStream fos = new FileOutputStream(new File("e:/hadoop-2.7.2.tar.gz.part2")); // 5 流的对拷 IOUtils.copyBytes(fis, fos, configuration); // 6 关闭资源 IOUtils.closeStream(fis); IOUtils.closeStream(fos);&#125; 对数据进行合并 1type hadoop-2.7.2.tar.gz.part2 &gt;&gt; hadoop-2.7.2.tar.gz.part1 合并完成后，将hadoop-2.7.2.tar.gz.part1重新命名为hadoop-2.7.2.tar.gz。解压发现该tar包非常完整。]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS Shell]]></title>
    <url>%2F2019%2F02%2F23%2Fhadoop%2FHDFS-Shell%2F</url>
    <content type="text"><![CDATA[1. HDFS Shell1.1. 查看所有命令的用法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152$ hdfs dfsUsage: hadoop fs [generic options] [-appendToFile &lt;localsrc&gt; ... &lt;dst&gt;] [-cat [-ignoreCrc] &lt;src&gt; ...] [-checksum &lt;src&gt; ...] [-chgrp [-R] GROUP PATH...] [-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH...] [-chown [-R] [OWNER][:[GROUP]] PATH...] [-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;] [-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-count [-q] [-h] &lt;path&gt; ...] [-cp [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt;] [-createSnapshot &lt;snapshotDir&gt; [&lt;snapshotName&gt;]] [-deleteSnapshot &lt;snapshotDir&gt; &lt;snapshotName&gt;] [-df [-h] [&lt;path&gt; ...]] [-du [-s] [-h] &lt;path&gt; ...] [-expunge] [-get [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt;] [-getfacl [-R] &lt;path&gt;] [-getfattr [-R] &#123;-n name | -d&#125; [-e en] &lt;path&gt;] [-getmerge [-nl] &lt;src&gt; &lt;localdst&gt;] [-help [cmd ...]] [-ls [-d] [-h] [-R] [&lt;path&gt; ...]] [-mkdir [-p] &lt;path&gt; ...] [-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt;] [-moveToLocal &lt;src&gt; &lt;localdst&gt;] [-mv &lt;src&gt; ... &lt;dst&gt;] [-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;] [-renameSnapshot &lt;snapshotDir&gt; &lt;oldName&gt; &lt;newName&gt;] [-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ...] [-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; ...] [-setfacl [-R] [&#123;-b|-k&#125; &#123;-m|-x &lt;acl_spec&gt;&#125; &lt;path&gt;]|[--set &lt;acl_spec&gt; &lt;path&gt;]] [-setfattr &#123;-n name [-v value] | -x name&#125; &lt;path&gt;] [-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ...] [-stat [format] &lt;path&gt; ...] [-tail [-f] &lt;file&gt;] [-test -[defsz] &lt;path&gt;] [-text [-ignoreCrc] &lt;src&gt; ...] [-touchz &lt;path&gt; ...] [-usage [cmd ...]]Generic options supported are-conf &lt;configuration file&gt; specify an application configuration file-D &lt;property=value&gt; use value for given property-fs &lt;local|namenode:port&gt; specify a namenode-jt &lt;local|resourcemanager:port&gt; specify a ResourceManager-files &lt;comma separated list of files&gt; specify comma separated files to be copied to the map reduce cluster-libjars &lt;comma separated list of jars&gt; specify comma separated jar files to include in the classpath.-archives &lt;comma separated list of archives&gt; specify comma separated archives to be unarchived on the compute machines.The general command line syntax isbin/hadoop command [genericOptions] [commandOptions] 1.2. help 查看某一命令的具体用法12hdfs dfs -help get # 查看get的用法hdfs dfs -help put # 查看put的用法 1.3. ls 目录浏览用法：-ls [-d] [-h] [-R] [&lt;path&gt; ...] 选项 描述 -d 查看目录本身信息 -h 显示文件大小单位便于阅读（byte单位省略） -R 递归显示 12345678910111213$ hdfs dfs -ls / # 列出目录Found 2 items-rw-r--r-- 3 root supergroup 199635269 2019-02-23 13:35 /hadoop-2.6.5.tar.gzdrwxr-xr-x - root supergroup 0 2019-02-23 13:03 /input$ hdfs dfs -ls -d / # 列出目录本身drwxr-xr-x - root supergroup 0 2019-02-23 13:35 /$ hdfs dfs -ls -R / # 递归查看根目录-rw-r--r-- 3 root supergroup 199635269 2019-02-23 13:35 /hadoop-2.6.5.tar.gzdrwxr-xr-x - root supergroup 0 2019-02-23 13:03 /input-rw-r--r-- 3 root supergroup 280 2019-02-23 13:03 /input/1.txt-rw-r--r-- 3 root supergroup 58 2019-02-23 13:03 /input/2.txt 1.4. mkdir 创建目录用法：-mkdir [-p] &lt;path&gt; ... 选项 描述 -p 递归创建 123hdfs dfs -mkdir /d1hdfs dfs -mkdir /d1 /d2 # 创建多个目录hdfs dfs -mkdir -p /d1/d2 1.5. moveFromLocal 从本地移动到HDFS用法：-moveFromLocal &lt;localsrc&gt; ... &lt;dst&gt; 说明：与put类似，区别在于moveFromLocal会删除本地文件 12hdfs dfs -moveFromLocal 1.txt /d1 # 移动文件hdfs dfs -moveFromLocal dir1 /d1 # 移动目录 1.6. cat 查看HDFS的文件用法：-cat [-ignoreCrc] &lt;src&gt; ... 12hdfs dfs -cat /input/1.txt # 查看文件hdfs dfs -cat /input/1.txt /input/2.txt # 查看多个文件 1.7. appendToFile 追加文件到已存在文件末尾用法：-appendToFile &lt;localsrc&gt; ... &lt;dst&gt; 说明：&lt;localsrc&gt;的值如果是 -，表示从stdin读取输入 12345$ hdfs dfs -appendToFile 2.txt /1.txt # 2.txt追加到1.txt$ hdfs dfs -appendToFile 2.txt 3.txt /1.txt # 2.txt、3.txt追加到1.txt$ hdfs dfs -appendToFile - /1.txt # 将控制台的输入追加到1.txthello world^D # CTRL+D结束输入 1.8. chgrp 修改所有组用法：-chgrp [-R] GROUP PATH... 123hdfs dfs -chgrp group1 /1.txthdfs dfs -chgrp group1 /dir1hdfs dfs -chgrp -R group1 /dir1 # 递归修改 1.9. chown 修改所有者和所有组用法：-chown [-R] [OWNER][:[GROUP]] PATH... 1234hdfs dfs -chown user1 /1.txthdfs dfs -chown user1:group1 /1.txthdfs dfs -chown user1:group1 /dir1hdfs dfs -chown -R user1:group1 /dir1 # 递归修改 1.10. chmod 修改权限用法：-chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; PATH.. 12345hdfs dfs -chmod a+x /1.txthdfs dfs -chmod +x /1.txt # 权限+x 即 a+x hdfs dfs -chmod a-x /1.txthdfs dfs -chmod -x /1.txt # 权限-x 即 a-xhdfs dfs -chmod -R 755 / # -R 递归修改 1.11. put / copyFromLocal 上传本地文件到HDFS用法：-copyFromLocal [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;、-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt; 选项 描述 -p 保留源文件的access时间戳、modification时间戳、所有者、所有组、和文件权限 -f 如果目标文件已存在，则覆盖 -l 允许DataNode懒惰持久化到磁盘（lazily persist），强制设置副本因子为1。该选项会导致可用性降低，请谨慎使用 1234hdfs dfs -put 1.txt /d1 # 上传1.txthdfs dfs -put 1.txt 2.txt /d1 # 上传1.txt和2.txthdfs dfs -put *.txt /d1 # 上传所有txt文件hdfs dfs -put d1 / # 上传整个d1目录 1.12. get / copyToLocal 下载文件到本地用法：-put [-f] [-p] [-l] &lt;localsrc&gt; ... &lt;dst&gt;，-copyToLocal [-p] [-ignoreCrc] [-crc] &lt;src&gt; ... &lt;localdst&gt; 选项 描述 -p 保留源文件的access时间戳、modification时间戳、所有者、所有组、和文件权限 123hdfs dfs -get /1.txt ~ # 下载1.txthdfs dfs -get /1.txt /2.txt ~ # 下载1.txt和2.txthdfs dfs -get /*.txt ~ # 下载所有的txt文件 1.13. cp 从HDFS一个路径拷贝到HDFS的另一个路径用法：-cp [-f] [-p | -p[topax]] &lt;src&gt; ... &lt;dst&gt; 选项 描述 -p all of the source and target pathnames are in the /.reserved/raw hierarchy. raw namespace xattr preservation is determined solely by the presence (or absence) of the /.reserved/raw prefix and not by the -p option -f 如果目标文件已存在，则覆盖 12345678$ hdfs dfs -ls -R /drwxr-xr-x - root supergroup 0 2018-04-08 23:42 /d1-rw-r--r-- 1 root supergroup 6 2018-04-08 23:16 /d1/1.txtdrwxr-xr-x - root supergroup 0 2018-04-08 23:42 /d2hdfs dfs -cp /d1/1.txt /d2 # 将1.txt拷贝到/d2目录下hdfs dfs -cp /d1/1.txt /dir100 # 不存在dir100目录，所以将1.txt拷贝到根目录下，并命名为"/dir100"hdfs dfs -cp /d1 /d3 # 可以递归复制整个目录 1.14. getmerge 合并下载多个文件到本地用法：-getmerge [-nl] &lt;src&gt; &lt;localdst&gt; 选项 描述 -nl 合并时，在每个文件之后添加一个空行 -f 如果目标文件已存在，则覆盖 12hdfs dfs -getmerge /*.txt merge.txt # 下载合并txt到本地，命名为merge.txthdfs dfs -getmerge /1.txt /2.txt /3.txt merge.txt # 下载下载1.txt、2.txt、3.txt 1.15. tail 查看文件末尾1KB数据用法：-tail [-f] &lt;file&gt; 注意：是查看末尾1KB数据，而不是最后几行 选项 描述 -f 监视文件末尾 12hdfs dfs -tail /1.txthdfs dfs -tail -f /1.txt 1.16. rm 删除文件用法：-rm [-f] [-r|-R] [-skipTrash] &lt;src&gt; ... 选项 描述 -f 若目标文件不存在，不显示提示信息 -r 或 -R 递归删除 1234hadoop fs -rm /1.txt # 删除1.txthadoop fs -rm /1.txt /2.txt # 删除1.txt和2.txthadoop fs -rm -r -f /d1 # 删除目录d1hadoop fs -rm -r -f /'*' # 删除根目录下的一切文件。星号作为参数传入，所以加引号防止星号被转义 1.17. rmdir 删除空目录用法：-rmdir [--ignore-fail-on-non-empty] &lt;dir&gt; 1hdfs dfs -rmdir /d1 1.18. rmr 递归删除用法：rmr等价于 rm -r 1.19. mv 从HDFS一个路径移动到HDFS的另一个路径用法：-mv &lt;src&gt; ... &lt;dst&gt; 12hadoop fs -mv /d1 /d2 # 更名hadoop fs -mv /*.txt /d3 # 将所有txt文件移动到d3目录下 1.20. du 统计目录各文件大小用法：-du [-s] [-h] &lt;path&gt; ... 选项 描述 -s 统计目录总大小 -h 显示文件大小单位便于阅读（byte单位省略） 12345678910111213141516171819$ hdfs dfs -du / # 统计目录下各文件大小115 /1.txt3 /2.txt3 /3.txt199635269 /hadoop-2.6.5.tar.gz338 /input$ hdfs dfs -du -h /115 /1.txt3 /2.txt3 /3.txt190.4 M /hadoop-2.6.5.tar.gz338 /input$ hdfs dfs -du -s /199635728 /$ hdfs dfs -du -s -h /190.4 M / 1.21. setrep 修改文件的数据块副本数用法：-setrep [-R] [-w] &lt;rep&gt; &lt;path&gt; ... 选项 描述 -w It requests that the command waits for the replication to complete. This can potentially take a very long time. -R It is accepted for backwards compatibility. It has no effect. 1234567891011121314$ hdfs dfs -ls /1.txt-rw-r--r-- 3 root root 115 2019-02-23 23:42 /1.txt # 现在有3个副本，分别保存在3个DataNode上$ hdfs dfs -setrep 2 /1.txt # 修改为2个副本。其中1个DataNode上的副本会被删除Replication 2 set: /1.txt$ hdfs dfs -ls /1.txt -rw-r--r-- 2 root root 115 2019-02-23 23:42 /1.txt # 只有2个副本$ hdfs dfs -setrep 10 /1.txt # 修改为10个副本。但是DataNode只有3个，所以目前副本数恢复到3个。直到只少动态添加7个DataNode，副本才会达到10个Replication 10 set: /1.txt$ hdfs dfs -ls /1.txt-rw-r--r-- 10 root root 115 2019-02-23 23:42 /1.txt 1.22. touchz 创建空文件用法：-touchz &lt;path&gt; ... 12hdfs dfs -touchz /file1hdfs dfs -touchz /file2 /file3]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 运行环境搭建]]></title>
    <url>%2F2019%2F02%2F22%2Fhadoop%2FHadoop-%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1. Hadoop目录结构123456789101112.├── bin # 存放对Hadoop相关服务（HDFS,YARN）进行操作的脚本├── etc # 存放Hadoop的配置文件├── include├── lib # 存放Hadoop的本地库（对数据进行压缩解压缩功能）├── libexec ├── LICENSE.txt├── logs # 存放日志文件├── NOTICE.txt├── README.txt├── sbin # 存放启动或停止Hadoop相关服务的脚本 ├── share # 存放Hadoop的依赖jar包、文档、和官方案例 2. Hadoop基本环境搭建 安装jdk 1.8 安装hadoop 2.1. 编辑etc/hadoop/hadoop-env.sh1234# 修改JAVA_HOME，这样hadoop才能找到JVMexport JAVA_HOME=/usr/lib/java/jdk# 修改PID生成文件成在目录HADOOP_PID_DIR，默认是/tmpexport HADOOP_PID_DIR=/usr/local/hadoop/tmp 2.2. 编辑etc/hadoop/mapred-env.sh1234# 修改JAVA_HOMEexport JAVA_HOME=/usr/lib/java/jdk# 修改PID生成文件成在目录export HADOOP_MAPRED_PID_DIR=/usr/local/hadoop/tmp 2.3. 编辑etc/hadoop/yarn-env.sh1234# 修改JAVA_HOMEexport JAVA_HOME=/usr/lib/java/jdk# 添加PID生成文件成在目录export YARN_PID_DIR=/usr/local/hadoop/tmp 3. Hadoop本地模式（Standalone）搭建刚安装完Hadoop，什么都不配置，默认就是本地模式。 这里的本地模式有两个意思 HDFS直接使用本地文件系统。因为fs.defaultFS的默认值是file:/// MapReduce运行在本地，而不是YARN。因为mapreduce.framework.name的默认值是local 3.1. 编辑/etc/hosts查看当前主机的hostname 12$ hostname # 查看当前主机名ubuntu 编辑/etc/hosts，为当前主机名添加一条记录。若没有，运行本地MapReduce时会出现java.net.UnknownHostException 错误 1192.168.57.100 ubuntu # 为主机名添加一条记录 3.2. 准备数据创建input目录 12cd /usr/local/hadoopmkdir input 创建 input/1.txt 1234567891011121314151617181920onetwothreefourfivesixseveneightnineteneleventwelvethirteenfourteenfifteensixteenseventeeneighteennineteentwenty 创建 input/2.txt 1234567891011121314applebananablueberrybonegrapelemenorangepearpeachstrawberryhello worldhadoop clusterapache hadoophadoop ecosystem 3.3. 官方grep案例运行grep案例。将input目录下所有文件作为输入，按行遍历，找出所有符合 正则表达式&#39;o.+&#39;的字符串，输出到output目录。注意运行前output目录不应该存在，否则运行会出现FileAlreadyExistsException错误 1hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar grep input output 'o.+' 查看生成的output目录 1234$ ls -l output/total 4-rw-r--r-- 1 root root 41 2月 22 14:52 part-r-00000 # 输出结果-rw-r--r-- 1 root root 0 2月 22 14:52 _SUCCESS # 空文件，仅仅是一个运行成功的标志 查看结果 123456789$ cat output/part*2 one1 ourteen1 our1 orange1 oop ecosystem1 oop cluster1 oop1 o world 3.4. 官方WordCount案例运行wordcount案例。将input目录下所有文件作为输入，输出到output目录 1hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar wordcount input output 查看结果 12345678910111213141516171819202122232425262728293031323334353637$ cat output/part*apache 1apple 1banana 1blueberry 1bone 1cluster 1ecosystem 1eight 1eighteen 1eleven 1fifteen 1five 1four 1fourteen 1grape 1hadoop 3hello 1lemen 1nine 1nineteen 1one 1orange 1peach 1pear 1seven 1seventeen 1six 1sixteen 1strawberry 1ten 1thirteen 1three 1twelve 1twenty 1two 1world 1 4. Hadoop伪分布模式（Pseudo-Distributed）搭建伪分布式所有服务仅在1台机器上运行，但是配置与完全分布式类似 4.1. 为主机名添加DNS记录编辑/etc/hosts，要为当前主机名添加一条记录 1192.168.57.100 ubuntu HDFS客户端也要编辑hosts文件，同样要添加DNS记录。因为之后客户端访问HDFS时，NameNode会让client去找DataNode（去找主机名为ubuntu的主机），如果没有添加记录，client就无法访问DataNode 1192.168.57.100 ubuntu 4.2. 配置ssh本机免密码登录 SSH免密钥登录原理： A使用ssh-key-gen命令生成密钥对 A将公钥拷贝到B的authorized_keys中 A使用SSH访问B时，用私钥对数据加密 B收到数据，从authorized_keys中找到A的公钥，对数据解密 B给A一个回复，回复的数据用A的公钥加密 A收到B的回复，用私钥解密。到此，A就成功连接上了B。 所以A想要免密钥登录B，需要预先将自己的公钥发送到B的authorized_keys中 运行ssh服务 123# 选以下任意一条命令即可/etc/init.d/ssh startsystemctl start ssh 设置本机免密码登录 12345ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa# -t: 指定加密算法。此处指定使用rsa加密算法# -P: 指定密钥的密码。因为是免密码，所以是空串 # -f: 指定生成的密钥文件名cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys # 将公钥添加到认证文件中 登录本地测试。默认情况下，ssh开启StrictHostKeyChecking yes，所以首次连接时会询问yes/no，回答yes即可 12ssh localhostexit 4.3. 编辑etc/hadoop/core-site.xml12345678910111213141516&lt;configuration&gt; &lt;property&gt; &lt;!-- hadoop的基本临时目录。dfs.namenode.name.dir之类的目录以该目录为基础 默认值是/tmp/hadoop-$&#123;user.name&#125;，而/tmp在系统重启时有可能被系统清理掉，后面配置的namenode和datanode数据存放路径都是基于$&#123;hadoop.tmp.dir&#125;的，这样系统重启后数据就全丢了，下一次运行HDFS时必须要重新格式化namenode生成fsimage才行，所以必须要修改 --&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- NameNode在哪个主机，监听哪个端口 默认值是 file:///，即HDFS默认是本机模式 --&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://ubuntu:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4.4. 编辑etc/hadoop/hdfs-site.xml1234567891011121314151617181920212223242526272829&lt;configuration&gt; &lt;property&gt; &lt;!-- 块副本数（备份数） 因为只有一台机器，所以设置副本数为1 默认副本数为3。即使配置副本数为3，因为现在DataNode只有1个节点，所以块数据也只会产生1个副本。但是动态增加DataNode之后，NameNode发现增加了几台DataName之后，就会将数据块复制到其它的DataNode上，直到副本数达到dfs.replication --&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- NameNode的fsimage存放的位置 默认值是 file://$&#123;hadoop.tmp.dir&#125;/dfs/name，虽然手动设置的值和默认路径相同，但是默认值“file://”不符合URL规范，会出错，所以必须手动设置该参数的值。 --&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- DataNode的块数据存放的位置 默认值是，file://$&#123;hadoop.tmp.dir&#125;/dfs/data，同上，要手动再设置一下 --&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 禁用权限检查, 这样在windows下开发时，使用windows用户就能向HDFS写入数据 --&gt; &lt;name&gt;dfs.permissions.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4.5. 格式化NameNodeHDFS的格式化和系统磁盘的格式化含义是一致的。 NameNode节点上，有两个最重要的路径 dfs.namenode.name.dir：存储元数据信息 dfs.namenode.edits.dir：存储操作日志 格式化时，NameNode会清空两个目录下的所有文件，之后，会在两个目录下创建以下文件： 12345678&#123;dfs.namenode.name.dir&#125;/current/fsimage # 存储命名空间（目录和文件）的元数据信息&#123;dfs.namenode.name.dir&#125;/current/fstime # 用来存储元数据上一次check point的时间&#123;dfs.namenode.name.dir&#125;/current/VERSION # 存储NameNode版本信息，命名空间ID(版本号)&#123;dfs.namenode.name.dir&#125;/image/fsimage&#123;dfs.namenode.edits.dir&#125;/current/edits # 存储对命名空间操作的日志信息&#123;dfs.namenode.edits.dir&#125;/current/fstime # 同上&#123;dfs.namenode.edits.dir&#125;/current/VERSION # 同上&#123;dfs.namenode.edits.dir&#125;/image/fsimage # 同上 因为默认${dfs.namenode.edits.dir}=${dfs.namenode.name.dir}，所以两个目录实际上是同一个目录。所以只会看到这些文件 12345&#123;dfs.namenode.name.dir&#125;/current/fsimage # 存储命名空间（目录和文件）的元数据信息&#123;dfs.namenode.name.dir&#125;/current/fstime # 用来存储元数据上一次check point的时间&#123;dfs.namenode.name.dir&#125;/current/VERSION # 存储NameNode版本信息，命名空间ID(版本号)&#123;dfs.namenode.name.dir&#125;/image/fsimage&#123;dfs.namenode.edits.dir&#125;/current/edits # 存储对命名空间操作的日志信息 所以格式化NameNode分两种情况： 首次格式化：HDFS文件系统fsimage还不存在，需要格式化生成文件系统，之后才能对HDFS进行操作。 再次格式化：清空HDFS文件系统的所有数据。 4.5.1. 首次格式化的操作现在HDFS的文件系统还不存在，所以要先格式化NameNode。 123# 以下两条命令等价，选择一条执行即可hdfs namenode -formathadoop namenode -format 看到以下信息，说明格式化成功 1INFO common.Storage: Storage directory /usr/local/hadoop/tmp/dfs/name has been successfully formatted 查看格式化生成的文件 123456789# tree tmp/tmp/└── dfs └── name └── current ├── fsimage_0000000000000000000 ├── fsimage_0000000000000000000.md5 ├── seen_txid └── VERSION 4.5.2. 再次格式化的操作第一步：先关闭Hadoop集群 第二步：再删除日志目录logs和${hadoop.tmp.dir}。注意一定要先关闭集群，不然一删除，logs和tmp目录又会生成。 12rm -rf logs/ # 删除logsrm -rf tmp # 删除$&#123;hadoop.tmp.dir&#125; 第三步：再次格式化 1hdfs namenode -format 4.5.3. 格式化前不删除tmp目录的后果在NameNode格式化后，会生成一个clusterID。之后DataNode和SecondaryNameNode与NameNode通信后，会生成相同的clusterID。 123456789101112$ find tmp/ -name 'VERSION'tmp/dfs/name/current/VERSION tmp/dfs/namesecondary/current/VERSIONtmp/dfs/data/current/BP-671477692-192.168.57.100-1550830174992/current/VERSIONtmp/dfs/data/current/VERSION$ cat tmp/dfs/name/current/VERSION | grep clusterID # NameNode的clusterIDclusterID=CID-b1086067-1516-4cde-ad96-3cbb3c2afb7b$ cat tmp/dfs/namesecondary/current/VERSION | grep clusterID # DataNode的clusterIDclusterID=CID-b1086067-1516-4cde-ad96-3cbb3c2afb7b$ cat tmp/dfs/data/current/VERSION | grep clusterID # SecondaryNameNode的clusterIDclusterID=CID-b1086067-1516-4cde-ad96-3cbb3c2afb7b 如果不删除tmp目录，直接格式化NameNode，那么NameNode会生成新的clusterID，与DataNode和SecondaryNameNode的clusterID就不一致了。重启启动集群时，就会报错 java.io.IOException: Incompatible namespaceIDs，导致集群无法启动。 4.6. 启动HDFS服务启动HDFS服务 1start-dfs.sh 查看进程 12345$ jps3157 DataNode3034 NameNode3371 SecondaryNameNode3483 Jps 访问 NameNode的WebUI，默认端口50070 4.7. 在本地运行MapReduce在mapred-default.xml中，mapreduce.framework.name默认值是local，所以MapReduce默认在本地运行。 在本地运行MapReduce 123hdfs dfs -put input / # 上传之前的input目录hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar wordcount /input /output # 以HDFS上的/input目录下所有文件作为输入，输出到HDFS的/output目录hdfs dfs -cat /output/part* # 查看输出结果 4.8. 编辑etc/hadoop/mapred-site.xml默认只有模板文件，所以先复制得到一份配置文件 1cp etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xml 编辑 etc/hadoop/mapred-site.xml 1234567&lt;configuration&gt; &lt;property&gt; &lt;!-- 默认值为local，即mapreduce默认在本地运行，改为使用yarn运行mapreduce程序 --&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; ​ 4.9. 编辑etc/hadoop/yarn-site.xml12345678910111213141516171819&lt;configuration&gt; &lt;property&gt; &lt;!-- NodeManager上运行的附属服务。需配置成mapreduce_shuffle，才可以在YARN上运行MapReduce程序 mapreduce_shuffle表示，Reducer以shuffle的方式获取数据 --&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定shuffle处理类，默认值就是ShuffleHandler，可不配置 --&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce_shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定YARN的ResourceManager所在节点，默认为当前主机名，默认是0.0.0.0，不配置关系也不大 --&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;ubuntu&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4.10. 在YARN上运行MapReduce之前已经配置了在YARN上运行MapReduce，现在尝试运行MapReduce 123hdfs dfs -put input / # 上传之前的input目录hdfs dfs -rm -r -f /output # 删除/output目录hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar wordcount /input /output # 运行MapReduce 因为YARN还没有启动，所以运行MapReduce后，会一直尝试连接YARN。CTRL+C 结束尝试。 1INFO ipc.Client: Retrying connect to server: 0.0.0.0/0.0.0.0:8032 # 不断重复出现Retrying connect 信息 启动YARN 1start-yarn.sh 再次运行MapReduce，成功 1hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar wordcount /input /output 4.11. 配置MapReduce历史服务器在配置启动JobHistory服务之前，YARN的WebUI界面里是无法查看作业的历史信息的 4.11.1. 编辑etc/hadoop/mapred-site.xml12345678910&lt;property&gt; &lt;!-- MapReduce JobHistory Server IPC host:port，默认值0.0.0.0:10020 --&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;ubuntu:10020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- MapReduce JobHistory Server Web UI host:port，默认值0.0.0.0:19888 --&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;ubuntu:19888&lt;/value&gt;&lt;/property&gt; 4.11.2. 启动历史服务器启动 1mr-jobhistory-daemon.sh start historyserver 查看进程 1234567$ jps2721 NodeManager2424 ResourceManager3336 JobHistoryServer # 历史服务1673 DataNode1595 NameNode3405 Jps 再次点击查看history 即可看到history的Overview主界面 Counters界面：查看各个计数器信息 Configuration界面：该作业在运行时，所有的配置信息 Map tasks界面：各个Map任务的运行信息 Reduce tasks界面：各个Reduce任务的运行信息 4.12. 配置YARN日志聚集日志聚集概念：应用运行完成以后，将程序运行日志信息上传到HDFS系统上 日志聚集功能好处：可以方便的查看到程序运行详情，方便开发调试 注意：开启日志聚集功能，需要重新启动NodeManager 、ResourceManager和HistoryManager 4.12.1. 关闭相关服务12mr-jobhistory-daemon.sh stop historyserverstop-yarn.sh 4.12.2. 编辑etc/hadoop/yarn-site.xml12345678910111213&lt;property&gt; &lt;!-- 开启YARN的日志聚集功能。默认值为false --&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 设置日志保留时间，单位为秒。 默认值为-1，表示不保留，即不启用日志聚合。所以只配置yarn.log-aggregation-enable，不设置yarn.log-aggregation.retain-seconds，也看不到日志。 设置7天，即604800秒 --&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;604800&lt;/value&gt;&lt;/property&gt; 4.12.3. 启动相关服务12start-yarn.shmr-jobhistory-daemon.sh start historyserver 4.12.4. 再次运行WordCount12hdfs dfs -rm -r -f /outputjar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.5.jar wordcount /input /output 4.12.5. 通过Web查看日志先进入history页面 再进入logs页面 进入日志页面。默认只显示4096字节的日志信息，可点击查看完整日志 4.13. 配置文件说明Hadoop配置文件分两类：默认配置文件和自定义配置文件，只有用户想修改某一默认配置值时，才需要修改自定义配置文件，更改相应属性值 要获取的默认文件 文件存放在Hadoop的jar包中的位置 core-default.xml hadoop-common-2.7.2.jar/ core-default.xml hdfs-default.xml hadoop-hdfs-2.7.2.jar/ hdfs-default.xml yarn-default.xml hadoop-yarn-common-2.7.2.jar/ yarn-default.xml mapred-default.xml hadoop-mapreduce-client-core-2.7.2.jar/ mapred-default.xml core-site.xml、hdfs-site.xml、yarn-site.xml、mapred-site.xml四个配置文件存放在$HADOOP_HOME/etc/hadoop这个路径上，用户可以根据项目需求重新进行修改配置。 5. Hadoop完全分布式搭建5.1. 集群规划 hostname hadoop1 hadoop2 hadoop3 ip 192.168.57.101 192.168.57.102 192.168.57.103 system ubuntu16.04 ubuntu16.04 ubuntu16.04 HDFS NameNode DataNode DataNode SecondaryNameNode DataNode YARN NodeManager ResourceManager NodeManager NodeManager 原则：NameNode、SecondaryNameNode、ResourceManager最好分别处在不同的节点上 5.2. 配置静态ip编辑 /etc/network/interfaces 123456789101112131415161718# hadoop1auto ens33iface ens33 inet staticaddress 192.168.57.101/24gateway 192.168.57.2dns-nameservers 114.114.114.114# hadoop3auto ens33iface ens33 inet staticaddress 192.168.57.102/24gateway 192.168.57.2dns-nameservers 114.114.114.114# hadoop2auto ens33iface ens33 inet staticaddress 192.168.57.103/24gateway 192.168.57.2dns-nameservers 114.114.114.114 5.3. 配置hostname编辑 /etc/hostname 123456# hadoop1hadoop1# hadoop2hadoop# hadoop3hadoop 5.4. 创建集群分发脚本xsync因为 /usr/local/bin目录在$PATH中，该目录下的命令可以直接执行，所以创建 /usr/local/bin/xsync，写入以下内容 1234567891011121314151617181920212223242526#!/bin/bash### 获取输入参数个数，如果没有参数，直接退出pcount=$#if ((pcount == 0)); then echo no args; exit;fi### 获取文件名称p1=$1fname=`basename $p1`echo fname=$fname### 获取上级目录到绝对路径pdir=`cd -P $(dirname $p1); pwd`echo pdir=$pdir### 获取当前用户名称user=`whoami`### 循环远程同步到各个节点上for ((host = 1; host &lt;= 3; host++)); do echo ------------------- hadoop$host -------------- rsync -rvl $pdir/$fname $user@hadoop$host:$pdirdone 添加可执行权限 1chmod 755 /usr/local/bin/xsync 5.5. 创建集群执行同一命令的脚本xcall创建 /usr/local/bin/xcall 12345678910111213141516171819202122#!/bin/bash### 获取输入参数个数，如果没有参数，直接退出pcount=$#if ((pcount == 0)); then echo no args; exit;fi### 获取当前用户名称user=`whoami`### 获取执行的命令（除了可执行文件名后面的所有参数）cmd=$@### 在各个节点上执行for ((host = 1; host &lt;= 3; host++)); do # 打印执行的命令 echo --------------------- hadoop$host: $@ --------------------- # ssh执行命令 ssh $user@hadoop$host $cmddone 添加可执行权限 1chmod 755 /usr/local/bin/xcall 命令测试 1234xcall echo 111 '&gt;' ~/1.txtxcall echo 111 '&gt;&gt;' ~/1.txtxcall source /etc/profile '&amp;&amp;' jpsxcall ls -al 5.6. 添加hosts记录编辑 /etc/hosts，添加以下记录 123192.168.57.101 hadoop1192.168.57.102 hadoop2192.168.57.103 hadoop3 同步 /etc/hosts 1xsync /etc/hosts 同步xsync脚本 1xsync /usr/local/bin/xsync 5.7. 配置SSH免密码登录 hadoop1 hadoop2 hadoop3 HDFS NameNode DataNode DataNode SecondaryNameNode DataNode YARN NodeManager ResourceManager NodeManager NodeManager hadoop1是NameNode，故要能够免密钥登录到所有的HDFS节点（包括自身NameNode 、DataNode、SecondaryNameNode） hadoop2是NodeManager，故要能够免密钥登录到所有的YARN节点（包括自身ResourceManager、NodeManager） hadoop1和hadoop2执行以下操作 12345678910# 生成密钥对ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa# 将公钥添加到自身以及其它节点的authorized_keys文件中ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop1ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop2ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop3# 连接测试ssh hadoop1ssh hadoop2ssh hadoop3 5.8. 安装jdk安装jdk，同步jdk和环境变量 12xsync /usr/lib/javaxsync /etc/profile 各节点使环境变量生效 1source /etc/profile 5.9. 安装并配置hadoop安装hadoop，配置基本环境，与之前的操作一样 123456789### 编辑etc/hadoop/hadoop-env.shexport JAVA_HOME=/usr/lib/java/jdkexport HADOOP_PID_DIR=/usr/local/hadoop/tmp### 编辑etc/hadoop/mapred-env.shexport JAVA_HOME=/usr/lib/java/jdkexport HADOOP_MAPRED_PID_DIR=/usr/local/hadoop/tmp### 编辑etc/hadoop/yarn-env.shexport JAVA_HOME=/usr/lib/java/jdkexport YARN_PID_DIR=/usr/local/hadoop/tmp 同步hadoop和环境变量 12xsync /usr/local/hadoopxsync /etc/profile 各节点使环境变量生效 1source /etc/profile 5.9.1. 编辑etc/hadoop/core-site.xml123456789101112&lt;configuration&gt; &lt;property&gt; &lt;!-- hadoop的基本临时目录 --&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- NameNode在哪个主机，监听哪个端口 --&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop1:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5.9.2. 编辑etc/hadoop/slaves编辑slaves，一行一个datanode和nodemanager。注意：文件中不允许有空行，每行后面不允许有多余的空格。 123hadoop1hadoop2hadoop3 5.9.3. 编辑etc/hadoop/hdfs-site.xml1234567891011121314151617181920212223242526&lt;configuration&gt; &lt;property&gt; &lt;!-- 块副本数（备份数） --&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- NameNode的fsimage存放的位置 默认值是 file://$&#123;hadoop.tmp.dir&#125;/dfs/name，虽然手动设置的值和默认路径相同，但是默认值“file://”不符合URL规范，会出错，所以必须手动设置该参数的值。 --&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- DataNode的块数据存放的位置 默认值是，file://$&#123;hadoop.tmp.dir&#125;/dfs/data，同上，要手动再设置一下 --&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/usr/local/hadoop/tmp/dfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- SecondaryNameNode的HTTP服务 --&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;hadoop3:50090&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5.9.4. 编辑etc/hadoop/mapred-site.xml默认只有模板文件，所以先复制得到一份配置文件 1cp etc/hadoop/mapred-site.xml.template etc/hadoop/mapred-site.xml 编辑 etc/hadoop/mapred-site.xml 1234567&lt;configuration&gt; &lt;property&gt; &lt;!-- 默认值为local，即mapreduce默认在本地运行，改为使用yarn运行mapreduce程序 --&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5.9.5. 编辑etc/hadoop/yarn-site.xml123456789101112&lt;configuration&gt; &lt;property&gt; &lt;!-- Reducer以shuffle的方式获取数据 --&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定YARN的ResourceManager所在节点 --&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop2&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5.9.6. 同步hadoop因为操作了很多文件，直接将整个hadoop同步一下 1xsync /usr/local/hadoop 5.9.7. 格式化NameNode在格式化之前注意： 确保各个节点的服务都于关闭状态 确保各个节点的logs和tmp目录已删除 hadoop1（NameNode）执行格式化命令 1hdfs namenode -format 5.9.8. 启动HDFShadoop1（NameNode）执行 1start-dfs.sh 查看进程 1234567891011$ jps # hadoop14259 DataNode4109 NameNode4766 Jps$ jps # hadoop23426 Jps3275 DataNode$ jps # hadoop32853 DataNode2972 SecondaryNameNode3037 Jps 5.9.9. 启动YARNhadoop2（ResourceManager）执行 1start-yarn.sh 查看进程 123456789101112131415$ jps # hadoop126753 Jps26215 NameNode26376 DataNode26649 NodeManager$ jps # hadoop226272 NodeManager25969 ResourceManager26403 Jps25838 DataNode$ jps # hadoop325680 NodeManager25428 DataNode25785 Jps25563 SecondaryNameNode 5.10. 集群基本测试input目录里有两个小文件 1234$ ls -l inputtotal 8-rw-r--r-- 1 root root 280 2月 23 13:02 1.txt-rw-r--r-- 1 root root 58 2月 23 13:03 2.txt 上传input目录 1hdfs dfs -put input /input Web查看。因为是小文件，所以都只有1个数据块，在3个DataNode上各有1个副本 查看块数据文件 123456$ ls -l tmp/dfs/data/current/BP-1856722919-192.168.57.101-1550860466565/current/finalized/subdir0/subdir0total 16-rw-r--r-- 1 root root 280 2月 23 13:03 blk_1073741825 # 1.txt对应的block-rw-r--r-- 1 root root 11 2月 23 13:03 blk_1073741825_1001.meta-rw-r--r-- 1 root root 58 2月 23 13:03 blk_1073741826 # 2.txt对应的block-rw-r--r-- 1 root root 11 2月 23 13:03 blk_1073741826_1002.meta 12cat blk_1073741825 # 1.txt的内容cat blk_1073741826 # 2.txt的内容 上传大文件 12$ ls -lh hadoop-2.6.5.tar.gz-rw-r--r-- 1 root root 191M 2月 18 19:34 hadoop-2.6.5.tar.gz 1hdfs dfs -put hadoop-2.6.5.tar.gz / Web查看。191M的文件被分成2块，在3个DataNode上各有1个副本 查看块数据文件 12345678910$ ls -l tmp/dfs/data/current/BP-1856722919-192.168.57.101-1550860466565/current/finalized/subdir0/subdir0total 196504-rw-r--r-- 1 root root 280 2月 23 13:03 blk_1073741825-rw-r--r-- 1 root root 11 2月 23 13:03 blk_1073741825_1001.meta-rw-r--r-- 1 root root 58 2月 23 13:03 blk_1073741826-rw-r--r-- 1 root root 11 2月 23 13:03 blk_1073741826_1002.meta-rw-r--r-- 1 root root 134217728 2月 23 13:35 blk_1073741827 # block1-rw-r--r-- 1 root root 1048583 2月 23 13:35 blk_1073741827_1003.meta-rw-r--r-- 1 root root 65417541 2月 23 13:35 blk_1073741828 # block2-rw-r--r-- 1 root root 511083 2月 23 13:35 blk_1073741828_1004.meta 12cat blk_1073741827 blk_1073741828 &gt;&gt; tmpfile # 拼接两个block，得到完整文件tar -zxvf tmpfile # 原本是tgz，所以能解压 5.11. 集群时间同步时间同步的方式：找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，比如，每隔十分钟，同步一次时间。 5.11.1. 时间服务器配置让hadoop1作为时间服务器 5.11.1.1. 安装ntp12dpkg -l | grep ntp # 查看是否安装ntpapt-get install -y ntp # 安装ntp 5.11.2. 编辑/etc/ntp.conf修改1：授权 192.168.57.0/24 网段操作权限 1234### 授权方式是否定式的，如果不做任何限制，就是所有权限# nomodify: 客户端不能通过ntpc和ntpq修改服务器的时间配置，但可以通过服务器来校时# notrap: 不提供trap这个远程事件登录 (remote event logging) 的功能restrict 192.168.57.0 mask 255.255.255.0 nomodify notrap 修改2：集群在局域网中，不使用其他互联网上的时间 12345# 默认会使用互联网上的时间，将以下语句注释#pool 0.ubuntu.pool.ntp.org iburst#pool 1.ubuntu.pool.ntp.org iburst#pool 2.ubuntu.pool.ntp.org iburst#pool 3.ubuntu.pool.ntp.org iburst 修改3：当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步 12server 127.127.1.0fudge 127.127.1.0 stratum 10 5.11.2.1. 重启ntp服务1systemctl restart ntp 5.11.3. 其他机器配置hadoop2、hadoop3是客户端，根据hadoop1校时 5.11.3.1. 安装ntpdate1apt-get install -y ntpdate 5.11.3.2. 测试ntpdate校时12345678timedatectl set-ntp no # 默认开启NTP时间同步，先将其关闭，才能修改系统时间# timedatectl set-time "1990-1-1 11:11:11" # 修改系统时间（timedatectl set-time不仅修改系统时间，还会修改硬件时间）date -s "1990-1-1 11:11:11" # 仅修改系统时间date # 查看系统时间timedatectl status # 查看系统时间和硬件时间ntpdate hadoop1 # 通过hadoop1进行校时timedatectl status # 再查看系统时间，查看时间是否校正timedatectl set-ntp yes # 测试结束，开启NTP时间同步 5.11.4. 设置校时计划任务开启cron服务 1systemctl start cron crontab -e 添加计划任务，每10分钟通过hadoop1进行校时 12# 注意命令要写绝对路径/usr/sbin/ntpdate，而不是写为ntpdate，因为crontab不会读取$PATH变量，会出现找不到ntpdate的情况*/10 * * * * /usr/sbin/ntpdate hadoop1]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 配置文件详解]]></title>
    <url>%2F2019%2F02%2F20%2Fhadoop%2FHadoop-%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[1. 决定各个服务组件在哪台服务器运行的配置1.1. NameNodeetc/hadoop/core-site.xml 12345&lt;property&gt; &lt;!-- 决定NameNode位于哪个主机，服务监听哪个端口 --&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;&lt;/property&gt; 1.2. DataNodeetc/hadoop/slaves 123# 一行代表一个datanode所在主机slave1slave2 1.3. SecondaryNameNodehdfs-site.xml 12345678910&lt;property&gt; &lt;!-- SecondaryNameNode的HTTP服务位于哪个主机，监听哪个端口 --&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;0.0.0.0:50090&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- SecondaryNameNode的HTTPS服务位于哪个主机，监听哪个端口 --&gt; &lt;name&gt;dfs.namenode.secondary.https-address&lt;/name&gt; &lt;value&gt;0.0.0.0:50091&lt;/value&gt;&lt;/property&gt; 1.4. ResourceManageryarn-site.xml 12345&lt;property&gt; &lt;!-- ResourceManager所在主机 --&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;0.0.0.0&lt;/value&gt;&lt;/property&gt; 1.5. NodeManagerNodeManager和DataNode的数量是恒定1：1的关系。Hadoop的设计原则是计算向数据靠拢，故NodeManager要向DataNode靠拢。在一般情况下，DataNode在哪个节点，NodeManager就会在哪个节点启动。 DataNode的地址由slaves文件决定，所以説，NodeManager的地址也由slaves文件决定。 123# etc/hadoop/slaves，一行代表一个datanode及nodemanager所在主机slave1slave2 在yarn-site.xml中有以下配置，应该是设置NodeManager服务监听的IP，而不是决定在哪台机器上启动 12345&lt;property&gt; &lt;!-- NodeManager监听的IP --&gt; &lt;name&gt;yarn.nodemanager.hostname&lt;/name&gt; &lt;value&gt;0.0.0.0&lt;/value&gt;&lt;/property&gt; 1.6. MapReduce JobHistorymapred-site.xml 123456789101112131415&lt;property&gt; &lt;!-- JobHistory IPC服务 --&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;0.0.0.0:10020&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- JobHistory Web UI HTTP服务 --&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;0.0.0.0:19888&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- JobHistory Web UI HTTPS服务 --&gt; &lt;name&gt;mapreduce.jobhistory.webapp.https.address&lt;/name&gt; &lt;value&gt;0.0.0.0:19890&lt;/value&gt;&lt;/property&gt;]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 组件启动关闭的三种方式]]></title>
    <url>%2F2019%2F02%2F20%2Fhadoop%2FHadoop%20%E7%BB%84%E4%BB%B6%E5%90%AF%E5%8A%A8%E5%85%B3%E9%97%AD%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[1. Hadoop2组件启动关闭的三种方式1.1. 方式一：各服务守护进程逐一启动关闭在各节点上执行开启关闭自己的服务 例如：namenode节点执行 hadoop-daemon.sh start|stop namenode 例如：datanode 节点执行 hadoop-daemon.sh start|stop datanode hdfs服务： 1hadoop-daemon.sh start|stop namenode|datanode|secondarynamenode yarn服务： 1yarn-daemon.sh start|stop resourcemanager|nodemanager mapreduce服务： 1mr-jobhistory-daemon.sh start|stop historyserver 1.2. 方式二：各个服务组件逐一启动关闭hdfs服务： 123# 只能在namenode执行以下命令（datanode、secondarynamenode没有权限）start-dfs.sh # 启动 namenode、datanode、secondarynamenodestop-dfs.sh yarn服务： 123# 只能在resourcemanager执行以下命令（nodemanager没有权限）start-yarn.sh # 启动 resourcemanager、nodemanagerstop-yarn.sh mapreduce服务：因为只有一个守护进程，所以就是使用 mr-jobhistory-daemon.sh 1mr-jobhistory-daemon.sh start|stop historyserver 1.3. 方式三：HDFS+YARN联合启动关闭hdfs + yarn服务： 12start-all.sh # 相当于先后执行 start-dfs.sh 和 start-yarn.shstop-all.sh # 相当于先后执行 stop-dfs.sh 和 stop-yarn.sh mapreduce服务：单独使用mr-jobhistory-daemon.sh 1mr-jobhistory-daemon.sh start|stop historyserver 注意：start-all.sh/stop-all.sh 不建议使用。因为执行的前提是当前节点既是namenode，又是resourcemanager。但是实际不可能将namenode和resourcemanager放在一个节点上，倘若这个节点挂了，那么HDFS和YARN就都挂了。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS 概述]]></title>
    <url>%2F2019%2F02%2F20%2Fhadoop%2FHDFS%20%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1. 什么是HDFS1.1. HDFS产生背景随着数据量越来越大，在一个操作系统存不下所有的数据，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种。 1.2. HDFS的定义HDFS (Hadoop Distributed File System)，它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。 HDFS的使用场景：适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。 1.3. HDFS特点HDFS以流式数据访问模式来存储超大文件，运行于商用硬件集群上。 支持超大文件 “超大文件”在这里指具有几百MB、几百GB甚至几百TB大小的文件。目前已经有存储PB级数据的Hadoop集群了 实现流式数据访问 HDFS的构建思路是这样的：一次写入、多次读取是&gt;最高效的访问模式。数据集通常由数据源生成或从数据源复制而来，接着长时间在此数据集上进行各种分析。每次分析都将涉及该数据集的大部分数据甚至全部，因此读取整个数据集的时间延迟比读取第一条记录的时间延迟更重要。 兼容廉价的商用硬件 Hadoop并不需要运行在昂贵且髙可靠的硬件上。它是设计运行在商用硬件(在各种零售店都能买到的普通硬件)的集群上的，因此至少对于庞大的集群来说，节点故障的几率还是非常髙的。HDFS遇到上述故障时，被设计成能够继续运行且不让用户察觉到明显的中断。 支持简单的文件模型 Hadoop对文件进行了简化，牺牲了一定的性能，但是获得了批量处理的特性，能快速处理批量数据集，只允许追加，不允许修改。 强大的跨平台兼容性 Hadoop是基于Java开发的，具有很好的跨平台特性 1.3.1. HDFS优点： 高容错性 数据自动保存多个副本 副本丢失后，自动恢复 适合批处理 移动计算而非数据 数据位置暴露给计算框架 适合大数据处理 数据规模：GB、TB、甚至PB级 文件规模：百万规模以上的文件数量 可构建在廉价机器上 通过多副本提高可靠性 提供了容错和恢复机制 1.3.2. HDFS缺点 不适合低延迟数据访问 比如毫秒级的数据存储（刚存完就能访问），低延迟与高吞吐率 无法高效存取小文件 每存储一个文件，至少要消耗NameNode节点150字节的内存。小文件数量多，会非常占用NameNode大量内存 小文件存储的寻道时间超过读取时间，违反了HDFS的设计目标 不支持并发写入、文件随机修改 一个文件只能有一个写者。例如用户A在上传ss.txt时，用户B不能同时上传ss.txt 仅支持数据追加append，不支持文件的随机修改 2. HDFS架构 2.1. HDFS数据单元：数据块HDFS中的文件在物理上以block形式存储。一个block在磁盘上对应两个文件，一个是数据本身，一个是块的元数据（包含数据块长度、校验和、时间戳） 注意HDFS中有两种元数据： 数据块元数据：存放在DataNode上，包含数据块长度、校验和、时间戳 文件元数据：存放在NameNode上，包含文件名、文件目录结构、文件的块副本数、文件的时间戳、文件权限、文件的数据块列表以及各个块所在DataNode等信息 2.1.1. 块大小块大小可通过dfs.blocksize来配置 Hadoop1.0中，块大小默认为64M Hadoop2.0中，块大小默认为128M。例如：200MB的文件要分为2块，分为128M + 72M 2.1.2. 为什么Hadoop2的块大小选择128M 假设寻址时间约为10ms，即HDFS查找到目标block起始位置的时间为10ms 经过研究，寻址时间为传输时间的1%时，为最佳状态。因此，传输时间=10ms/0.01=1000ms=1s 目前机械磁盘的传输速率普遍约为100MB/S。1秒内可以写数据100MB，近似128MB，所以块大小就选择了128M。 如果使用SSD，其传输速率远大于100MB/S，完全可以设置块大小为256M甚至更大。 总结：HDFS块大小设置主要取决于磁盘传输速率。 2.1.3. 为什么数据块不能设置太小?HDFS块设置太小，会增加硬盘寻址时间（硬盘寻道时间）。 块设置太小，会导致块数量非常多，只到其中一个数据块花费的时间自然就更长。这违反了HDFS的设计原则。 HDFS块设置太小，会增加Namenode内存消耗 对于HDFS，他只有一个Namenode节点，他的内存相对于Datanode来说，是极其有限的。然而，namenode需要在其内存FSImage文件中中记录在Datanode中的数据块信息，假如数据块大小设置过少，而需要维护的数据块信息就会过多，那Namenode的内存可能就会伤不起了 2.1.4. 为什么数据块不能设置太大?Map崩溃问题 系统需要重新启动，启动过程需要重新加载数据，数据块越大，数据加载时间越长，系统恢复过程越长 监管时间问题 主节点监管其他节点的情况，每个节点会周期性的把完成的工作和状态的更新报告回来。如果一个节点保持沉默超过一个预设的时间间隔，主节点记录下这个节点状态为死亡，并把分配给这个节点的数据发到别的节点。对于这个“预设的时间间隔”，这是从数据块的角度大概估算的。假如是对于64MB的数据块，我可以假设你10分钟之内无论如何也能解决了吧，超过10分钟也没反应，那就是死了。可对于640MB或是1G以上的数据，我应该要估算个多长的时间内？估算的时间短了，那就误判死亡了，分分钟更坏的情况是所有节点都会被判死亡。估算的时间长了，那等待的时间就过长了。所以对于过大的数据块，这个“预设的时间间隔”不好估算 MapReduce问题分解问题 数据块过大会导致MapReduce就一两个任务执行完全牺牲了MapReduce的并行度，发挥不了分布式并行处理的效果。 约束Map输出 在MapReduce框架里，Map之后的数据是要经过排序才执行Reduce操作的。想想归并排序算法的思想，对小文件进行排序，然后将小文件归并成大文件的思想。如果数据块很大，排序就变慢了。 2.1.5. 数据块副本数每个块有多个副本，存储在不同的DataNode上。副本数默认为3 2.1.6. 数据块副本存放策略假设数据块有三个副本： 第一个副本：放置在Client本地机架的节点上（就近原则）；如果Client是集群外提交，则随机挑选一台磁盘不太满、CPU不太忙的节点 第二个副本：放置在与第一个副本不同的机架的节点上。因为同一个机架一般使用同一个电源，电源挂掉则整个机架的服务器都挂掉。所以要放在另一个机架的节点上。 第三个副本：与第二个副本相同机架的其他节点上，随机节点。因为前2个副本已经分别存放在位于两个不同机架的节点上，安全性已经得到一定的保证。同一个机架的节点一般连接同一个高速交换机，所以现在把第3个副本放到与第2个副本相同的机架上，还保证了数据的快速传输。 如果有更多的其它副本：则挑选随机节点 2.1.7. 数据块损坏（corruption）处理情况1：client读取数据时 客户端请求DataNode读取数据块，DataNode读取block时，会计算该block的checksum 如果计算得到的checksum，与block创建时的值不一样，说明block已经损坏 client得知该block已经损坏，会读取其它DataNode上的副本块 下一次DataNode向NameNode发送块列表报告时，NameNode得知该数据块已经损坏，会复制该block的副本，达到预设的副本数 情况2：DataNode会自动在其文件创建后三周验证其checksum，处理同上。 2.1.8. 验证每个数据块有两个文件dfs/data/ 是HDFS数据的存储路径。可以看到每个数据块有两份文件，一个是数据本身，一个是元数据。 12345678910111213141516171819202122$ tree /usr/local/hadoop/tmp/dfs/data//usr/local/hadoop/tmp/dfs/data/├── current│ ├── BP-798495748-192.168.57.100-1550512504378│ │ ├── current│ │ │ ├── dfsUsed│ │ │ ├── finalized│ │ │ │ └── subdir0│ │ │ │ └── subdir0│ │ │ │ ├── blk_1073741825 # 数据文件│ │ │ │ ├── blk_1073741825_1003.meta # 元数据文件│ │ │ │ ├── blk_1073741826│ │ │ │ ├── blk_1073741826_1004.meta│ │ │ │ ├── blk_1073741827│ │ │ │ └── blk_1073741827_1005.meta│ │ │ ├── rbw│ │ │ └── VERSION│ │ ├── dncp_block_verification.log.curr│ │ ├── dncp_block_verification.log.prev│ │ └── tmp│ └── VERSION└── in_use.lock 2.2. HDFS命名空间管理HDFS的命名空间包含目录、文件和数据块 HDFS使用的是传统的分级文件体系，因此，用户可以像使用普通文件系统一样，创建、删除目录和文件，在目录间转移文件，重命名文件等。 注意：只能在文件中追加信息，但是不能修改文件内容。 2.3. NameNode（NN）NameNode是整个HDFS的管家 管理整个HDFS的名称空间（也称元数据metadata） 处理客户端的读写请求。管理客户端对HDFS的访问。客户端读写数据实际是自己到DataNode上操作的，但是要先与NameNode交互，得到元数据信息（数据在哪些DataNode上）。NameNode会根据全局情况做出决定，客户端读取数据时，NameNode尽量让客户端读取最近的副本。 配置副本策略，决定数据块副本存放在哪些DataNode上。 周期性从每个DataNode接收心跳信号和块状态报告（BlockReport）。块状态报告包含了该DataNode上所有的数据块列表信息。 NameNode保存了两个核心的数据结构，即FsImage（保存HDFS元数据）和edits（保存HDFS操作日志） FsImage用于维护文件系统树以及文件树中所有的文件和文件夹的元数据- edits中记录了所有针对文件的创建、删除、重命名等操作 2.3.1. FsImage和edits元数据在NameNode内存中有一份，同时还要写到磁盘上，文件名为FsImage FsImage文件包含文件系统中所有目录和文件inode的序列化形式。每个inode是一个文件或目录的元数据的内部表示，并包含此类信息：文件的复制等级、修改和访问时间、访问权限、块大小以及组成文件的块。对于目录，则存储修改时间、权限和配额元数据。 FsImage文件没有记录块存储在哪个数据节点。而是由名称节点把这些映射保留在内存中，当数据节点加入HDFS集群时，数据节点会把自己所包含的块列表告知给名称节点，直接加载到NameNode的内存中，此后会定期执行这种告知操作，以确保名称节点的块映射是最新的。 元数据发生改变时，不会立即同步到FsImage文件，而是在edits记录元数据的操作日志。经过一段时间后，再由SecondaryNameNode将edits中的内容合并到FsImage中 2.3.2. 为什么元数据FsImage在内存中有一份，在磁盘中也要有一份？首先，我们做个假设，如果元数据直接存储在NameNode节点的磁盘中，因为经常需要进行随机访问，还有响应客户请求，必然是效率过低。因此，元数据需要存放在内存中。但如果只存在内存中，一旦断电，元数据丢失，整个集群就无法工作了。因此产生在磁盘中备份元数据的FsImage。 2.3.3. 为什么要有Edits操作日志当在内存中的元数据更新时，如果同时更新FsImage，就会导致效率过低，但如果不更新，就会存在一致性问题，一旦NameNode节点断电，就会产生数据丢失。因此，引入Edits文件（只进行追加操作，效率很高）。每当元数据有更新或者添加元数据时，修改内存中的元数据并追加到Edits中。这样，一旦NameNode节点断电，可以通过FsImage和Edits的合并，合成元数据。 2.3.4. NameNode需要格式化NameNode一开始需要格式化，目的是生成FsImage 1hdfs namenode -format 2.4. DataNode（DN） 存储块数据、及其数据长度、校验和、时间戳 负责执行数据块读写操作 DataNode启动后向NameNode注册，通过后，周期性（1小时）向NameNode上报块列表信息，以保证集群的可靠性 每3秒一次向NameNode发送心跳，心跳返回结果带有NameNode给DataNode命令（如复制块数据到另一台机器，或删除某个数据块）。如果NameNode超过10分钟没有收到某个DataNode的心心跳，则标记该DataNode不可用，之后NameNode都不会再向该DataNode传输任何数据，除非该DataNode启动后向NameNode重新注册 2.4.1. 数据完整性思考：如果电脑磁盘里面存储的数据是控制高铁信号灯的红灯信号（1）和绿灯信号（0），但是存储该数据的磁盘坏了，一直显示是绿灯，是否很危险？同理DataNode节点上的数据损坏了，却没有发现，是否也很危险，那么如何解决呢？ 如下是DataNode节点保证数据完整性的方法。1）当DataNode读取Block的时候，它会计算CheckSum。2）如果计算后的CheckSum，与Block创建时值不一样，说明Block已经损坏。3）Client读取其他DataNode上的Block。4）DataNode在其文件创建后周期验证CheckSum，如图3-16所示。 2.4.2. DataNode掉线时限参数设置 需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒 12345678910&lt;property&gt; &lt;!-- 单位为毫秒 --&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!-- 单位为秒 --&gt; &lt;name&gt;dfs.heartbeat.interval&lt;/name&gt; &lt;value&gt;3&lt;/value&gt;&lt;/property&gt; 2.5. SecondaryNameNode（SNN/2NN）SecondaryNameNode的作用： 辅助NameNode合并fsimage和edits 辅助合并fsimage和edits时，会在本地保留一份fsimage.ckpt，所以还起到元数据冷备份的作用。紧急情况下，可辅助恢复NameNode 即使SecondaryNameNode不启动，HDFS也能正常工作 2.5.1. 为什么要辅助NameNode合并fsimage和edits假如HDFS起来之后，几个月都没有停止、重启。如果操作了很多数据，edits就会非常大，下一次NameNode重启时，读取edits要花很多时间。 此时就需要SecondaryNameNode，每隔一段时间（当然触发情况不止是这一个），就去帮助NameNode合并fsimage和edits，生成新的fsimage，再把新的fsimage复制给NameNode。 这样下一次NameNode重启时，edits就不会特别大，读取edits不用花太多时间，就加快了NameNode的启动时间。 2.5.2. SecondaryNameNode触发合并的情况 情况1：根据hdfs-site.xml设置的dfs.namenode.checkpoint.period，checkpoint默认周期3600秒（1小时） 情况2：根据hdfs-site.xml设置的dfs.namenode.checkpoint.txns，默认edits中的记录超过100万条，就触发checkpoint。 那么如何知道记录达到100万条呢？根据hdfs-site.xml设置的dfs.namenode.checkpoint.check.period，每60秒检查一次，edits中的记录有多少条。 2.5.3. 合并流程 当edits文件的大小达到一个临界值(默认是64MB)或者间隔一段时间(默认是1小时)的时候checkpoint会触发SecondaryNameNode进行合并工作。 当触发一个checkpoint操作时，NameNode会生成一个新的edits即上图中的edits.new文件，之后元数据的改变都会写入edits.new文件中，而不是原来的edits文件。 SecondaryNameNode会将NameNode的edits文件和FsImage下载到本地。 SecondaryNameNode将本地的FsImage文件加载到内存中，然后再与edits文件进行合并生成一个新的FsImage文件即上图中的FsImage.ckpt文件。 SecondaryNameNode将新生成的FsImage.ckpt文件发送到NameNode节点。 NameNode结点的edits.new文件和FsImage.ckpt文件会替换掉原来的edits文件和FsImage文件，至此，刚好是一个轮回即在NameNode中又是edits和FsImage文件了。 等待下一次checkpoint触发SecondaryNameNode进行工作，一直这样循环操作。 以上过程既实现了FsImage和EditsLog的合并。因为其间名称节点的元数据被拷贝到第二名称节点上，所以说起到冷备份的效果。 2.6. HDFS Client客户端 文件切分。文件上传HDFS的时候，Client将文件切分成一个1的Block,然后进行上传; 与NameNode交互，获取文件的位置信息； 与DataNode交互，读取或者写入数据； Client提供一些命令来管理HDFS,比如NameNode格式化； Client可以通过一些命令来访问HDFS,比如对HDFS増删查改操作； 3. HDFS安全机制3.1. HDFS副本]]></content>
      <categories>
        <category>Hadoop</category>
        <category>HDFS</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 概述]]></title>
    <url>%2F2019%2F02%2F20%2Fhadoop%2FHadoop%20%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[1. 什么是hadoop Hadoop是一个由Apache基金会所开发的分布式系统基础架构 主要解决海量数据的存储和分析计算问题。 广义上来说，HADOOP通常是指一个更广泛的概念——HADOOP生态圈 1.1. hadoop核心组件它包括三部分：HDFS，YARN，和MapReduce 组件 描述 功能 HDFS Hadoop分布式文件系统 分布式存储海量数据集 MapReduce 基于YARN分布式并行处理海量数据集 分布式计算海量数据集 YARN 任务调度和集群资源管理框架 任务调度、资源管理 1.2. hadoop狭义与广义狭义hadoop：大数据分布式存储（HDFS） + 分布式计算 + 资源调度（YARN）框架。 广义hadoop：指hadoop生态系统（生态圈）。hadoop框架是其中最重要最基础的一个部分。生态系统中的每一个子系统只能解决某一个领域的特定问题域。 HDFS：分布式文件系统MapReduce：分布式运算程序开发框架Hbase：基于HADOOP的分布式海量数据库，离线分析和在线业务通吃Hive：基于大数据技术（文件系统+运算框架）的SQL数据仓库工具，使用方便，功能丰富，基于MR延迟大Sqoop：数据导入导出工具Flume：数据采集框架ZOOKEEPER：分布式协调服务基础组件Mahout：基于mapreduce/spark/flink等分布式运算框架的机器学习算法库Oozie：工作流调度框架Sqoop：数据导入导出工具Flume：日志数据采集框架 2. Hadoop发展历史1）Lucene——Doug Cutting开创的开源软件，用java书写代码，实现与Google类似的全文搜索功能，它提供了全文检索引擎的架构，包括完整的查询引擎和索引引擎 2）2001年年底成为apache基金会的一个子项目 3）对于大数量的场景，Lucene面对与Google同样的困难 4）学习和模仿Google解决这些问题的办法 ：微型版Nutch 5）可以说Google是hadoop的思想之源(Google在大数据方面的三篇论文) GFS —-&gt;HDFSMapReduce —-&gt;MapReduceBigTable —-&gt;HBase 6）2003-2004年，Google公开了部分GFS和Mapreduce思想的细节，以此为基础Doug Cutting等人用了2年业余时间实现了DFS和Mapreduce机制，使Nutch性能飙升 7）2005 年Hadoop 作为 Lucene的子项目 Nutch的一部分正式引入Apache基金会。2006 年 3 月份，Map-Reduce和Nutch Distributed File System (NDFS) 分别被纳入称为 Hadoop 的项目中 8）名字来源于Doug Cutting儿子的玩具大象 9）Hadoop就此诞生并迅速发展，标志这云计算时代来临 3. Hadoop三大发行版本Hadoop三大发行版本：Apache、Cloudera、Hortonworks。 Apache版本最原始（最基础）的版本，对于入门学习最好 Cloudera在大型互联网企业中用的较多。使用免费，解决问题收费 Hortonworks文档较好。使用免费，解决问题收费 3.1. Apache Hadoop官网地址：http://hadoop.apache.org/releases.html 下载地址：https://archive.apache.org/dist/hadoop/common/ 3.2. Cloudera Hadoop官网地址：https://www.cloudera.com/downloads/cdh/5-10-0.html 下载地址：http://archive-primary.cloudera.com/cdh5/cdh/5/ 全称 Cloudera’s Distribution Including Apache Hadoop（简称CDH） （1）2008年成立的Cloudera是最早将Hadoop商用的公司，为合作伙伴提供Hadoop的商用解决方案，主要是包括支持、咨询服务、培训。 （2）2009年Hadoop的创始人Doug Cutting也加盟Cloudera公司。Cloudera产品主要为CDH，Cloudera Manager，Cloudera Support （3）CDH是Cloudera的Hadoop发行版，完全开源，比Apache Hadoop在兼容性，安全性，稳定性上有所增强。 （4）Cloudera Manager是集群的软件分发及管理监控平台，可以在几个小时内部署好一个Hadoop集群，并对集群的节点及服务进行实时监控。Cloudera Support即是对Hadoop的技术支持。 （5）Cloudera的标价为每年每个节点4000美元。Cloudera开发并贡献了可实时处理大数据的Impala项目。 3.3. Hortonworks Hadoop官网地址：https://hortonworks.com/products/data-center/hdp/ 下载地址：https://hortonworks.com/downloads/#data-platform （1）2011年成立的Hortonworks是雅虎与硅谷风投公司Benchmark Capital合资组建。 （2）公司成立之初就吸纳了大约25名至30名专门研究Hadoop的雅虎工程师，上述工程师均在2005年开始协助雅虎开发Hadoop，贡献了Hadoop 80%的代码。 （3）雅虎工程副总裁、雅虎Hadoop开发团队负责人Eric Baldeschwieler出任Hortonworks的首席执行官。 （4）Hortonworks的主打产品是Hortonworks Data Platform（HDP），也同样是100%开源的产品，HDP除常见的项目外还包括了Ambari，一款开源的安装和管理系统。 （5）HCatalog，一个元数据管理系统，HCatalog现已集成到Facebook开源的Hive中。Hortonworks的Stinger开创性的极大的优化了Hive项目。Hortonworks为入门提供了一个非常好的，易于使用的沙盒。 （6）Hortonworks开发了很多增强特性并提交至核心主干，这使得Apache Hadoop能够在包括Window Server和Windows Azure在内的Microsoft Windows平台上本地运行。定价以集群为基础，每10个节点每年为12500美元。 4. Hadoop的优势（4高）1）高可靠性：Hadoop底层维护多个数据副本，所以即使Hadoop某个计算元素或存储出现故障，也不会导致数据的丢失。2）高扩展性：在集群间分配任务数据，可方便的扩展数以千计的节点。3）高效性：在MapReduce的思想下，Hadoop是并行工作的，以加快任务处理速度。4）高容错性：能够自动将失败的任务重新分配。 5. Hadoop各版本区别⭐⭐ Question：Hadoop1.x与2.x有什么区别 Hadoop1.0 有三个组件： Common：支持其他模块的工具模块（Configuration、RPC、序列化机制、日志操作） HDFS：一个高可靠、高吞吐量的分布式文件系统，负责数据存储 MapReduce：分布式的资源调度和离线并行计算框架。不仅要处理数据（计算），还要管理集群资源（资源指CPU、内存、磁盘，如CPU分配8核给谁用、内存分配128G给谁用、磁盘分配8T给谁用），又当爹又当妈 😂 Hadoop2.0 有四个组件： YARN成为Hadoop的操作系统。任务调度和集群资源管理由YARN负责。 在YARN上可以运行各种数据处理框架。不仅能运行原来的批处理框架MapReduce，还能运行流式处理框架Storm、内存计算框架Spark等等 在Hadoop1.x时代，MapReduce同时处理业务逻辑计算和资源调度，耦合性较大。 在Hadoop2.x时代，增加了YARN，由YARN负责任务调度和资源管理，MapReduce只负责计算，实现了解耦、模块化。 6. 大数据技术生态体系⭐ 6.1. 案例：电商推荐系统项目框架]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
</search>
